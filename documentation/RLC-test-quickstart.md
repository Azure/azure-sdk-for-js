# Table of contents

- [Table of contents](#table-of-contents)
- [Overview](#overview)
- [Prerequisites](#prerequisites)
- [How to Run Test](#how-to-run-test)
  - [Test strucure](#test-strucure)
  - [Fail to run tests in the first time](#fail-to-run-tests-in-the-first-time)
  - [Running tests in playback mode](#running-tests-in-playback-mode)
- [Writing New Tests](#writing-new-tests)
  - [Example: Basic Azure service interaction and recording](#example-basic-azure-service-interaction-and-recording)

# Overview

This page is to help you write and run tests for Azure Javascript SDK. The Azure SDK test framework uses the [`test-recorder`](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/test-utils/recorder/README.md) library, which in turn rests upon on a HTTP recording system ([testproxy](https://github.com/Azure/azure-sdk-tools/tree/main/tools/test-proxy)) that enables tests dependent on network interaction to be run offline.

# Prerequisites

- Docker
  - [Docker](https://www.docker.com/get-started/) is required, as the [test proxy server](https://github.com/Azure/azure-sdk-tools/tree/main/tools/test-proxy) is running in a container during testing
  - When running the tests, ensure the Docker daemon is running and you have permission to use it.
- Rush 5.x
  - Install/update Rush globally via `npm install -g @microsoft/rush`
- Any of [the LTS versions of Node.js](https://nodejs.org/en/about/releases/)
- A C++ compiler toolchain and Python (for compiling machine-code modules)
  - Refer [here](https://github.com/Azure/azure-sdk-for-js/blob/main/CONTRIBUTING.md#prerequisites) for more details

# How to Run Test

This section describes how to run the SDK tests. If you want to run the tests of a specific project, go to that project's folder and execute `rushx test`. All of the tests will automatically run both in NodeJS and in the browser. To target these environments individually, you can run `rushx test:node` and `rushx test:browser`. Let's take `purview-catalog-rest` as an example.

## Test strucure

At the moment, tests in our repo depend on one of the two different versions of the recorder tool (`@azure-tools/test-recorder`) - `1.a.b` and `2.x.y`. Currently version 2.x.y utilize the [Azure SDK Tools Test Proxy](https://github.com/Azure/azure-sdk-tools/blob/main/tools/test-proxy/README.md) to record and playback HTTP interactions.

To migrate an existing test recorder to use the test proxy, or to learn more about using the test proxy, refer to the [test recoeder migration guide](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/test-utils/recorder/MIGRATION.md).

Eventually, all the tests will be migrated to depend on the 2.x.y version of the recorder and our guide is based on the 2.x.y one.

If you are the first time to generate SDK you could enable the config `generate-test: true` in `README.md`. We'll generate simple utils and a sample test file for you with below similar structure. They only contains basics for testing, so you need to update to your own utility and test cases.

```
sdk/
├─ purview/
│  ├─ purview-catalog-rest/
│  │  ├─ src/
│  │  │  ├─ ...
│  │  ├─ test/
│  │  │  ├─ public/
│  │  │  |  ├─ utils/
│  │  │  |  |  ├─ env.ts
│  │  │  |  |  ├─ recordedClient.ts
│  │  │  ├─ sampleTest.spec.ts
```

## Fail to run tests in the first time

Before running tests it's advised to update the dependencises and build our project by running the command `rush update && rush build -t <your-package-name>`. And please notice this command is time-consuming and take around 10 mins. You could find more [details](https://github.com/Azure/azure-sdk-for-js/blob/main/CONTRIBUTING.md#installing-and-managing-dependencies).

Here we could run the tests in `purview-catalog-rest`. By default, these npm scripts run previously recorded tests. The recordings have been generated by using a custom recording library called [`test-recorder`](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/test-utils/recorder/README.md).

```Shell
sdk/purview/purview-catalog-rest> rushx test
```

If you are the first time to run tests you may fail with below message because there is no any recordings found.

```
[node-tests]   My test
[node-tests]     1) "before each" hook for "sample test"
[node-tests]     2) "after each" hook for "sample test"
[node-tests]
[node-tests]
[node-tests]   0 passing (225ms)
[node-tests]   2 failing
[node-tests]
[node-tests]   1) My test
[node-tests]        "before each" hook for "sample test":
[node-tests]      RecorderError: Start request failed.
```

To record or update your recordings you need to set the environment variable `TEST_MODE` to `record`. On Linux, you could use `export` to set env variable:

```shell
export TEST_MODE=record && rushx test
```

On Windows, you could use `set`:

```shell
SET TEST_MODE=record&& rushx test
```

Then you could have following similar logs and also go to the folder `purview-catalog-rest/recordings` to check the recording files.

```
[test-info] ===TEST_MODE="record"===
[0] [test-proxy] Attempting to start test proxy at http://localhost:5000 & https://localhost:5001.
[0]
[0] [test-proxy] Image tag obtained from the powershell script => 1.0.0-dev.20220427.1
[0]
[0] [test-proxy] Check the output file "test-proxy-output.log" for test-proxy logs.
[node-tests] [check-with-timeout] waiting for 1000ms
[node-tests] [check-with-timeout] waiting for 1000ms
[node-tests] [test-proxy] Proxy tool seems to be active at http://localhost:5000
[node-tests]
[node-tests] [check-with-timeout] checkWithTimeout condition returned true
[node-tests]
[node-tests]
[node-tests]   My test
[node-tests]     √ sample test
[node-tests]
[node-tests]
[node-tests]   1 passing (223ms)
```

## Running tests in playback mode

If you have existing recordings then the tests have been run against generated the HTTP recordings, you can run your tests in `playback` mode.

On Linux, you could use below commands:

```shell
export TEST_MODE=playback && rushx test
```

On Windows, you can use:

```shell
SET TEST_MODE=playback&& rushx test
```

Then the log could indicate that it is in `playback` mode.

```
[test-info] ===TEST_MODE="playback"===
```

# Writing New Tests

In the `test` directory create a file with the naming pattern `<what_you_are_testing>.spec.ts`. It's recommanded to seprate your test cases into `pulic` and `internal`(if you have) folders.

## Example: Basic Azure service interaction and recording

This simple test creates a resource and checks that its name is assigned correctly. Take `purview-catalog-rest` as example:

- Step 1: Create your test file and add one test case with resource creation, here we have purview catalog glossary test file `glossary.spec.ts` and one case named `Should create a glossary`
- Step 2: Add the utility method `createClient` in `public/utils/recordedClient.ts` to share the `PurviewCatalogClient` creation.
  - Call `createTestCredential` to init your credential and refer [here](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/test-utils/recorder/MIGRATION.md#aad-and-the-new-noopcredential) for more details
  - Wrap the `option` with test options by calling `recorder.configureClientOptions(options)`
- Step 3: Call `createClient` to prepare the client and call `client.path("/atlas/v2/glossary").post()` to create our glossary resource
- Step 4: Specify environment variables that would be replaced in the recording and set during playback in the map `envSetupForPlayback` under the file `public/utils/recordedClient.ts`. This could be used to ensure that secrets and user-specific options do not appear in the recording body
- Step 5: Add necessary assertions in your test case, then run and record your test

`glossary.spec.ts`

```typescript
import { Recorder } from "@azure-tools/test-recorder";
import { assert } from "chai";
import { PurviewCatalogClient } from "../../src";
import { createClient, createRecorder } from "./utils/recordedClient";

describe("My test", () => {
  let recorder: Recorder;
  let client: PurviewCatalogClient;
  let glossaryName: string;

  beforeEach(async function () {
    recorder = await createRecorder(this);
    client = await createClient(recorder);
    glossaryName = "js-testing";
  });

  afterEach(async function () {
    await recorder.stop();
  });

  it("Should create a glossary", async () => {
    const glossary = await client.path("/atlas/v2/glossary").post({
      body: {
        name: glossaryName,
        shortDescription: "Example Short Description",
        longDescription: "Example Long Description",
        language: "en",
        usage: "Example Glossary",
      },
    });
    assert.strictEqual(glossary.status, "200");
  });
});
```

`utils/recordedClient.ts`

```typescript
import { Context } from "mocha";
import { Recorder, RecorderStartOptions } from "@azure-tools/test-recorder";
import PurviewCatalog, { PurviewCatalogClient } from "../../../src";
import { createTestCredential } from "@azure-tools/test-credential";
import { ClientOptions } from "@azure-rest/core-client";
import "./env";

const envSetupForPlayback: Record<string, string> = {
  ENDPOINT: "https://endpoint",
  AZURE_CLIENT_ID: "azure_client_id",
  AZURE_CLIENT_SECRET: "azure_client_secret",
  AZURE_TENANT_ID: "88888888-8888-8888-8888-888888888888",
  SUBSCRIPTION_ID: "azure_subscription_id",
  PURVIEW_CATALOG_GLOSSARY_ENV: "glossary_custom_env", // Add environment vara you'd like to replace
};

const recorderEnvSetup: RecorderStartOptions = {
  envSetupForPlaayback,
};

/**
 * creates the recorder and reads the environment variables from the `.env` file.
 * Should be called first in the test suite to make sure environment variables are
 * read before they are being used.
 */
export async function createRecorder(context: Context): Promise<Recorder> {
  const recorder = new Recorder(context.currentTest);
  await recorder.start(recorderEnvSetup);
  return recorder;
}

// Add your client creation factory
export function createClient(recorder: Recorder, options?: ClientOptions): PurviewCatalogClient {
  // Using createTestCredential so it could work in playback mode
  const credential = createTestCredential();
  const client = PurviewCatalog("<endpoint>", credential, recorder.configureClientOptions(options));
  return client;
}
```
