/*
 * Copyright (c) Microsoft Corporation. All rights reserved.
 * Licensed under the MIT License. See License.txt in the project root for license information.
 *
 * Code generated by Microsoft (R) AutoRest Code Generator.
 * Changes may cause incorrect behavior and will be lost if the code is regenerated.
 */

import { BaseResource, CloudError, AzureServiceClientOptions } from "@azure/ms-rest-azure-js";
import * as msRest from "@azure/ms-rest-js";

export { BaseResource, CloudError };

/**
 * Azure Data Factory top-level resource.
 */
export interface Resource extends BaseResource {
  /**
   * The resource identifier.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly id?: string;
  /**
   * The resource name.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly name?: string;
  /**
   * The resource type.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly type?: string;
  /**
   * The resource location.
   */
  location?: string;
  /**
   * The resource tags.
   */
  tags?: { [propertyName: string]: string };
  /**
   * Etag identifies change in the resource.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly eTag?: string;
}

/**
 * Azure Data Factory nested resource, which belongs to a factory.
 */
export interface SubResource extends BaseResource {
  /**
   * The resource identifier.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly id?: string;
  /**
   * The resource name.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly name?: string;
  /**
   * The resource type.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly type?: string;
  /**
   * Etag identifies change in the resource.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly etag?: string;
}

/**
 * Azure Data Factory nested debug resource.
 */
export interface SubResourceDebugResource {
  /**
   * The resource name.
   */
  name?: string;
}

/**
 * Azure Data Factory expression definition.
 */
export interface Expression {
  /**
   * Expression value.
   */
  value: string;
}

/**
 * Contains the possible cases for SecretBase.
 */
export type SecretBaseUnion = SecretBase | SecureString | AzureKeyVaultSecretReference;

/**
 * The base definition of a secret type.
 */
export interface SecretBase {
  /**
   * Polymorphic Discriminator
   */
  type: "SecretBase";
}

/**
 * Azure Data Factory secure string definition. The string value will be masked with asterisks '*'
 * during Get or List API calls.
 */
export interface SecureString {
  /**
   * Polymorphic Discriminator
   */
  type: "SecureString";
  /**
   * Value of secure string.
   */
  value: string;
}

/**
 * Linked service reference type.
 */
export interface LinkedServiceReference {
  /**
   * Reference LinkedService name.
   */
  referenceName: string;
  /**
   * Arguments for LinkedService.
   */
  parameters?: { [propertyName: string]: any };
}

/**
 * Azure Key Vault secret reference.
 */
export interface AzureKeyVaultSecretReference {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureKeyVaultSecret";
  /**
   * The Azure Key Vault linked service reference.
   */
  store: LinkedServiceReference;
  /**
   * The name of the secret in Azure Key Vault. Type: string (or Expression with resultType
   * string).
   */
  secretName: any;
  /**
   * The version of the secret in Azure Key Vault. The default value is the latest version of the
   * secret. Type: string (or Expression with resultType string).
   */
  secretVersion?: any;
}

/**
 * Identity properties of the factory resource.
 */
export interface FactoryIdentity {
  /**
   * The principal id of the identity.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly principalId?: string;
  /**
   * The client tenant id of the identity.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly tenantId?: string;
}

/**
 * Contains the possible cases for FactoryRepoConfiguration.
 */
export type FactoryRepoConfigurationUnion = FactoryRepoConfiguration | FactoryVSTSConfiguration | FactoryGitHubConfiguration;

/**
 * Factory's git repo information.
 */
export interface FactoryRepoConfiguration {
  /**
   * Polymorphic Discriminator
   */
  type: "FactoryRepoConfiguration";
  /**
   * Account name.
   */
  accountName: string;
  /**
   * Repository name.
   */
  repositoryName: string;
  /**
   * Collaboration branch.
   */
  collaborationBranch: string;
  /**
   * Root folder.
   */
  rootFolder: string;
  /**
   * Last commit id.
   */
  lastCommitId?: string;
}

/**
 * Definition of a single parameter for an entity.
 */
export interface GlobalParameterSpecification {
  /**
   * Global Parameter type. Possible values include: 'Object', 'String', 'Int', 'Float', 'Bool',
   * 'Array'
   */
  type: GlobalParameterType;
  /**
   * Value of parameter.
   */
  value: any;
}

/**
 * Factory resource type.
 */
export interface Factory extends Resource {
  /**
   * Managed service identity of the factory.
   */
  identity?: FactoryIdentity;
  /**
   * Factory provisioning state, example Succeeded.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly provisioningState?: string;
  /**
   * Time the factory was created in ISO8601 format.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly createTime?: Date;
  /**
   * Version of the factory.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly version?: string;
  /**
   * Git repo information of the factory.
   */
  repoConfiguration?: FactoryRepoConfigurationUnion;
  /**
   * List of parameters for factory.
   */
  globalParameters?: { [propertyName: string]: GlobalParameterSpecification };
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Contains the possible cases for IntegrationRuntime.
 */
export type IntegrationRuntimeUnion = IntegrationRuntime | SelfHostedIntegrationRuntime | ManagedIntegrationRuntime;

/**
 * Azure Data Factory nested object which serves as a compute resource for activities.
 */
export interface IntegrationRuntime {
  /**
   * Polymorphic Discriminator
   */
  type: "IntegrationRuntime";
  /**
   * Integration runtime description.
   */
  description?: string;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Integration runtime resource type.
 */
export interface IntegrationRuntimeResource extends SubResource {
  /**
   * Integration runtime properties.
   */
  properties: IntegrationRuntimeUnion;
}

/**
 * Integration runtime reference type.
 */
export interface IntegrationRuntimeReference {
  /**
   * Reference integration runtime name.
   */
  referenceName: string;
  /**
   * Arguments for integration runtime.
   */
  parameters?: { [propertyName: string]: any };
}

/**
 * Integration runtime debug resource.
 */
export interface IntegrationRuntimeDebugResource extends SubResourceDebugResource {
  /**
   * Integration runtime properties.
   */
  properties: IntegrationRuntimeUnion;
}

/**
 * Contains the possible cases for IntegrationRuntimeStatus.
 */
export type IntegrationRuntimeStatusUnion = IntegrationRuntimeStatus | SelfHostedIntegrationRuntimeStatus | ManagedIntegrationRuntimeStatus;

/**
 * Integration runtime status.
 */
export interface IntegrationRuntimeStatus {
  /**
   * Polymorphic Discriminator
   */
  type: "IntegrationRuntimeStatus";
  /**
   * The data factory name which the integration runtime belong to.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly dataFactoryName?: string;
  /**
   * The state of integration runtime. Possible values include: 'Initial', 'Stopped', 'Started',
   * 'Starting', 'Stopping', 'NeedRegistration', 'Online', 'Limited', 'Offline', 'AccessDenied'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly state?: IntegrationRuntimeState;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Integration runtime status response.
 */
export interface IntegrationRuntimeStatusResponse {
  /**
   * The integration runtime name.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly name?: string;
  /**
   * Integration runtime properties.
   */
  properties: IntegrationRuntimeStatusUnion;
}

/**
 * A list of integration runtime status.
 */
export interface IntegrationRuntimeStatusListResponse {
  /**
   * List of integration runtime status.
   */
  value: IntegrationRuntimeStatusResponse[];
  /**
   * The link to the next page of results, if any remaining results exist.
   */
  nextLink?: string;
}

/**
 * Update integration runtime request.
 */
export interface UpdateIntegrationRuntimeRequest {
  /**
   * Enables or disables the auto-update feature of the self-hosted integration runtime. See
   * https://go.microsoft.com/fwlink/?linkid=854189. Possible values include: 'On', 'Off'
   */
  autoUpdate?: IntegrationRuntimeAutoUpdate;
  /**
   * The time offset (in hours) in the day, e.g., PT03H is 3 hours. The integration runtime auto
   * update will happen on that time.
   */
  updateDelayOffset?: string;
}

/**
 * Update integration runtime node request.
 */
export interface UpdateIntegrationRuntimeNodeRequest {
  /**
   * The number of concurrent jobs permitted to run on the integration runtime node. Values between
   * 1 and maxConcurrentJobs(inclusive) are allowed.
   */
  concurrentJobsLimit?: number;
}

/**
 * Data factory name for linked integration runtime request.
 */
export interface LinkedIntegrationRuntimeRequest {
  /**
   * The data factory name for linked integration runtime.
   */
  linkedFactoryName: string;
}

/**
 * The linked integration runtime information.
 */
export interface CreateLinkedIntegrationRuntimeRequest {
  /**
   * The name of the linked integration runtime.
   */
  name?: string;
  /**
   * The ID of the subscription that the linked integration runtime belongs to.
   */
  subscriptionId?: string;
  /**
   * The name of the data factory that the linked integration runtime belongs to.
   */
  dataFactoryName?: string;
  /**
   * The location of the data factory that the linked integration runtime belongs to.
   */
  dataFactoryLocation?: string;
}

/**
 * Definition of a single parameter for an entity.
 */
export interface ParameterSpecification {
  /**
   * Parameter type. Possible values include: 'Object', 'String', 'Int', 'Float', 'Bool', 'Array',
   * 'SecureString'
   */
  type: ParameterType;
  /**
   * Default value of parameter.
   */
  defaultValue?: any;
}

/**
 * Contains the possible cases for LinkedService.
 */
export type LinkedServiceUnion = LinkedService | SharePointOnlineListLinkedService | SnowflakeLinkedService | AzureFunctionLinkedService | AzureDataExplorerLinkedService | SapTableLinkedService | GoogleAdWordsLinkedService | OracleServiceCloudLinkedService | DynamicsAXLinkedService | ResponsysLinkedService | AzureDatabricksLinkedService | AzureDataLakeAnalyticsLinkedService | HDInsightOnDemandLinkedService | SalesforceMarketingCloudLinkedService | NetezzaLinkedService | VerticaLinkedService | ZohoLinkedService | XeroLinkedService | SquareLinkedService | SparkLinkedService | ShopifyLinkedService | ServiceNowLinkedService | QuickBooksLinkedService | PrestoLinkedService | PhoenixLinkedService | PaypalLinkedService | MarketoLinkedService | AzureMariaDBLinkedService | MariaDBLinkedService | MagentoLinkedService | JiraLinkedService | ImpalaLinkedService | HubspotLinkedService | HiveLinkedService | HBaseLinkedService | GreenplumLinkedService | GoogleBigQueryLinkedService | EloquaLinkedService | DrillLinkedService | CouchbaseLinkedService | ConcurLinkedService | AzurePostgreSqlLinkedService | AmazonMWSLinkedService | SapHanaLinkedService | SapBWLinkedService | SftpServerLinkedService | FtpServerLinkedService | HttpLinkedService | AzureSearchLinkedService | CustomDataSourceLinkedService | AmazonRedshiftLinkedService | AmazonS3LinkedService | RestServiceLinkedService | SapOpenHubLinkedService | SapEccLinkedService | SapCloudForCustomerLinkedService | SalesforceServiceCloudLinkedService | SalesforceLinkedService | Office365LinkedService | AzureBlobFSLinkedService | AzureDataLakeStoreLinkedService | CosmosDbMongoDbApiLinkedService | MongoDbV2LinkedService | MongoDbLinkedService | CassandraLinkedService | WebLinkedService | ODataLinkedService | HdfsLinkedService | MicrosoftAccessLinkedService | InformixLinkedService | OdbcLinkedService | AzureMLServiceLinkedService | AzureMLLinkedService | TeradataLinkedService | Db2LinkedService | SybaseLinkedService | PostgreSqlLinkedService | MySqlLinkedService | AzureMySqlLinkedService | OracleLinkedService | GoogleCloudStorageLinkedService | AzureFileStorageLinkedService | FileServerLinkedService | HDInsightLinkedService | CommonDataServiceForAppsLinkedService | DynamicsCrmLinkedService | DynamicsLinkedService | CosmosDbLinkedService | AzureKeyVaultLinkedService | AzureBatchLinkedService | AzureSqlMILinkedService | AzureSqlDatabaseLinkedService | SqlServerLinkedService | AzureSqlDWLinkedService | AzureTableStorageLinkedService | AzureBlobStorageLinkedService | AzureStorageLinkedService;

/**
 * The Azure Data Factory nested object which contains the information and credential which can be
 * used to connect with related store or compute resource.
 */
export interface LinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "LinkedService";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Linked service resource type.
 */
export interface LinkedServiceResource extends SubResource {
  /**
   * Properties of linked service.
   */
  properties: LinkedServiceUnion;
}

/**
 * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
 */
export interface DatasetFolder {
  /**
   * The name of the folder that this Dataset is in.
   */
  name?: string;
}

/**
 * Contains the possible cases for Dataset.
 */
export type DatasetUnion = Dataset | SharePointOnlineListResourceDataset | SnowflakeDataset | GoogleAdWordsObjectDataset | AzureDataExplorerTableDataset | OracleServiceCloudObjectDataset | DynamicsAXResourceDataset | ResponsysObjectDataset | SalesforceMarketingCloudObjectDataset | VerticaTableDataset | NetezzaTableDataset | ZohoObjectDataset | XeroObjectDataset | SquareObjectDataset | SparkObjectDataset | ShopifyObjectDataset | ServiceNowObjectDataset | QuickBooksObjectDataset | PrestoObjectDataset | PhoenixObjectDataset | PaypalObjectDataset | MarketoObjectDataset | AzureMariaDBTableDataset | MariaDBTableDataset | MagentoObjectDataset | JiraObjectDataset | ImpalaObjectDataset | HubspotObjectDataset | HiveObjectDataset | HBaseObjectDataset | GreenplumTableDataset | GoogleBigQueryObjectDataset | EloquaObjectDataset | DrillTableDataset | CouchbaseTableDataset | ConcurObjectDataset | AzurePostgreSqlTableDataset | AmazonMWSObjectDataset | HttpDataset | AzureSearchIndexDataset | WebTableDataset | SapTableResourceDataset | RestResourceDataset | SqlServerTableDataset | SapOpenHubTableDataset | SapHanaTableDataset | SapEccResourceDataset | SapCloudForCustomerResourceDataset | SapBwCubeDataset | SybaseTableDataset | SalesforceServiceCloudObjectDataset | SalesforceObjectDataset | MicrosoftAccessTableDataset | PostgreSqlTableDataset | MySqlTableDataset | OdbcTableDataset | InformixTableDataset | RelationalTableDataset | Db2TableDataset | AmazonRedshiftTableDataset | AzureMySqlTableDataset | TeradataTableDataset | OracleTableDataset | ODataResourceDataset | CosmosDbMongoDbApiCollectionDataset | MongoDbV2CollectionDataset | MongoDbCollectionDataset | FileShareDataset | Office365Dataset | AzureBlobFSDataset | AzureDataLakeStoreDataset | CommonDataServiceForAppsEntityDataset | DynamicsCrmEntityDataset | DynamicsEntityDataset | DocumentDbCollectionDataset | CosmosDbSqlApiCollectionDataset | CustomDataset | CassandraTableDataset | AzureSqlDWTableDataset | AzureSqlMITableDataset | AzureSqlTableDataset | AzureTableDataset | AzureBlobDataset | BinaryDataset | OrcDataset | XmlDataset | JsonDataset | DelimitedTextDataset | ParquetDataset | ExcelDataset | AvroDataset | AmazonS3Dataset;

/**
 * The Azure Data Factory nested object which identifies data within different data stores, such as
 * tables, files, folders, and documents.
 */
export interface Dataset {
  /**
   * Polymorphic Discriminator
   */
  type: "Dataset";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Dataset resource type.
 */
export interface DatasetResource extends SubResource {
  /**
   * Dataset properties.
   */
  properties: DatasetUnion;
}

/**
 * Activity dependency information.
 */
export interface ActivityDependency {
  /**
   * Activity name.
   */
  activity: string;
  /**
   * Match-Condition for the dependency.
   */
  dependencyConditions: DependencyCondition[];
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * User property.
 */
export interface UserProperty {
  /**
   * User property name.
   */
  name: string;
  /**
   * User property value. Type: string (or Expression with resultType string).
   */
  value: any;
}

/**
 * Contains the possible cases for Activity.
 */
export type ActivityUnion = Activity | ExecutionActivityUnion | ControlActivityUnion;

/**
 * A pipeline activity.
 */
export interface Activity {
  /**
   * Polymorphic Discriminator
   */
  type: "Activity";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Definition of a single variable for a Pipeline.
 */
export interface VariableSpecification {
  /**
   * Variable type. Possible values include: 'String', 'Bool', 'Array'
   */
  type: VariableType;
  /**
   * Default value of variable.
   */
  defaultValue?: any;
}

/**
 * The folder that this Pipeline is in. If not specified, Pipeline will appear at the root level.
 */
export interface PipelineFolder {
  /**
   * The name of the folder that this Pipeline is in.
   */
  name?: string;
}

/**
 * Pipeline resource type.
 */
export interface PipelineResource extends SubResource {
  /**
   * The description of the pipeline.
   */
  description?: string;
  /**
   * List of activities in pipeline.
   */
  activities?: ActivityUnion[];
  /**
   * List of parameters for pipeline.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of variables for pipeline.
   */
  variables?: { [propertyName: string]: VariableSpecification };
  /**
   * The max number of concurrent runs for the pipeline.
   */
  concurrency?: number;
  /**
   * List of tags that can be used for describing the Pipeline.
   */
  annotations?: any[];
  /**
   * Dimensions emitted by Pipeline.
   */
  runDimensions?: { [propertyName: string]: any };
  /**
   * The folder that this Pipeline is in. If not specified, Pipeline will appear at the root level.
   */
  folder?: PipelineFolder;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Contains the possible cases for Trigger.
 */
export type TriggerUnion = Trigger | ChainingTrigger | RerunTumblingWindowTrigger | TumblingWindowTrigger | MultiplePipelineTriggerUnion;

/**
 * Azure data factory nested object which contains information about creating pipeline run
 */
export interface Trigger {
  /**
   * Polymorphic Discriminator
   */
  type: "Trigger";
  /**
   * Trigger description.
   */
  description?: string;
  /**
   * Indicates if trigger is running or not. Updated when Start/Stop APIs are called on the
   * Trigger. Possible values include: 'Started', 'Stopped', 'Disabled'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly runtimeState?: TriggerRuntimeState;
  /**
   * List of tags that can be used for describing the trigger.
   */
  annotations?: any[];
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Trigger resource type.
 */
export interface TriggerResource extends SubResource {
  /**
   * Properties of the trigger.
   */
  properties: TriggerUnion;
}

/**
 * A query of triggers.
 */
export interface TriggerQueryResponse {
  /**
   * List of triggers.
   */
  value: TriggerResource[];
  /**
   * The continuation token for getting the next page of results, if any remaining results exist,
   * null otherwise.
   */
  continuationToken?: string;
}

/**
 * Response body with a run identifier.
 */
export interface CreateRunResponse {
  /**
   * Identifier of a run.
   */
  runId: string;
}

/**
 * Defines the response of a trigger subscription operation.
 */
export interface TriggerSubscriptionOperationStatus {
  /**
   * Trigger name.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly triggerName?: string;
  /**
   * Event Subscription Status. Possible values include: 'Enabled', 'Provisioning',
   * 'Deprovisioning', 'Disabled', 'Unknown'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly status?: EventSubscriptionStatus;
}

/**
 * Factory's VSTS repo information.
 */
export interface FactoryVSTSConfiguration {
  /**
   * Polymorphic Discriminator
   */
  type: "FactoryVSTSConfiguration";
  /**
   * Account name.
   */
  accountName: string;
  /**
   * Repository name.
   */
  repositoryName: string;
  /**
   * Collaboration branch.
   */
  collaborationBranch: string;
  /**
   * Root folder.
   */
  rootFolder: string;
  /**
   * Last commit id.
   */
  lastCommitId?: string;
  /**
   * VSTS project name.
   */
  projectName: string;
  /**
   * VSTS tenant id.
   */
  tenantId?: string;
}

/**
 * Factory's GitHub repo information.
 */
export interface FactoryGitHubConfiguration {
  /**
   * Polymorphic Discriminator
   */
  type: "FactoryGitHubConfiguration";
  /**
   * Account name.
   */
  accountName: string;
  /**
   * Repository name.
   */
  repositoryName: string;
  /**
   * Collaboration branch.
   */
  collaborationBranch: string;
  /**
   * Root folder.
   */
  rootFolder: string;
  /**
   * Last commit id.
   */
  lastCommitId?: string;
  /**
   * GitHub Enterprise host name. For example: https://github.mydomain.com
   */
  hostName?: string;
}

/**
 * Factory's git repo information.
 */
export interface FactoryRepoUpdate {
  /**
   * The factory resource id.
   */
  factoryResourceId?: string;
  /**
   * Git repo information of the factory.
   */
  repoConfiguration?: FactoryRepoConfigurationUnion;
}

/**
 * Get GitHub access token request definition.
 */
export interface GitHubAccessTokenRequest {
  /**
   * GitHub access code.
   */
  gitHubAccessCode: string;
  /**
   * GitHub application client ID.
   */
  gitHubClientId?: string;
  /**
   * GitHub access token base URL.
   */
  gitHubAccessTokenBaseUrl: string;
}

/**
 * Get GitHub access token response definition.
 */
export interface GitHubAccessTokenResponse {
  /**
   * GitHub access token.
   */
  gitHubAccessToken?: string;
}

/**
 * Get Data Plane read only token request definition.
 */
export interface UserAccessPolicy {
  /**
   * The string with permissions for Data Plane access. Currently only 'r' is supported which
   * grants read only access.
   */
  permissions?: string;
  /**
   * The resource path to get access relative to factory. Currently only empty string is supported
   * which corresponds to the factory resource.
   */
  accessResourcePath?: string;
  /**
   * The name of the profile. Currently only the default is supported. The default value is
   * DefaultProfile.
   */
  profileName?: string;
  /**
   * Start time for the token. If not specified the current time will be used.
   */
  startTime?: string;
  /**
   * Expiration time for the token. Maximum duration for the token is eight hours and by default
   * the token will expire in eight hours.
   */
  expireTime?: string;
}

/**
 * Get Data Plane read only token response definition.
 */
export interface AccessPolicyResponse {
  /**
   * The user access policy.
   */
  policy?: UserAccessPolicy;
  /**
   * Data Plane read only access token.
   */
  accessToken?: string;
  /**
   * Data Plane service base URL.
   */
  dataPlaneUrl?: string;
}

/**
 * Pipeline reference type.
 */
export interface PipelineReference {
  /**
   * Reference pipeline name.
   */
  referenceName: string;
  /**
   * Reference name.
   */
  name?: string;
}

/**
 * Pipeline that needs to be triggered with the given parameters.
 */
export interface TriggerPipelineReference {
  /**
   * Pipeline reference.
   */
  pipelineReference?: PipelineReference;
  /**
   * Pipeline parameters.
   */
  parameters?: { [propertyName: string]: any };
}

/**
 * Parameters for updating a factory resource.
 */
export interface FactoryUpdateParameters {
  /**
   * The resource tags.
   */
  tags?: { [propertyName: string]: string };
  /**
   * Managed service identity of the factory.
   */
  identity?: FactoryIdentity;
}

/**
 * Dataset reference type.
 */
export interface DatasetReference {
  /**
   * Reference dataset name.
   */
  referenceName: string;
  /**
   * Arguments for dataset.
   */
  parameters?: { [propertyName: string]: any };
}

/**
 * Dataset debug resource.
 */
export interface DatasetDebugResource extends SubResourceDebugResource {
  /**
   * Dataset properties.
   */
  properties: DatasetUnion;
}

/**
 * Linked service debug resource.
 */
export interface LinkedServiceDebugResource extends SubResourceDebugResource {
  /**
   * Properties of linked service.
   */
  properties: LinkedServiceUnion;
}

/**
 * Query parameters for triggers.
 */
export interface TriggerFilterParameters {
  /**
   * The continuation token for getting the next page of results. Null for first page.
   */
  continuationToken?: string;
  /**
   * The name of the parent TumblingWindowTrigger to get the child rerun triggers
   */
  parentTriggerName?: string;
}

/**
 * Query filter option for listing runs.
 */
export interface RunQueryFilter {
  /**
   * Parameter name to be used for filter. The allowed operands to query pipeline runs are
   * PipelineName, RunStart, RunEnd and Status; to query activity runs are ActivityName,
   * ActivityRunStart, ActivityRunEnd, ActivityType and Status, and to query trigger runs are
   * TriggerName, TriggerRunTimestamp and Status. Possible values include: 'PipelineName',
   * 'Status', 'RunStart', 'RunEnd', 'ActivityName', 'ActivityRunStart', 'ActivityRunEnd',
   * 'ActivityType', 'TriggerName', 'TriggerRunTimestamp', 'RunGroupId', 'LatestOnly'
   */
  operand: RunQueryFilterOperand;
  /**
   * Operator to be used for filter. Possible values include: 'Equals', 'NotEquals', 'In', 'NotIn'
   */
  operator: RunQueryFilterOperator;
  /**
   * List of filter values.
   */
  values: string[];
}

/**
 * An object to provide order by options for listing runs.
 */
export interface RunQueryOrderBy {
  /**
   * Parameter name to be used for order by. The allowed parameters to order by for pipeline runs
   * are PipelineName, RunStart, RunEnd and Status; for activity runs are ActivityName,
   * ActivityRunStart, ActivityRunEnd and Status; for trigger runs are TriggerName,
   * TriggerRunTimestamp and Status. Possible values include: 'RunStart', 'RunEnd', 'PipelineName',
   * 'Status', 'ActivityName', 'ActivityRunStart', 'ActivityRunEnd', 'TriggerName',
   * 'TriggerRunTimestamp'
   */
  orderBy: RunQueryOrderByField;
  /**
   * Sorting order of the parameter. Possible values include: 'ASC', 'DESC'
   */
  order: RunQueryOrder;
}

/**
 * Query parameters for listing runs.
 */
export interface RunFilterParameters {
  /**
   * The continuation token for getting the next page of results. Null for first page.
   */
  continuationToken?: string;
  /**
   * The time at or after which the run event was updated in 'ISO 8601' format.
   */
  lastUpdatedAfter: Date;
  /**
   * The time at or before which the run event was updated in 'ISO 8601' format.
   */
  lastUpdatedBefore: Date;
  /**
   * List of filters.
   */
  filters?: RunQueryFilter[];
  /**
   * List of OrderBy option.
   */
  orderBy?: RunQueryOrderBy[];
}

/**
 * Provides entity name and id that started the pipeline run.
 */
export interface PipelineRunInvokedBy {
  /**
   * Name of the entity that started the pipeline run.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly name?: string;
  /**
   * The ID of the entity that started the run.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly id?: string;
  /**
   * The type of the entity that started the run.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly invokedByType?: string;
}

/**
 * Information about a pipeline run.
 */
export interface PipelineRun {
  /**
   * Identifier of a run.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly runId?: string;
  /**
   * Identifier that correlates all the recovery runs of a pipeline run.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly runGroupId?: string;
  /**
   * Indicates if the recovered pipeline run is the latest in its group.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly isLatest?: boolean;
  /**
   * The pipeline name.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly pipelineName?: string;
  /**
   * The full or partial list of parameter name, value pair used in the pipeline run.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly parameters?: { [propertyName: string]: string };
  /**
   * Run dimensions emitted by Pipeline run.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly runDimensions?: { [propertyName: string]: string };
  /**
   * Entity that started the pipeline run.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly invokedBy?: PipelineRunInvokedBy;
  /**
   * The last updated timestamp for the pipeline run event in ISO8601 format.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly lastUpdated?: Date;
  /**
   * The start time of a pipeline run in ISO8601 format.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly runStart?: Date;
  /**
   * The end time of a pipeline run in ISO8601 format.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly runEnd?: Date;
  /**
   * The duration of a pipeline run.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly durationInMs?: number;
  /**
   * The status of a pipeline run.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly status?: string;
  /**
   * The message from a pipeline run.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly message?: string;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * A list pipeline runs.
 */
export interface PipelineRunsQueryResponse {
  /**
   * List of pipeline runs.
   */
  value: PipelineRun[];
  /**
   * The continuation token for getting the next page of results, if any remaining results exist,
   * null otherwise.
   */
  continuationToken?: string;
}

/**
 * Information about an activity run in a pipeline.
 */
export interface ActivityRun {
  /**
   * The name of the pipeline.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly pipelineName?: string;
  /**
   * The id of the pipeline run.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly pipelineRunId?: string;
  /**
   * The name of the activity.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly activityName?: string;
  /**
   * The type of the activity.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly activityType?: string;
  /**
   * The id of the activity run.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly activityRunId?: string;
  /**
   * The name of the compute linked service.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly linkedServiceName?: string;
  /**
   * The status of the activity run.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly status?: string;
  /**
   * The start time of the activity run in 'ISO 8601' format.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly activityRunStart?: Date;
  /**
   * The end time of the activity run in 'ISO 8601' format.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly activityRunEnd?: Date;
  /**
   * The duration of the activity run.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly durationInMs?: number;
  /**
   * The input for the activity.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly input?: any;
  /**
   * The output for the activity.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly output?: any;
  /**
   * The error if any from the activity run.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly error?: any;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * A list activity runs.
 */
export interface ActivityRunsQueryResponse {
  /**
   * List of activity runs.
   */
  value: ActivityRun[];
  /**
   * The continuation token for getting the next page of results, if any remaining results exist,
   * null otherwise.
   */
  continuationToken?: string;
}

/**
 * Trigger runs.
 */
export interface TriggerRun {
  /**
   * Trigger run id.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly triggerRunId?: string;
  /**
   * Trigger name.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly triggerName?: string;
  /**
   * Trigger type.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly triggerType?: string;
  /**
   * Trigger run start time.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly triggerRunTimestamp?: Date;
  /**
   * Trigger run status. Possible values include: 'Succeeded', 'Failed', 'Inprogress'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly status?: TriggerRunStatus;
  /**
   * Trigger error message.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly message?: string;
  /**
   * List of property name and value related to trigger run. Name, value pair depends on type of
   * trigger.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly properties?: { [propertyName: string]: string };
  /**
   * List of pipeline name and run Id triggered by the trigger run.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly triggeredPipelines?: { [propertyName: string]: string };
  /**
   * Run dimension for which trigger was fired.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly runDimension?: { [propertyName: string]: string };
  /**
   * Status of the upstream pipelines.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly dependencyStatus?: { [propertyName: string]: any };
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * A list of trigger runs.
 */
export interface TriggerRunsQueryResponse {
  /**
   * List of trigger runs.
   */
  value: TriggerRun[];
  /**
   * The continuation token for getting the next page of results, if any remaining results exist,
   * null otherwise.
   */
  continuationToken?: string;
}

/**
 * Metadata associated with the operation.
 */
export interface OperationDisplay {
  /**
   * The description of the operation.
   */
  description?: string;
  /**
   * The name of the provider.
   */
  provider?: string;
  /**
   * The name of the resource type on which the operation is performed.
   */
  resource?: string;
  /**
   * The type of operation: get, read, delete, etc.
   */
  operation?: string;
}

/**
 * Details about an operation related to logs.
 */
export interface OperationLogSpecification {
  /**
   * The name of the log category.
   */
  name?: string;
  /**
   * Localized display name.
   */
  displayName?: string;
  /**
   * Blobs created in the customer storage account, per hour.
   */
  blobDuration?: string;
}

/**
 * Defines how often data for a metric becomes available.
 */
export interface OperationMetricAvailability {
  /**
   * The granularity for the metric.
   */
  timeGrain?: string;
  /**
   * Blob created in the customer storage account, per hour.
   */
  blobDuration?: string;
}

/**
 * Defines the metric dimension.
 */
export interface OperationMetricDimension {
  /**
   * The name of the dimension for the metric.
   */
  name?: string;
  /**
   * The display name of the metric dimension.
   */
  displayName?: string;
  /**
   * Whether the dimension should be exported to Azure Monitor.
   */
  toBeExportedForShoebox?: boolean;
}

/**
 * Details about an operation related to metrics.
 */
export interface OperationMetricSpecification {
  /**
   * The name of the metric.
   */
  name?: string;
  /**
   * Localized display name of the metric.
   */
  displayName?: string;
  /**
   * The description of the metric.
   */
  displayDescription?: string;
  /**
   * The unit that the metric is measured in.
   */
  unit?: string;
  /**
   * The type of metric aggregation.
   */
  aggregationType?: string;
  /**
   * Whether or not the service is using regional MDM accounts.
   */
  enableRegionalMdmAccount?: string;
  /**
   * The name of the MDM account.
   */
  sourceMdmAccount?: string;
  /**
   * The name of the MDM namespace.
   */
  sourceMdmNamespace?: string;
  /**
   * Defines how often data for metrics becomes available.
   */
  availabilities?: OperationMetricAvailability[];
  /**
   * Defines the metric dimension.
   */
  dimensions?: OperationMetricDimension[];
}

/**
 * Details about a service operation.
 */
export interface OperationServiceSpecification {
  /**
   * Details about operations related to logs.
   */
  logSpecifications?: OperationLogSpecification[];
  /**
   * Details about operations related to metrics.
   */
  metricSpecifications?: OperationMetricSpecification[];
}

/**
 * Azure Data Factory API operation definition.
 */
export interface Operation {
  /**
   * Operation name: {provider}/{resource}/{operation}
   */
  name?: string;
  /**
   * The intended executor of the operation.
   */
  origin?: string;
  /**
   * Metadata associated with the operation.
   */
  display?: OperationDisplay;
  /**
   * Details about a service operation.
   */
  serviceSpecification?: OperationServiceSpecification;
}

/**
 * The request payload of get SSIS object metadata.
 */
export interface GetSsisObjectMetadataRequest {
  /**
   * Metadata path.
   */
  metadataPath?: string;
}

/**
 * The status of the operation.
 */
export interface SsisObjectMetadataStatusResponse {
  /**
   * The status of the operation.
   */
  status?: string;
  /**
   * The operation name.
   */
  name?: string;
  /**
   * The operation properties.
   */
  properties?: string;
  /**
   * The operation error message.
   */
  error?: string;
}

/**
 * The exposure control request.
 */
export interface ExposureControlRequest {
  /**
   * The feature name.
   */
  featureName?: string;
  /**
   * The feature type.
   */
  featureType?: string;
}

/**
 * The exposure control response.
 */
export interface ExposureControlResponse {
  /**
   * The feature name.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly featureName?: string;
  /**
   * The feature value.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly value?: string;
}

/**
 * The folder that this data flow is in. If not specified, Data flow will appear at the root level.
 */
export interface DataFlowFolder {
  /**
   * The name of the folder that this data flow is in.
   */
  name?: string;
}

/**
 * Contains the possible cases for DataFlow.
 */
export type DataFlowUnion = DataFlow | MappingDataFlow;

/**
 * Azure Data Factory nested object which contains a flow with data movements and transformations.
 */
export interface DataFlow {
  /**
   * Polymorphic Discriminator
   */
  type: "DataFlow";
  /**
   * The description of the data flow.
   */
  description?: string;
  /**
   * List of tags that can be used for describing the data flow.
   */
  annotations?: any[];
  /**
   * The folder that this data flow is in. If not specified, Data flow will appear at the root
   * level.
   */
  folder?: DataFlowFolder;
}

/**
 * Data flow resource type.
 */
export interface DataFlowResource extends SubResource {
  /**
   * Data flow properties.
   */
  properties: DataFlowUnion;
}

/**
 * Request body structure for creating data flow debug session.
 */
export interface CreateDataFlowDebugSessionRequest {
  /**
   * Compute type of the cluster. The value will be overwritten by the same setting in integration
   * runtime if provided.
   */
  computeType?: string;
  /**
   * Core count of the cluster. The value will be overwritten by the same setting in integration
   * runtime if provided.
   */
  coreCount?: number;
  /**
   * Time to live setting of the cluster in minutes.
   */
  timeToLive?: number;
  /**
   * Set to use integration runtime setting for data flow debug session.
   */
  integrationRuntime?: IntegrationRuntimeDebugResource;
}

/**
 * Response body structure for creating data flow debug session.
 */
export interface CreateDataFlowDebugSessionResponse {
  /**
   * The state of the debug session.
   */
  status?: string;
  /**
   * The ID of data flow debug session.
   */
  sessionId?: string;
}

/**
 * Definition of data flow source setting for debug.
 */
export interface DataFlowSourceSetting {
  /**
   * The data flow source name.
   */
  sourceName?: string;
  /**
   * Defines the row limit of data flow source in debug.
   */
  rowLimit?: number;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Staging info for execute data flow activity.
 */
export interface DataFlowStagingInfo {
  /**
   * Staging linked service reference.
   */
  linkedService?: LinkedServiceReference;
  /**
   * Folder path for staging blob.
   */
  folderPath?: string;
}

/**
 * Data flow debug resource.
 */
export interface DataFlowDebugResource extends SubResourceDebugResource {
  /**
   * Data flow properties.
   */
  properties: DataFlowUnion;
}

/**
 * Data flow debug settings.
 */
export interface DataFlowDebugPackageDebugSettings {
  /**
   * Source setting for data flow debug.
   */
  sourceSettings?: DataFlowSourceSetting[];
  /**
   * Data flow parameters.
   */
  parameters?: { [propertyName: string]: any };
  /**
   * Parameters for dataset.
   */
  datasetParameters?: any;
}

/**
 * Request body structure for starting data flow debug session.
 */
export interface DataFlowDebugPackage {
  /**
   * The ID of data flow debug session.
   */
  sessionId?: string;
  /**
   * Data flow instance.
   */
  dataFlow?: DataFlowDebugResource;
  /**
   * List of datasets.
   */
  datasets?: DatasetDebugResource[];
  /**
   * List of linked services.
   */
  linkedServices?: LinkedServiceDebugResource[];
  /**
   * Staging info for debug session.
   */
  staging?: DataFlowStagingInfo;
  /**
   * Data flow debug settings.
   */
  debugSettings?: DataFlowDebugPackageDebugSettings;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Response body structure for starting data flow debug session.
 */
export interface AddDataFlowToDebugSessionResponse {
  /**
   * The ID of data flow debug job version.
   */
  jobVersion?: string;
}

/**
 * Request body structure for deleting data flow debug session.
 */
export interface DeleteDataFlowDebugSessionRequest {
  /**
   * The ID of data flow debug session.
   */
  sessionId?: string;
}

/**
 * Structure of command payload.
 */
export interface DataFlowDebugCommandPayload {
  /**
   * The stream name which is used for preview.
   */
  streamName: string;
  /**
   * Row limits for preview response.
   */
  rowLimits?: number;
  /**
   * Array of column names.
   */
  columns?: string[];
  /**
   * The expression which is used for preview.
   */
  expression?: string;
}

/**
 * Request body structure for data flow debug command.
 */
export interface DataFlowDebugCommandRequest {
  /**
   * The ID of data flow debug session.
   */
  sessionId?: string;
  /**
   * The command type. Possible values include: 'executePreviewQuery', 'executeStatisticsQuery',
   * 'executeExpressionQuery'
   */
  command?: DataFlowDebugCommandType;
  /**
   * The command payload object.
   */
  commandPayload?: DataFlowDebugCommandPayload;
}

/**
 * Response body structure of data flow result for data preview, statistics or expression preview.
 */
export interface DataFlowDebugCommandResponse {
  /**
   * The run status of data preview, statistics or expression preview.
   */
  status?: string;
  /**
   * The result data of data preview, statistics or expression preview.
   */
  data?: string;
}

/**
 * Data flow debug session info.
 */
export interface DataFlowDebugSessionInfo {
  /**
   * The name of the data flow.
   */
  dataFlowName?: string;
  /**
   * Compute type of the cluster.
   */
  computeType?: string;
  /**
   * Core count of the cluster.
   */
  coreCount?: number;
  /**
   * Node count of the cluster. (deprecated property)
   */
  nodeCount?: number;
  /**
   * Attached integration runtime name of data flow debug session.
   */
  integrationRuntimeName?: string;
  /**
   * The ID of data flow debug session.
   */
  sessionId?: string;
  /**
   * Start time of data flow debug session.
   */
  startTime?: string;
  /**
   * Compute type of the cluster.
   */
  timeToLiveInMinutes?: number;
  /**
   * Last activity time of data flow debug session.
   */
  lastActivityTime?: string;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Response body structure for get data factory operation status.
 */
export interface GetDataFactoryOperationStatusResponse {
  /**
   * Status of the operation.
   */
  status?: string;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Data flow reference type.
 */
export interface DataFlowReference {
  /**
   * Reference data flow name.
   */
  referenceName: string;
  /**
   * Reference data flow parameters from dataset.
   */
  datasetParameters?: any;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * A data flow transformation.
 */
export interface Transformation {
  /**
   * Transformation name.
   */
  name: string;
  /**
   * Transformation description.
   */
  description?: string;
}

/**
 * Transformation for data flow sink.
 */
export interface DataFlowSink extends Transformation {
  /**
   * Dataset reference.
   */
  dataset?: DatasetReference;
  /**
   * Linked service reference.
   */
  linkedService?: LinkedServiceReference;
  /**
   * Schema linked service reference.
   */
  schemaLinkedService?: LinkedServiceReference;
}

/**
 * Transformation for data flow source.
 */
export interface DataFlowSource extends Transformation {
  /**
   * Dataset reference.
   */
  dataset?: DatasetReference;
  /**
   * Linked service reference.
   */
  linkedService?: LinkedServiceReference;
  /**
   * Schema linked service reference.
   */
  schemaLinkedService?: LinkedServiceReference;
}

/**
 * Mapping data flow.
 */
export interface MappingDataFlow {
  /**
   * Polymorphic Discriminator
   */
  type: "MappingDataFlow";
  /**
   * The description of the data flow.
   */
  description?: string;
  /**
   * List of tags that can be used for describing the data flow.
   */
  annotations?: any[];
  /**
   * The folder that this data flow is in. If not specified, Data flow will appear at the root
   * level.
   */
  folder?: DataFlowFolder;
  /**
   * List of sources in data flow.
   */
  sources?: DataFlowSource[];
  /**
   * List of sinks in data flow.
   */
  sinks?: DataFlowSink[];
  /**
   * List of transformations in data flow.
   */
  transformations?: Transformation[];
  /**
   * DataFlow script.
   */
  script?: string;
}

/**
 * SharePoint Online List linked service.
 */
export interface SharePointOnlineListLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "SharePointOnlineList";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The URL of the SharePoint Online site. For example,
   * https://contoso.sharepoint.com/sites/siteName. Type: string (or Expression with resultType
   * string).
   */
  siteUrl: any;
  /**
   * The tenant ID under which your application resides. You can find it from Azure portal Active
   * Directory overview page. Type: string (or Expression with resultType string).
   */
  tenantId: any;
  /**
   * The application (client) ID of your application registered in Azure Active Directory. Make
   * sure to grant SharePoint site permission to this application. Type: string (or Expression with
   * resultType string).
   */
  servicePrincipalId: any;
  /**
   * The client secret of your application registered in Azure Active Directory. Type: string (or
   * Expression with resultType string).
   */
  servicePrincipalKey: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Snowflake linked service.
 */
export interface SnowflakeLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Snowflake";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The connection string of snowflake. Type: string, SecureString.
   */
  connectionString: any;
  /**
   * The Azure key vault secret reference of password in connection string.
   */
  password?: AzureKeyVaultSecretReference;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Azure Function linked service.
 */
export interface AzureFunctionLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureFunction";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The endpoint of the Azure Function App. URL will be in the format
   * https://<accountName>.azurewebsites.net.
   */
  functionAppUrl: any;
  /**
   * Function or Host key for Azure Function App.
   */
  functionKey?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Azure Data Explorer (Kusto) linked service.
 */
export interface AzureDataExplorerLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureDataExplorer";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The endpoint of Azure Data Explorer (the engine's endpoint). URL will be in the format
   * https://<clusterName>.<regionName>.kusto.windows.net. Type: string (or Expression with
   * resultType string)
   */
  endpoint: any;
  /**
   * The ID of the service principal used to authenticate against Azure Data Explorer. Type: string
   * (or Expression with resultType string).
   */
  servicePrincipalId: any;
  /**
   * The key of the service principal used to authenticate against Kusto.
   */
  servicePrincipalKey: SecretBaseUnion;
  /**
   * Database name for connection. Type: string (or Expression with resultType string).
   */
  database: any;
  /**
   * The name or ID of the tenant to which the service principal belongs. Type: string (or
   * Expression with resultType string).
   */
  tenant: any;
}

/**
 * SAP Table Linked Service.
 */
export interface SapTableLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "SapTable";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * Host name of the SAP instance where the table is located. Type: string (or Expression with
   * resultType string).
   */
  server?: any;
  /**
   * System number of the SAP system where the table is located. (Usually a two-digit decimal
   * number represented as a string.) Type: string (or Expression with resultType string).
   */
  systemNumber?: any;
  /**
   * Client ID of the client on the SAP system where the table is located. (Usually a three-digit
   * decimal number represented as a string) Type: string (or Expression with resultType string).
   */
  clientId?: any;
  /**
   * Language of the SAP system where the table is located. The default value is EN. Type: string
   * (or Expression with resultType string).
   */
  language?: any;
  /**
   * SystemID of the SAP system where the table is located. Type: string (or Expression with
   * resultType string).
   */
  systemId?: any;
  /**
   * Username to access the SAP server where the table is located. Type: string (or Expression with
   * resultType string).
   */
  userName?: any;
  /**
   * Password to access the SAP server where the table is located.
   */
  password?: SecretBaseUnion;
  /**
   * The hostname of the SAP Message Server. Type: string (or Expression with resultType string).
   */
  messageServer?: any;
  /**
   * The service name or port number of the Message Server. Type: string (or Expression with
   * resultType string).
   */
  messageServerService?: any;
  /**
   * SNC activation indicator to access the SAP server where the table is located. Must be either 0
   * (off) or 1 (on). Type: string (or Expression with resultType string).
   */
  sncMode?: any;
  /**
   * Initiator's SNC name to access the SAP server where the table is located. Type: string (or
   * Expression with resultType string).
   */
  sncMyName?: any;
  /**
   * Communication partner's SNC name to access the SAP server where the table is located. Type:
   * string (or Expression with resultType string).
   */
  sncPartnerName?: any;
  /**
   * External security product's library to access the SAP server where the table is located. Type:
   * string (or Expression with resultType string).
   */
  sncLibraryPath?: any;
  /**
   * SNC Quality of Protection. Allowed value include: 1, 2, 3, 8, 9. Type: string (or Expression
   * with resultType string).
   */
  sncQop?: any;
  /**
   * The Logon Group for the SAP System. Type: string (or Expression with resultType string).
   */
  logonGroup?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Google AdWords service linked service.
 */
export interface GoogleAdWordsLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "GoogleAdWords";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The Client customer ID of the AdWords account that you want to fetch report data for.
   */
  clientCustomerID: any;
  /**
   * The developer token associated with the manager account that you use to grant access to the
   * AdWords API.
   */
  developerToken: SecretBaseUnion;
  /**
   * The OAuth 2.0 authentication mechanism used for authentication. ServiceAuthentication can only
   * be used on self-hosted IR. Possible values include: 'ServiceAuthentication',
   * 'UserAuthentication'
   */
  authenticationType: GoogleAdWordsAuthenticationType;
  /**
   * The refresh token obtained from Google for authorizing access to AdWords for
   * UserAuthentication.
   */
  refreshToken?: SecretBaseUnion;
  /**
   * The client id of the google application used to acquire the refresh token. Type: string (or
   * Expression with resultType string).
   */
  clientId?: any;
  /**
   * The client secret of the google application used to acquire the refresh token.
   */
  clientSecret?: SecretBaseUnion;
  /**
   * The service account email ID that is used for ServiceAuthentication and can only be used on
   * self-hosted IR.
   */
  email?: any;
  /**
   * The full path to the .p12 key file that is used to authenticate the service account email
   * address and can only be used on self-hosted IR.
   */
  keyFilePath?: any;
  /**
   * The full path of the .pem file containing trusted CA certificates for verifying the server
   * when connecting over SSL. This property can only be set when using SSL on self-hosted IR. The
   * default value is the cacerts.pem file installed with the IR.
   */
  trustedCertPath?: any;
  /**
   * Specifies whether to use a CA certificate from the system trust store or from a specified PEM
   * file. The default value is false.
   */
  useSystemTrustStore?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Oracle Service Cloud linked service.
 */
export interface OracleServiceCloudLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "OracleServiceCloud";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The URL of the Oracle Service Cloud instance.
   */
  host: any;
  /**
   * The user name that you use to access Oracle Service Cloud server.
   */
  username: any;
  /**
   * The password corresponding to the user name that you provided in the username key.
   */
  password: SecretBaseUnion;
  /**
   * Specifies whether the data source endpoints are encrypted using HTTPS. The default value is
   * true. Type: boolean (or Expression with resultType boolean).
   */
  useEncryptedEndpoints?: any;
  /**
   * Specifies whether to require the host name in the server's certificate to match the host name
   * of the server when connecting over SSL. The default value is true. Type: boolean (or
   * Expression with resultType boolean).
   */
  useHostVerification?: any;
  /**
   * Specifies whether to verify the identity of the server when connecting over SSL. The default
   * value is true. Type: boolean (or Expression with resultType boolean).
   */
  usePeerVerification?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Dynamics AX linked service.
 */
export interface DynamicsAXLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "DynamicsAX";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The Dynamics AX (or Dynamics 365 Finance and Operations) instance OData endpoint.
   */
  url: any;
  /**
   * Specify the application's client ID. Type: string (or Expression with resultType string).
   */
  servicePrincipalId: any;
  /**
   * Specify the application's key. Mark this field as a SecureString to store it securely in Data
   * Factory, or reference a secret stored in Azure Key Vault. Type: string (or Expression with
   * resultType string).
   */
  servicePrincipalKey: SecretBaseUnion;
  /**
   * Specify the tenant information (domain name or tenant ID) under which your application
   * resides. Retrieve it by hovering the mouse in the top-right corner of the Azure portal. Type:
   * string (or Expression with resultType string).
   */
  tenant: any;
  /**
   * Specify the resource you are requesting authorization. Type: string (or Expression with
   * resultType string).
   */
  aadResourceId: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Responsys linked service.
 */
export interface ResponsysLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Responsys";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The endpoint of the Responsys server.
   */
  endpoint: any;
  /**
   * The client ID associated with the Responsys application. Type: string (or Expression with
   * resultType string).
   */
  clientId: any;
  /**
   * The client secret associated with the Responsys application. Type: string (or Expression with
   * resultType string).
   */
  clientSecret?: SecretBaseUnion;
  /**
   * Specifies whether the data source endpoints are encrypted using HTTPS. The default value is
   * true. Type: boolean (or Expression with resultType boolean).
   */
  useEncryptedEndpoints?: any;
  /**
   * Specifies whether to require the host name in the server's certificate to match the host name
   * of the server when connecting over SSL. The default value is true. Type: boolean (or
   * Expression with resultType boolean).
   */
  useHostVerification?: any;
  /**
   * Specifies whether to verify the identity of the server when connecting over SSL. The default
   * value is true. Type: boolean (or Expression with resultType boolean).
   */
  usePeerVerification?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Azure Databricks linked service.
 */
export interface AzureDatabricksLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureDatabricks";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * <REGION>.azuredatabricks.net, domain name of your Databricks deployment. Type: string (or
   * Expression with resultType string).
   */
  domain: any;
  /**
   * Access token for databricks REST API. Refer to
   * https://docs.azuredatabricks.net/api/latest/authentication.html. Type: string (or Expression
   * with resultType string).
   */
  accessToken: SecretBaseUnion;
  /**
   * The id of an existing interactive cluster that will be used for all runs of this activity.
   * Type: string (or Expression with resultType string).
   */
  existingClusterId?: any;
  /**
   * The id of an existing instance pool that will be used for all runs of this activity. Type:
   * string (or Expression with resultType string).
   */
  instancePoolId?: any;
  /**
   * If not using an existing interactive cluster, this specifies the Spark version of a new job
   * cluster or instance pool nodes created for each run of this activity. Required if
   * instancePoolId is specified. Type: string (or Expression with resultType string).
   */
  newClusterVersion?: any;
  /**
   * If not using an existing interactive cluster, this specifies the number of worker nodes to use
   * for the new job cluster or instance pool. For new job clusters, this a string-formatted Int32,
   * like '1' means numOfWorker is 1 or '1:10' means auto-scale from 1 (min) to 10 (max). For
   * instance pools, this is a string-formatted Int32, and can only specify a fixed number of
   * worker nodes, such as '2'. Required if newClusterVersion is specified. Type: string (or
   * Expression with resultType string).
   */
  newClusterNumOfWorker?: any;
  /**
   * The node type of the new job cluster. This property is required if newClusterVersion is
   * specified and instancePoolId is not specified. If instancePoolId is specified, this property
   * is ignored. Type: string (or Expression with resultType string).
   */
  newClusterNodeType?: any;
  /**
   * A set of optional, user-specified Spark configuration key-value pairs.
   */
  newClusterSparkConf?: { [propertyName: string]: any };
  /**
   * A set of optional, user-specified Spark environment variables key-value pairs.
   */
  newClusterSparkEnvVars?: { [propertyName: string]: any };
  /**
   * Additional tags for cluster resources. This property is ignored in instance pool
   * configurations.
   */
  newClusterCustomTags?: { [propertyName: string]: any };
  /**
   * The driver node type for the new job cluster. This property is ignored in instance pool
   * configurations. Type: string (or Expression with resultType string).
   */
  newClusterDriverNodeType?: any;
  /**
   * User-defined initialization scripts for the new cluster. Type: array of strings (or Expression
   * with resultType array of strings).
   */
  newClusterInitScripts?: any;
  /**
   * Enable the elastic disk on the new cluster. This property is now ignored, and takes the
   * default elastic disk behavior in Databricks (elastic disks are always enabled). Type: boolean
   * (or Expression with resultType boolean).
   */
  newClusterEnableElasticDisk?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Azure Data Lake Analytics linked service.
 */
export interface AzureDataLakeAnalyticsLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureDataLakeAnalytics";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The Azure Data Lake Analytics account name. Type: string (or Expression with resultType
   * string).
   */
  accountName: any;
  /**
   * The ID of the application used to authenticate against the Azure Data Lake Analytics account.
   * Type: string (or Expression with resultType string).
   */
  servicePrincipalId?: any;
  /**
   * The Key of the application used to authenticate against the Azure Data Lake Analytics account.
   */
  servicePrincipalKey?: SecretBaseUnion;
  /**
   * The name or ID of the tenant to which the service principal belongs. Type: string (or
   * Expression with resultType string).
   */
  tenant: any;
  /**
   * Data Lake Analytics account subscription ID (if different from Data Factory account). Type:
   * string (or Expression with resultType string).
   */
  subscriptionId?: any;
  /**
   * Data Lake Analytics account resource group name (if different from Data Factory account).
   * Type: string (or Expression with resultType string).
   */
  resourceGroupName?: any;
  /**
   * Azure Data Lake Analytics URI Type: string (or Expression with resultType string).
   */
  dataLakeAnalyticsUri?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Custom script action to run on HDI ondemand cluster once it's up.
 */
export interface ScriptAction {
  /**
   * The user provided name of the script action.
   */
  name: string;
  /**
   * The URI for the script action.
   */
  uri: string;
  /**
   * The node types on which the script action should be executed.
   */
  roles: any;
  /**
   * The parameters for the script action.
   */
  parameters?: string;
}

/**
 * HDInsight ondemand linked service.
 */
export interface HDInsightOnDemandLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "HDInsightOnDemand";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * Number of worker/data nodes in the cluster. Suggestion value: 4. Type: string (or Expression
   * with resultType string).
   */
  clusterSize: any;
  /**
   * The allowed idle time for the on-demand HDInsight cluster. Specifies how long the on-demand
   * HDInsight cluster stays alive after completion of an activity run if there are no other active
   * jobs in the cluster. The minimum value is 5 mins. Type: string (or Expression with resultType
   * string).
   */
  timeToLive: any;
  /**
   * Version of the HDInsight cluster. Type: string (or Expression with resultType string).
   */
  version: any;
  /**
   * Azure Storage linked service to be used by the on-demand cluster for storing and processing
   * data.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * The customers subscription to host the cluster. Type: string (or Expression with resultType
   * string).
   */
  hostSubscriptionId: any;
  /**
   * The service principal id for the hostSubscriptionId. Type: string (or Expression with
   * resultType string).
   */
  servicePrincipalId?: any;
  /**
   * The key for the service principal id.
   */
  servicePrincipalKey?: SecretBaseUnion;
  /**
   * The Tenant id/name to which the service principal belongs. Type: string (or Expression with
   * resultType string).
   */
  tenant: any;
  /**
   * The resource group where the cluster belongs. Type: string (or Expression with resultType
   * string).
   */
  clusterResourceGroup: any;
  /**
   * The prefix of cluster name, postfix will be distinct with timestamp. Type: string (or
   * Expression with resultType string).
   */
  clusterNamePrefix?: any;
  /**
   * The username to access the cluster. Type: string (or Expression with resultType string).
   */
  clusterUserName?: any;
  /**
   * The password to access the cluster.
   */
  clusterPassword?: SecretBaseUnion;
  /**
   * The username to SSH remotely connect to clusters node (for Linux). Type: string (or
   * Expression with resultType string).
   */
  clusterSshUserName?: any;
  /**
   * The password to SSH remotely connect clusters node (for Linux).
   */
  clusterSshPassword?: SecretBaseUnion;
  /**
   * Specifies additional storage accounts for the HDInsight linked service so that the Data
   * Factory service can register them on your behalf.
   */
  additionalLinkedServiceNames?: LinkedServiceReference[];
  /**
   * The name of Azure SQL linked service that point to the HCatalog database. The on-demand
   * HDInsight cluster is created by using the Azure SQL database as the metastore.
   */
  hcatalogLinkedServiceName?: LinkedServiceReference;
  /**
   * The cluster type. Type: string (or Expression with resultType string).
   */
  clusterType?: any;
  /**
   * The version of spark if the cluster type is 'spark'. Type: string (or Expression with
   * resultType string).
   */
  sparkVersion?: any;
  /**
   * Specifies the core configuration parameters (as in core-site.xml) for the HDInsight cluster to
   * be created.
   */
  coreConfiguration?: any;
  /**
   * Specifies the HBase configuration parameters (hbase-site.xml) for the HDInsight cluster.
   */
  hBaseConfiguration?: any;
  /**
   * Specifies the HDFS configuration parameters (hdfs-site.xml) for the HDInsight cluster.
   */
  hdfsConfiguration?: any;
  /**
   * Specifies the hive configuration parameters (hive-site.xml) for the HDInsight cluster.
   */
  hiveConfiguration?: any;
  /**
   * Specifies the MapReduce configuration parameters (mapred-site.xml) for the HDInsight cluster.
   */
  mapReduceConfiguration?: any;
  /**
   * Specifies the Oozie configuration parameters (oozie-site.xml) for the HDInsight cluster.
   */
  oozieConfiguration?: any;
  /**
   * Specifies the Storm configuration parameters (storm-site.xml) for the HDInsight cluster.
   */
  stormConfiguration?: any;
  /**
   * Specifies the Yarn configuration parameters (yarn-site.xml) for the HDInsight cluster.
   */
  yarnConfiguration?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
  /**
   * Specifies the size of the head node for the HDInsight cluster.
   */
  headNodeSize?: any;
  /**
   * Specifies the size of the data node for the HDInsight cluster.
   */
  dataNodeSize?: any;
  /**
   * Specifies the size of the Zoo Keeper node for the HDInsight cluster.
   */
  zookeeperNodeSize?: any;
  /**
   * Custom script actions to run on HDI ondemand cluster once it's up. Please refer to
   * https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-customize-cluster-linux?toc=%2Fen-us%2Fazure%2Fhdinsight%2Fr-server%2FTOC.json&bc=%2Fen-us%2Fazure%2Fbread%2Ftoc.json#understanding-script-actions.
   */
  scriptActions?: ScriptAction[];
  /**
   * The ARM resource ID for the vNet to which the cluster should be joined after creation. Type:
   * string (or Expression with resultType string).
   */
  virtualNetworkId?: any;
  /**
   * The ARM resource ID for the subnet in the vNet. If virtualNetworkId was specified, then this
   * property is required. Type: string (or Expression with resultType string).
   */
  subnetName?: any;
}

/**
 * Salesforce Marketing Cloud linked service.
 */
export interface SalesforceMarketingCloudLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "SalesforceMarketingCloud";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The client ID associated with the Salesforce Marketing Cloud application. Type: string (or
   * Expression with resultType string).
   */
  clientId: any;
  /**
   * The client secret associated with the Salesforce Marketing Cloud application. Type: string (or
   * Expression with resultType string).
   */
  clientSecret?: SecretBaseUnion;
  /**
   * Specifies whether the data source endpoints are encrypted using HTTPS. The default value is
   * true. Type: boolean (or Expression with resultType boolean).
   */
  useEncryptedEndpoints?: any;
  /**
   * Specifies whether to require the host name in the server's certificate to match the host name
   * of the server when connecting over SSL. The default value is true. Type: boolean (or
   * Expression with resultType boolean).
   */
  useHostVerification?: any;
  /**
   * Specifies whether to verify the identity of the server when connecting over SSL. The default
   * value is true. Type: boolean (or Expression with resultType boolean).
   */
  usePeerVerification?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Netezza linked service.
 */
export interface NetezzaLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Netezza";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * An ODBC connection string. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString?: any;
  /**
   * The Azure key vault secret reference of password in connection string.
   */
  pwd?: AzureKeyVaultSecretReference;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Vertica linked service.
 */
export interface VerticaLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Vertica";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * An ODBC connection string. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString?: any;
  /**
   * The Azure key vault secret reference of password in connection string.
   */
  pwd?: AzureKeyVaultSecretReference;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Zoho server linked service.
 */
export interface ZohoLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Zoho";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The endpoint of the Zoho server. (i.e. crm.zoho.com/crm/private)
   */
  endpoint: any;
  /**
   * The access token for Zoho authentication.
   */
  accessToken?: SecretBaseUnion;
  /**
   * Specifies whether the data source endpoints are encrypted using HTTPS. The default value is
   * true.
   */
  useEncryptedEndpoints?: any;
  /**
   * Specifies whether to require the host name in the server's certificate to match the host name
   * of the server when connecting over SSL. The default value is true.
   */
  useHostVerification?: any;
  /**
   * Specifies whether to verify the identity of the server when connecting over SSL. The default
   * value is true.
   */
  usePeerVerification?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Xero Service linked service.
 */
export interface XeroLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Xero";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The endpoint of the Xero server. (i.e. api.xero.com)
   */
  host: any;
  /**
   * The consumer key associated with the Xero application.
   */
  consumerKey?: SecretBaseUnion;
  /**
   * The private key from the .pem file that was generated for your Xero private application. You
   * must include all the text from the .pem file, including the Unix line endings(
   * ).
   */
  privateKey?: SecretBaseUnion;
  /**
   * Specifies whether the data source endpoints are encrypted using HTTPS. The default value is
   * true.
   */
  useEncryptedEndpoints?: any;
  /**
   * Specifies whether to require the host name in the server's certificate to match the host name
   * of the server when connecting over SSL. The default value is true.
   */
  useHostVerification?: any;
  /**
   * Specifies whether to verify the identity of the server when connecting over SSL. The default
   * value is true.
   */
  usePeerVerification?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Square Service linked service.
 */
export interface SquareLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Square";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The URLof the Square instance. (i.e. mystore.mysquare.com)
   */
  host: any;
  /**
   * The client ID associated with your Square application.
   */
  clientId: any;
  /**
   * The client secret associated with your Square application.
   */
  clientSecret?: SecretBaseUnion;
  /**
   * The redirect URL assigned in the Square application dashboard. (i.e. http://localhost:2500)
   */
  redirectUri: any;
  /**
   * Specifies whether the data source endpoints are encrypted using HTTPS. The default value is
   * true.
   */
  useEncryptedEndpoints?: any;
  /**
   * Specifies whether to require the host name in the server's certificate to match the host name
   * of the server when connecting over SSL. The default value is true.
   */
  useHostVerification?: any;
  /**
   * Specifies whether to verify the identity of the server when connecting over SSL. The default
   * value is true.
   */
  usePeerVerification?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Spark Server linked service.
 */
export interface SparkLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Spark";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * IP address or host name of the Spark server
   */
  host: any;
  /**
   * The TCP port that the Spark server uses to listen for client connections.
   */
  port: any;
  /**
   * The type of Spark server. Possible values include: 'SharkServer', 'SharkServer2',
   * 'SparkThriftServer'
   */
  serverType?: SparkServerType;
  /**
   * The transport protocol to use in the Thrift layer. Possible values include: 'Binary', 'SASL',
   * 'HTTP '
   */
  thriftTransportProtocol?: SparkThriftTransportProtocol;
  /**
   * The authentication method used to access the Spark server. Possible values include:
   * 'Anonymous', 'Username', 'UsernameAndPassword', 'WindowsAzureHDInsightService'
   */
  authenticationType: SparkAuthenticationType;
  /**
   * The user name that you use to access Spark Server.
   */
  username?: any;
  /**
   * The password corresponding to the user name that you provided in the Username field
   */
  password?: SecretBaseUnion;
  /**
   * The partial URL corresponding to the Spark server.
   */
  httpPath?: any;
  /**
   * Specifies whether the connections to the server are encrypted using SSL. The default value is
   * false.
   */
  enableSsl?: any;
  /**
   * The full path of the .pem file containing trusted CA certificates for verifying the server
   * when connecting over SSL. This property can only be set when using SSL on self-hosted IR. The
   * default value is the cacerts.pem file installed with the IR.
   */
  trustedCertPath?: any;
  /**
   * Specifies whether to use a CA certificate from the system trust store or from a specified PEM
   * file. The default value is false.
   */
  useSystemTrustStore?: any;
  /**
   * Specifies whether to require a CA-issued SSL certificate name to match the host name of the
   * server when connecting over SSL. The default value is false.
   */
  allowHostNameCNMismatch?: any;
  /**
   * Specifies whether to allow self-signed certificates from the server. The default value is
   * false.
   */
  allowSelfSignedServerCert?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Shopify Service linked service.
 */
export interface ShopifyLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Shopify";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The endpoint of the Shopify server. (i.e. mystore.myshopify.com)
   */
  host: any;
  /**
   * The API access token that can be used to access Shopifys data. The token won't expire if it
   * is offline mode.
   */
  accessToken?: SecretBaseUnion;
  /**
   * Specifies whether the data source endpoints are encrypted using HTTPS. The default value is
   * true.
   */
  useEncryptedEndpoints?: any;
  /**
   * Specifies whether to require the host name in the server's certificate to match the host name
   * of the server when connecting over SSL. The default value is true.
   */
  useHostVerification?: any;
  /**
   * Specifies whether to verify the identity of the server when connecting over SSL. The default
   * value is true.
   */
  usePeerVerification?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * ServiceNow server linked service.
 */
export interface ServiceNowLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "ServiceNow";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The endpoint of the ServiceNow server. (i.e. <instance>.service-now.com)
   */
  endpoint: any;
  /**
   * The authentication type to use. Possible values include: 'Basic', 'OAuth2'
   */
  authenticationType: ServiceNowAuthenticationType;
  /**
   * The user name used to connect to the ServiceNow server for Basic and OAuth2 authentication.
   */
  username?: any;
  /**
   * The password corresponding to the user name for Basic and OAuth2 authentication.
   */
  password?: SecretBaseUnion;
  /**
   * The client id for OAuth2 authentication.
   */
  clientId?: any;
  /**
   * The client secret for OAuth2 authentication.
   */
  clientSecret?: SecretBaseUnion;
  /**
   * Specifies whether the data source endpoints are encrypted using HTTPS. The default value is
   * true.
   */
  useEncryptedEndpoints?: any;
  /**
   * Specifies whether to require the host name in the server's certificate to match the host name
   * of the server when connecting over SSL. The default value is true.
   */
  useHostVerification?: any;
  /**
   * Specifies whether to verify the identity of the server when connecting over SSL. The default
   * value is true.
   */
  usePeerVerification?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * QuickBooks server linked service.
 */
export interface QuickBooksLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "QuickBooks";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The endpoint of the QuickBooks server. (i.e. quickbooks.api.intuit.com)
   */
  endpoint: any;
  /**
   * The company ID of the QuickBooks company to authorize.
   */
  companyId: any;
  /**
   * The consumer key for OAuth 1.0 authentication.
   */
  consumerKey: any;
  /**
   * The consumer secret for OAuth 1.0 authentication.
   */
  consumerSecret: SecretBaseUnion;
  /**
   * The access token for OAuth 1.0 authentication.
   */
  accessToken: SecretBaseUnion;
  /**
   * The access token secret for OAuth 1.0 authentication.
   */
  accessTokenSecret: SecretBaseUnion;
  /**
   * Specifies whether the data source endpoints are encrypted using HTTPS. The default value is
   * true.
   */
  useEncryptedEndpoints?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Presto server linked service.
 */
export interface PrestoLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Presto";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The IP address or host name of the Presto server. (i.e. 192.168.222.160)
   */
  host: any;
  /**
   * The version of the Presto server. (i.e. 0.148-t)
   */
  serverVersion: any;
  /**
   * The catalog context for all request against the server.
   */
  catalog: any;
  /**
   * The TCP port that the Presto server uses to listen for client connections. The default value
   * is 8080.
   */
  port?: any;
  /**
   * The authentication mechanism used to connect to the Presto server. Possible values include:
   * 'Anonymous', 'LDAP'
   */
  authenticationType: PrestoAuthenticationType;
  /**
   * The user name used to connect to the Presto server.
   */
  username?: any;
  /**
   * The password corresponding to the user name.
   */
  password?: SecretBaseUnion;
  /**
   * Specifies whether the connections to the server are encrypted using SSL. The default value is
   * false.
   */
  enableSsl?: any;
  /**
   * The full path of the .pem file containing trusted CA certificates for verifying the server
   * when connecting over SSL. This property can only be set when using SSL on self-hosted IR. The
   * default value is the cacerts.pem file installed with the IR.
   */
  trustedCertPath?: any;
  /**
   * Specifies whether to use a CA certificate from the system trust store or from a specified PEM
   * file. The default value is false.
   */
  useSystemTrustStore?: any;
  /**
   * Specifies whether to require a CA-issued SSL certificate name to match the host name of the
   * server when connecting over SSL. The default value is false.
   */
  allowHostNameCNMismatch?: any;
  /**
   * Specifies whether to allow self-signed certificates from the server. The default value is
   * false.
   */
  allowSelfSignedServerCert?: any;
  /**
   * The local time zone used by the connection. Valid values for this option are specified in the
   * IANA Time Zone Database. The default value is the system time zone.
   */
  timeZoneID?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Phoenix server linked service.
 */
export interface PhoenixLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Phoenix";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The IP address or host name of the Phoenix server. (i.e. 192.168.222.160)
   */
  host: any;
  /**
   * The TCP port that the Phoenix server uses to listen for client connections. The default value
   * is 8765.
   */
  port?: any;
  /**
   * The partial URL corresponding to the Phoenix server. (i.e. /gateway/sandbox/phoenix/version).
   * The default value is hbasephoenix if using WindowsAzureHDInsightService.
   */
  httpPath?: any;
  /**
   * The authentication mechanism used to connect to the Phoenix server. Possible values include:
   * 'Anonymous', 'UsernameAndPassword', 'WindowsAzureHDInsightService'
   */
  authenticationType: PhoenixAuthenticationType;
  /**
   * The user name used to connect to the Phoenix server.
   */
  username?: any;
  /**
   * The password corresponding to the user name.
   */
  password?: SecretBaseUnion;
  /**
   * Specifies whether the connections to the server are encrypted using SSL. The default value is
   * false.
   */
  enableSsl?: any;
  /**
   * The full path of the .pem file containing trusted CA certificates for verifying the server
   * when connecting over SSL. This property can only be set when using SSL on self-hosted IR. The
   * default value is the cacerts.pem file installed with the IR.
   */
  trustedCertPath?: any;
  /**
   * Specifies whether to use a CA certificate from the system trust store or from a specified PEM
   * file. The default value is false.
   */
  useSystemTrustStore?: any;
  /**
   * Specifies whether to require a CA-issued SSL certificate name to match the host name of the
   * server when connecting over SSL. The default value is false.
   */
  allowHostNameCNMismatch?: any;
  /**
   * Specifies whether to allow self-signed certificates from the server. The default value is
   * false.
   */
  allowSelfSignedServerCert?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Paypal Service linked service.
 */
export interface PaypalLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Paypal";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The URLof the PayPal instance. (i.e. api.sandbox.paypal.com)
   */
  host: any;
  /**
   * The client ID associated with your PayPal application.
   */
  clientId: any;
  /**
   * The client secret associated with your PayPal application.
   */
  clientSecret?: SecretBaseUnion;
  /**
   * Specifies whether the data source endpoints are encrypted using HTTPS. The default value is
   * true.
   */
  useEncryptedEndpoints?: any;
  /**
   * Specifies whether to require the host name in the server's certificate to match the host name
   * of the server when connecting over SSL. The default value is true.
   */
  useHostVerification?: any;
  /**
   * Specifies whether to verify the identity of the server when connecting over SSL. The default
   * value is true.
   */
  usePeerVerification?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Marketo server linked service.
 */
export interface MarketoLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Marketo";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The endpoint of the Marketo server. (i.e. 123-ABC-321.mktorest.com)
   */
  endpoint: any;
  /**
   * The client Id of your Marketo service.
   */
  clientId: any;
  /**
   * The client secret of your Marketo service.
   */
  clientSecret?: SecretBaseUnion;
  /**
   * Specifies whether the data source endpoints are encrypted using HTTPS. The default value is
   * true.
   */
  useEncryptedEndpoints?: any;
  /**
   * Specifies whether to require the host name in the server's certificate to match the host name
   * of the server when connecting over SSL. The default value is true.
   */
  useHostVerification?: any;
  /**
   * Specifies whether to verify the identity of the server when connecting over SSL. The default
   * value is true.
   */
  usePeerVerification?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Azure Database for MariaDB linked service.
 */
export interface AzureMariaDBLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureMariaDB";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * An ODBC connection string. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString?: any;
  /**
   * The Azure key vault secret reference of password in connection string.
   */
  pwd?: AzureKeyVaultSecretReference;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * MariaDB server linked service.
 */
export interface MariaDBLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "MariaDB";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * An ODBC connection string. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString?: any;
  /**
   * The Azure key vault secret reference of password in connection string.
   */
  pwd?: AzureKeyVaultSecretReference;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Magento server linked service.
 */
export interface MagentoLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Magento";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The URL of the Magento instance. (i.e. 192.168.222.110/magento3)
   */
  host: any;
  /**
   * The access token from Magento.
   */
  accessToken?: SecretBaseUnion;
  /**
   * Specifies whether the data source endpoints are encrypted using HTTPS. The default value is
   * true.
   */
  useEncryptedEndpoints?: any;
  /**
   * Specifies whether to require the host name in the server's certificate to match the host name
   * of the server when connecting over SSL. The default value is true.
   */
  useHostVerification?: any;
  /**
   * Specifies whether to verify the identity of the server when connecting over SSL. The default
   * value is true.
   */
  usePeerVerification?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Jira Service linked service.
 */
export interface JiraLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Jira";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The IP address or host name of the Jira service. (e.g. jira.example.com)
   */
  host: any;
  /**
   * The TCP port that the Jira server uses to listen for client connections. The default value is
   * 443 if connecting through HTTPS, or 8080 if connecting through HTTP.
   */
  port?: any;
  /**
   * The user name that you use to access Jira Service.
   */
  username: any;
  /**
   * The password corresponding to the user name that you provided in the username field.
   */
  password?: SecretBaseUnion;
  /**
   * Specifies whether the data source endpoints are encrypted using HTTPS. The default value is
   * true.
   */
  useEncryptedEndpoints?: any;
  /**
   * Specifies whether to require the host name in the server's certificate to match the host name
   * of the server when connecting over SSL. The default value is true.
   */
  useHostVerification?: any;
  /**
   * Specifies whether to verify the identity of the server when connecting over SSL. The default
   * value is true.
   */
  usePeerVerification?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Impala server linked service.
 */
export interface ImpalaLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Impala";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The IP address or host name of the Impala server. (i.e. 192.168.222.160)
   */
  host: any;
  /**
   * The TCP port that the Impala server uses to listen for client connections. The default value
   * is 21050.
   */
  port?: any;
  /**
   * The authentication type to use. Possible values include: 'Anonymous', 'SASLUsername',
   * 'UsernameAndPassword'
   */
  authenticationType: ImpalaAuthenticationType;
  /**
   * The user name used to access the Impala server. The default value is anonymous when using
   * SASLUsername.
   */
  username?: any;
  /**
   * The password corresponding to the user name when using UsernameAndPassword.
   */
  password?: SecretBaseUnion;
  /**
   * Specifies whether the connections to the server are encrypted using SSL. The default value is
   * false.
   */
  enableSsl?: any;
  /**
   * The full path of the .pem file containing trusted CA certificates for verifying the server
   * when connecting over SSL. This property can only be set when using SSL on self-hosted IR. The
   * default value is the cacerts.pem file installed with the IR.
   */
  trustedCertPath?: any;
  /**
   * Specifies whether to use a CA certificate from the system trust store or from a specified PEM
   * file. The default value is false.
   */
  useSystemTrustStore?: any;
  /**
   * Specifies whether to require a CA-issued SSL certificate name to match the host name of the
   * server when connecting over SSL. The default value is false.
   */
  allowHostNameCNMismatch?: any;
  /**
   * Specifies whether to allow self-signed certificates from the server. The default value is
   * false.
   */
  allowSelfSignedServerCert?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Hubspot Service linked service.
 */
export interface HubspotLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Hubspot";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The client ID associated with your Hubspot application.
   */
  clientId: any;
  /**
   * The client secret associated with your Hubspot application.
   */
  clientSecret?: SecretBaseUnion;
  /**
   * The access token obtained when initiallyauthenticatingyourOAuth integration.
   */
  accessToken?: SecretBaseUnion;
  /**
   * The refresh token obtained when initiallyauthenticatingyourOAuth integration.
   */
  refreshToken?: SecretBaseUnion;
  /**
   * Specifies whether the data source endpoints are encrypted using HTTPS. The default value is
   * true.
   */
  useEncryptedEndpoints?: any;
  /**
   * Specifies whether to require the host name in the server's certificate to match the host name
   * of the server when connecting over SSL. The default value is true.
   */
  useHostVerification?: any;
  /**
   * Specifies whether to verify the identity of the server when connecting over SSL. The default
   * value is true.
   */
  usePeerVerification?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Hive Server linked service.
 */
export interface HiveLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Hive";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * IP address or host name of the Hive server, separated by ';' for multiple hosts (only when
   * serviceDiscoveryMode is enable).
   */
  host: any;
  /**
   * The TCP port that the Hive server uses to listen for client connections.
   */
  port?: any;
  /**
   * The type of Hive server. Possible values include: 'HiveServer1', 'HiveServer2',
   * 'HiveThriftServer'
   */
  serverType?: HiveServerType;
  /**
   * The transport protocol to use in the Thrift layer. Possible values include: 'Binary', 'SASL',
   * 'HTTP '
   */
  thriftTransportProtocol?: HiveThriftTransportProtocol;
  /**
   * The authentication method used to access the Hive server. Possible values include:
   * 'Anonymous', 'Username', 'UsernameAndPassword', 'WindowsAzureHDInsightService'
   */
  authenticationType: HiveAuthenticationType;
  /**
   * true to indicate using the ZooKeeper service, false not.
   */
  serviceDiscoveryMode?: any;
  /**
   * The namespace on ZooKeeper under which Hive Server 2 nodes are added.
   */
  zooKeeperNameSpace?: any;
  /**
   * Specifies whether the driver uses native HiveQL queries,or converts them into an equivalent
   * form in HiveQL.
   */
  useNativeQuery?: any;
  /**
   * The user name that you use to access Hive Server.
   */
  username?: any;
  /**
   * The password corresponding to the user name that you provided in the Username field
   */
  password?: SecretBaseUnion;
  /**
   * The partial URL corresponding to the Hive server.
   */
  httpPath?: any;
  /**
   * Specifies whether the connections to the server are encrypted using SSL. The default value is
   * false.
   */
  enableSsl?: any;
  /**
   * The full path of the .pem file containing trusted CA certificates for verifying the server
   * when connecting over SSL. This property can only be set when using SSL on self-hosted IR. The
   * default value is the cacerts.pem file installed with the IR.
   */
  trustedCertPath?: any;
  /**
   * Specifies whether to use a CA certificate from the system trust store or from a specified PEM
   * file. The default value is false.
   */
  useSystemTrustStore?: any;
  /**
   * Specifies whether to require a CA-issued SSL certificate name to match the host name of the
   * server when connecting over SSL. The default value is false.
   */
  allowHostNameCNMismatch?: any;
  /**
   * Specifies whether to allow self-signed certificates from the server. The default value is
   * false.
   */
  allowSelfSignedServerCert?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * HBase server linked service.
 */
export interface HBaseLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "HBase";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The IP address or host name of the HBase server. (i.e. 192.168.222.160)
   */
  host: any;
  /**
   * The TCP port that the HBase instance uses to listen for client connections. The default value
   * is 9090.
   */
  port?: any;
  /**
   * The partial URL corresponding to the HBase server. (i.e. /gateway/sandbox/hbase/version)
   */
  httpPath?: any;
  /**
   * The authentication mechanism to use to connect to the HBase server. Possible values include:
   * 'Anonymous', 'Basic'
   */
  authenticationType: HBaseAuthenticationType;
  /**
   * The user name used to connect to the HBase instance.
   */
  username?: any;
  /**
   * The password corresponding to the user name.
   */
  password?: SecretBaseUnion;
  /**
   * Specifies whether the connections to the server are encrypted using SSL. The default value is
   * false.
   */
  enableSsl?: any;
  /**
   * The full path of the .pem file containing trusted CA certificates for verifying the server
   * when connecting over SSL. This property can only be set when using SSL on self-hosted IR. The
   * default value is the cacerts.pem file installed with the IR.
   */
  trustedCertPath?: any;
  /**
   * Specifies whether to require a CA-issued SSL certificate name to match the host name of the
   * server when connecting over SSL. The default value is false.
   */
  allowHostNameCNMismatch?: any;
  /**
   * Specifies whether to allow self-signed certificates from the server. The default value is
   * false.
   */
  allowSelfSignedServerCert?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Greenplum Database linked service.
 */
export interface GreenplumLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Greenplum";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * An ODBC connection string. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString?: any;
  /**
   * The Azure key vault secret reference of password in connection string.
   */
  pwd?: AzureKeyVaultSecretReference;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Google BigQuery service linked service.
 */
export interface GoogleBigQueryLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "GoogleBigQuery";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The default BigQuery project to query against.
   */
  project: any;
  /**
   * A comma-separated list of public BigQuery projects to access.
   */
  additionalProjects?: any;
  /**
   * Whether to request access to Google Drive. Allowing Google Drive access enables support for
   * federated tables that combine BigQuery data with data from Google Drive. The default value is
   * false.
   */
  requestGoogleDriveScope?: any;
  /**
   * The OAuth 2.0 authentication mechanism used for authentication. ServiceAuthentication can only
   * be used on self-hosted IR. Possible values include: 'ServiceAuthentication',
   * 'UserAuthentication'
   */
  authenticationType: GoogleBigQueryAuthenticationType;
  /**
   * The refresh token obtained from Google for authorizing access to BigQuery for
   * UserAuthentication.
   */
  refreshToken?: SecretBaseUnion;
  /**
   * The client id of the google application used to acquire the refresh token. Type: string (or
   * Expression with resultType string).
   */
  clientId?: any;
  /**
   * The client secret of the google application used to acquire the refresh token.
   */
  clientSecret?: SecretBaseUnion;
  /**
   * The service account email ID that is used for ServiceAuthentication and can only be used on
   * self-hosted IR.
   */
  email?: any;
  /**
   * The full path to the .p12 key file that is used to authenticate the service account email
   * address and can only be used on self-hosted IR.
   */
  keyFilePath?: any;
  /**
   * The full path of the .pem file containing trusted CA certificates for verifying the server
   * when connecting over SSL. This property can only be set when using SSL on self-hosted IR. The
   * default value is the cacerts.pem file installed with the IR.
   */
  trustedCertPath?: any;
  /**
   * Specifies whether to use a CA certificate from the system trust store or from a specified PEM
   * file. The default value is false.
   */
  useSystemTrustStore?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Eloqua server linked service.
 */
export interface EloquaLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Eloqua";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The endpoint of the Eloqua server. (i.e. eloqua.example.com)
   */
  endpoint: any;
  /**
   * The site name and user name of your Eloqua account in the form: sitename/username. (i.e.
   * Eloqua/Alice)
   */
  username: any;
  /**
   * The password corresponding to the user name.
   */
  password?: SecretBaseUnion;
  /**
   * Specifies whether the data source endpoints are encrypted using HTTPS. The default value is
   * true.
   */
  useEncryptedEndpoints?: any;
  /**
   * Specifies whether to require the host name in the server's certificate to match the host name
   * of the server when connecting over SSL. The default value is true.
   */
  useHostVerification?: any;
  /**
   * Specifies whether to verify the identity of the server when connecting over SSL. The default
   * value is true.
   */
  usePeerVerification?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Drill server linked service.
 */
export interface DrillLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Drill";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * An ODBC connection string. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString?: any;
  /**
   * The Azure key vault secret reference of password in connection string.
   */
  pwd?: AzureKeyVaultSecretReference;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Couchbase server linked service.
 */
export interface CouchbaseLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Couchbase";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * An ODBC connection string. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString?: any;
  /**
   * The Azure key vault secret reference of credString in connection string.
   */
  credString?: AzureKeyVaultSecretReference;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Concur Service linked service.
 */
export interface ConcurLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Concur";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * Application client_id supplied by Concur App Management.
   */
  clientId: any;
  /**
   * The user name that you use to access Concur Service.
   */
  username: any;
  /**
   * The password corresponding to the user name that you provided in the username field.
   */
  password?: SecretBaseUnion;
  /**
   * Specifies whether the data source endpoints are encrypted using HTTPS. The default value is
   * true.
   */
  useEncryptedEndpoints?: any;
  /**
   * Specifies whether to require the host name in the server's certificate to match the host name
   * of the server when connecting over SSL. The default value is true.
   */
  useHostVerification?: any;
  /**
   * Specifies whether to verify the identity of the server when connecting over SSL. The default
   * value is true.
   */
  usePeerVerification?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Azure PostgreSQL linked service.
 */
export interface AzurePostgreSqlLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzurePostgreSql";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * An ODBC connection string. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString?: any;
  /**
   * The Azure key vault secret reference of password in connection string.
   */
  password?: AzureKeyVaultSecretReference;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Amazon Marketplace Web Service linked service.
 */
export interface AmazonMWSLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AmazonMWS";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The endpoint of the Amazon MWS server, (i.e. mws.amazonservices.com)
   */
  endpoint: any;
  /**
   * The Amazon Marketplace ID you want to retrieve data from. To retrieve data from multiple
   * Marketplace IDs, separate them with a comma (,). (i.e. A2EUQ1WTGCTBG2)
   */
  marketplaceID: any;
  /**
   * The Amazon seller ID.
   */
  sellerID: any;
  /**
   * The Amazon MWS authentication token.
   */
  mwsAuthToken?: SecretBaseUnion;
  /**
   * The access key id used to access data.
   */
  accessKeyId: any;
  /**
   * The secret key used to access data.
   */
  secretKey?: SecretBaseUnion;
  /**
   * Specifies whether the data source endpoints are encrypted using HTTPS. The default value is
   * true.
   */
  useEncryptedEndpoints?: any;
  /**
   * Specifies whether to require the host name in the server's certificate to match the host name
   * of the server when connecting over SSL. The default value is true.
   */
  useHostVerification?: any;
  /**
   * Specifies whether to verify the identity of the server when connecting over SSL. The default
   * value is true.
   */
  usePeerVerification?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * SAP HANA Linked Service.
 */
export interface SapHanaLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "SapHana";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * SAP HANA ODBC connection string. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString?: any;
  /**
   * Host name of the SAP HANA server. Type: string (or Expression with resultType string).
   */
  server?: any;
  /**
   * The authentication type to be used to connect to the SAP HANA server. Possible values include:
   * 'Basic', 'Windows'
   */
  authenticationType?: SapHanaAuthenticationType;
  /**
   * Username to access the SAP HANA server. Type: string (or Expression with resultType string).
   */
  userName?: any;
  /**
   * Password to access the SAP HANA server.
   */
  password?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * SAP Business Warehouse Linked Service.
 */
export interface SapBWLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "SapBW";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * Host name of the SAP BW instance. Type: string (or Expression with resultType string).
   */
  server: any;
  /**
   * System number of the BW system. (Usually a two-digit decimal number represented as a string.)
   * Type: string (or Expression with resultType string).
   */
  systemNumber: any;
  /**
   * Client ID of the client on the BW system. (Usually a three-digit decimal number represented as
   * a string) Type: string (or Expression with resultType string).
   */
  clientId: any;
  /**
   * Username to access the SAP BW server. Type: string (or Expression with resultType string).
   */
  userName?: any;
  /**
   * Password to access the SAP BW server.
   */
  password?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * A linked service for an SSH File Transfer Protocol (SFTP) server.
 */
export interface SftpServerLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Sftp";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The SFTP server host name. Type: string (or Expression with resultType string).
   */
  host: any;
  /**
   * The TCP port number that the SFTP server uses to listen for client connections. Default value
   * is 22. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  port?: any;
  /**
   * The authentication type to be used to connect to the FTP server. Possible values include:
   * 'Basic', 'SshPublicKey'
   */
  authenticationType?: SftpAuthenticationType;
  /**
   * The username used to log on to the SFTP server. Type: string (or Expression with resultType
   * string).
   */
  userName?: any;
  /**
   * Password to logon the SFTP server for Basic authentication.
   */
  password?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
  /**
   * The SSH private key file path for SshPublicKey authentication. Only valid for on-premises
   * copy. For on-premises copy with SshPublicKey authentication, either PrivateKeyPath or
   * PrivateKeyContent should be specified. SSH private key should be OpenSSH format. Type: string
   * (or Expression with resultType string).
   */
  privateKeyPath?: any;
  /**
   * Base64 encoded SSH private key content for SshPublicKey authentication. For on-premises copy
   * with SshPublicKey authentication, either PrivateKeyPath or PrivateKeyContent should be
   * specified. SSH private key should be OpenSSH format.
   */
  privateKeyContent?: SecretBaseUnion;
  /**
   * The password to decrypt the SSH private key if the SSH private key is encrypted.
   */
  passPhrase?: SecretBaseUnion;
  /**
   * If true, skip the SSH host key validation. Default value is false. Type: boolean (or
   * Expression with resultType boolean).
   */
  skipHostKeyValidation?: any;
  /**
   * The host key finger-print of the SFTP server. When SkipHostKeyValidation is false,
   * HostKeyFingerprint should be specified. Type: string (or Expression with resultType string).
   */
  hostKeyFingerprint?: any;
}

/**
 * A FTP server Linked Service.
 */
export interface FtpServerLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "FtpServer";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * Host name of the FTP server. Type: string (or Expression with resultType string).
   */
  host: any;
  /**
   * The TCP port number that the FTP server uses to listen for client connections. Default value
   * is 21. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  port?: any;
  /**
   * The authentication type to be used to connect to the FTP server. Possible values include:
   * 'Basic', 'Anonymous'
   */
  authenticationType?: FtpAuthenticationType;
  /**
   * Username to logon the FTP server. Type: string (or Expression with resultType string).
   */
  userName?: any;
  /**
   * Password to logon the FTP server.
   */
  password?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
  /**
   * If true, connect to the FTP server over SSL/TLS channel. Default value is true. Type: boolean
   * (or Expression with resultType boolean).
   */
  enableSsl?: any;
  /**
   * If true, validate the FTP server SSL certificate when connect over SSL/TLS channel. Default
   * value is true. Type: boolean (or Expression with resultType boolean).
   */
  enableServerCertificateValidation?: any;
}

/**
 * Linked service for an HTTP source.
 */
export interface HttpLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "HttpServer";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The base URL of the HTTP endpoint, e.g. http://www.microsoft.com. Type: string (or Expression
   * with resultType string).
   */
  url: any;
  /**
   * The authentication type to be used to connect to the HTTP server. Possible values include:
   * 'Basic', 'Anonymous', 'Digest', 'Windows', 'ClientCertificate'
   */
  authenticationType?: HttpAuthenticationType;
  /**
   * User name for Basic, Digest, or Windows authentication. Type: string (or Expression with
   * resultType string).
   */
  userName?: any;
  /**
   * Password for Basic, Digest, Windows, or ClientCertificate with EmbeddedCertData
   * authentication.
   */
  password?: SecretBaseUnion;
  /**
   * Base64 encoded certificate data for ClientCertificate authentication. For on-premises copy
   * with ClientCertificate authentication, either CertThumbprint or EmbeddedCertData/Password
   * should be specified. Type: string (or Expression with resultType string).
   */
  embeddedCertData?: any;
  /**
   * Thumbprint of certificate for ClientCertificate authentication. Only valid for on-premises
   * copy. For on-premises copy with ClientCertificate authentication, either CertThumbprint or
   * EmbeddedCertData/Password should be specified. Type: string (or Expression with resultType
   * string).
   */
  certThumbprint?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
  /**
   * If true, validate the HTTPS server SSL certificate. Default value is true. Type: boolean (or
   * Expression with resultType boolean).
   */
  enableServerCertificateValidation?: any;
}

/**
 * Linked service for Windows Azure Search Service.
 */
export interface AzureSearchLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureSearch";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * URL for Azure Search service. Type: string (or Expression with resultType string).
   */
  url: any;
  /**
   * Admin Key for Azure Search service
   */
  key?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Custom linked service.
 */
export interface CustomDataSourceLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "CustomDataSource";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * Custom linked service properties.
   */
  typeProperties: any;
}

/**
 * Linked service for Amazon Redshift.
 */
export interface AmazonRedshiftLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AmazonRedshift";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The name of the Amazon Redshift server. Type: string (or Expression with resultType string).
   */
  server: any;
  /**
   * The username of the Amazon Redshift source. Type: string (or Expression with resultType
   * string).
   */
  username?: any;
  /**
   * The password of the Amazon Redshift source.
   */
  password?: SecretBaseUnion;
  /**
   * The database name of the Amazon Redshift source. Type: string (or Expression with resultType
   * string).
   */
  database: any;
  /**
   * The TCP port number that the Amazon Redshift server uses to listen for client connections. The
   * default value is 5439. Type: integer (or Expression with resultType integer).
   */
  port?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Linked service for Amazon S3.
 */
export interface AmazonS3LinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AmazonS3";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The access key identifier of the Amazon S3 Identity and Access Management (IAM) user. Type:
   * string (or Expression with resultType string).
   */
  accessKeyId?: any;
  /**
   * The secret access key of the Amazon S3 Identity and Access Management (IAM) user.
   */
  secretAccessKey?: SecretBaseUnion;
  /**
   * This value specifies the endpoint to access with the S3 Connector. This is an optional
   * property; change it only if you want to try a different service endpoint or want to switch
   * between https and http. Type: string (or Expression with resultType string).
   */
  serviceUrl?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Rest Service linked service.
 */
export interface RestServiceLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "RestService";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The base URL of the REST service.
   */
  url: any;
  /**
   * Whether to validate server side SSL certificate when connecting to the endpoint.The default
   * value is true. Type: boolean (or Expression with resultType boolean).
   */
  enableServerCertificateValidation?: any;
  /**
   * Type of authentication used to connect to the REST service. Possible values include:
   * 'Anonymous', 'Basic', 'AadServicePrincipal', 'ManagedServiceIdentity'
   */
  authenticationType: RestServiceAuthenticationType;
  /**
   * The user name used in Basic authentication type.
   */
  userName?: any;
  /**
   * The password used in Basic authentication type.
   */
  password?: SecretBaseUnion;
  /**
   * The application's client ID used in AadServicePrincipal authentication type.
   */
  servicePrincipalId?: any;
  /**
   * The application's key used in AadServicePrincipal authentication type.
   */
  servicePrincipalKey?: SecretBaseUnion;
  /**
   * The tenant information (domain name or tenant ID) used in AadServicePrincipal authentication
   * type under which your application resides.
   */
  tenant?: any;
  /**
   * The resource you are requesting authorization to use.
   */
  aadResourceId?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * SAP Business Warehouse Open Hub Destination Linked Service.
 */
export interface SapOpenHubLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "SapOpenHub";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * Host name of the SAP BW instance where the open hub destination is located. Type: string (or
   * Expression with resultType string).
   */
  server: any;
  /**
   * System number of the BW system where the open hub destination is located. (Usually a two-digit
   * decimal number represented as a string.) Type: string (or Expression with resultType string).
   */
  systemNumber: any;
  /**
   * Client ID of the client on the BW system where the open hub destination is located. (Usually a
   * three-digit decimal number represented as a string) Type: string (or Expression with
   * resultType string).
   */
  clientId: any;
  /**
   * Language of the BW system where the open hub destination is located. The default value is EN.
   * Type: string (or Expression with resultType string).
   */
  language?: any;
  /**
   * Username to access the SAP BW server where the open hub destination is located. Type: string
   * (or Expression with resultType string).
   */
  userName?: any;
  /**
   * Password to access the SAP BW server where the open hub destination is located.
   */
  password?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Linked service for SAP ERP Central Component(SAP ECC).
 */
export interface SapEccLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "SapEcc";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The URL of SAP ECC OData API. For example,
   * '[https://hostname:port/sap/opu/odata/sap/servicename/]'. Type: string (or Expression with
   * resultType string).
   */
  url: string;
  /**
   * The username for Basic authentication. Type: string (or Expression with resultType string).
   */
  username?: string;
  /**
   * The password for Basic authentication.
   */
  password?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Either encryptedCredential or username/password must
   * be provided. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: string;
}

/**
 * Linked service for SAP Cloud for Customer.
 */
export interface SapCloudForCustomerLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "SapCloudForCustomer";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The URL of SAP Cloud for Customer OData API. For example,
   * '[https://[tenantname].crm.ondemand.com/sap/c4c/odata/v1]'. Type: string (or Expression with
   * resultType string).
   */
  url: any;
  /**
   * The username for Basic authentication. Type: string (or Expression with resultType string).
   */
  username?: any;
  /**
   * The password for Basic authentication.
   */
  password?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Either encryptedCredential or username/password must
   * be provided. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Linked service for Salesforce Service Cloud.
 */
export interface SalesforceServiceCloudLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "SalesforceServiceCloud";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The URL of Salesforce Service Cloud instance. Default is 'https://login.salesforce.com'. To
   * copy data from sandbox, specify 'https://test.salesforce.com'. To copy data from custom
   * domain, specify, for example, 'https://[domain].my.salesforce.com'. Type: string (or
   * Expression with resultType string).
   */
  environmentUrl?: any;
  /**
   * The username for Basic authentication of the Salesforce instance. Type: string (or Expression
   * with resultType string).
   */
  username?: any;
  /**
   * The password for Basic authentication of the Salesforce instance.
   */
  password?: SecretBaseUnion;
  /**
   * The security token is optional to remotely access Salesforce instance.
   */
  securityToken?: SecretBaseUnion;
  /**
   * The Salesforce API version used in ADF. Type: string (or Expression with resultType string).
   */
  apiVersion?: any;
  /**
   * Extended properties appended to the connection string. Type: string (or Expression with
   * resultType string).
   */
  extendedProperties?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Linked service for Salesforce.
 */
export interface SalesforceLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Salesforce";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The URL of Salesforce instance. Default is 'https://login.salesforce.com'. To copy data from
   * sandbox, specify 'https://test.salesforce.com'. To copy data from custom domain, specify, for
   * example, 'https://[domain].my.salesforce.com'. Type: string (or Expression with resultType
   * string).
   */
  environmentUrl?: any;
  /**
   * The username for Basic authentication of the Salesforce instance. Type: string (or Expression
   * with resultType string).
   */
  username?: any;
  /**
   * The password for Basic authentication of the Salesforce instance.
   */
  password?: SecretBaseUnion;
  /**
   * The security token is optional to remotely access Salesforce instance.
   */
  securityToken?: SecretBaseUnion;
  /**
   * The Salesforce API version used in ADF. Type: string (or Expression with resultType string).
   */
  apiVersion?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Office365 linked service.
 */
export interface Office365LinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Office365";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * Azure tenant ID to which the Office 365 account belongs. Type: string (or Expression with
   * resultType string).
   */
  office365TenantId: any;
  /**
   * Specify the tenant information under which your Azure AD web application resides. Type: string
   * (or Expression with resultType string).
   */
  servicePrincipalTenantId: any;
  /**
   * Specify the application's client ID. Type: string (or Expression with resultType string).
   */
  servicePrincipalId: any;
  /**
   * Specify the application's key.
   */
  servicePrincipalKey: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Azure Data Lake Storage Gen2 linked service.
 */
export interface AzureBlobFSLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureBlobFS";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * Endpoint for the Azure Data Lake Storage Gen2 service. Type: string (or Expression with
   * resultType string).
   */
  url: any;
  /**
   * Account key for the Azure Data Lake Storage Gen2 service. Type: string (or Expression with
   * resultType string).
   */
  accountKey?: any;
  /**
   * The ID of the application used to authenticate against the Azure Data Lake Storage Gen2
   * account. Type: string (or Expression with resultType string).
   */
  servicePrincipalId?: any;
  /**
   * The Key of the application used to authenticate against the Azure Data Lake Storage Gen2
   * account.
   */
  servicePrincipalKey?: SecretBaseUnion;
  /**
   * The name or ID of the tenant to which the service principal belongs. Type: string (or
   * Expression with resultType string).
   */
  tenant?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Azure Data Lake Store linked service.
 */
export interface AzureDataLakeStoreLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureDataLakeStore";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * Data Lake Store service URI. Type: string (or Expression with resultType string).
   */
  dataLakeStoreUri: any;
  /**
   * The ID of the application used to authenticate against the Azure Data Lake Store account.
   * Type: string (or Expression with resultType string).
   */
  servicePrincipalId?: any;
  /**
   * The Key of the application used to authenticate against the Azure Data Lake Store account.
   */
  servicePrincipalKey?: SecretBaseUnion;
  /**
   * The name or ID of the tenant to which the service principal belongs. Type: string (or
   * Expression with resultType string).
   */
  tenant?: any;
  /**
   * Data Lake Store account name. Type: string (or Expression with resultType string).
   */
  accountName?: any;
  /**
   * Data Lake Store account subscription ID (if different from Data Factory account). Type: string
   * (or Expression with resultType string).
   */
  subscriptionId?: any;
  /**
   * Data Lake Store account resource group name (if different from Data Factory account). Type:
   * string (or Expression with resultType string).
   */
  resourceGroupName?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Linked service for CosmosDB (MongoDB API) data source.
 */
export interface CosmosDbMongoDbApiLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "CosmosDbMongoDbApi";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The CosmosDB (MongoDB API) connection string. Type: string, SecureString or
   * AzureKeyVaultSecretReference. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString: any;
  /**
   * The name of the CosmosDB (MongoDB API) database that you want to access. Type: string (or
   * Expression with resultType string).
   */
  database: any;
}

/**
 * Linked service for MongoDB data source.
 */
export interface MongoDbV2LinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "MongoDbV2";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The MongoDB connection string. Type: string, SecureString or AzureKeyVaultSecretReference.
   * Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString: any;
  /**
   * The name of the MongoDB database that you want to access. Type: string (or Expression with
   * resultType string).
   */
  database: any;
}

/**
 * Linked service for MongoDb data source.
 */
export interface MongoDbLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "MongoDb";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The IP address or server name of the MongoDB server. Type: string (or Expression with
   * resultType string).
   */
  server: any;
  /**
   * The authentication type to be used to connect to the MongoDB database. Possible values
   * include: 'Basic', 'Anonymous'
   */
  authenticationType?: MongoDbAuthenticationType;
  /**
   * The name of the MongoDB database that you want to access. Type: string (or Expression with
   * resultType string).
   */
  databaseName: any;
  /**
   * Username for authentication. Type: string (or Expression with resultType string).
   */
  username?: any;
  /**
   * Password for authentication.
   */
  password?: SecretBaseUnion;
  /**
   * Database to verify the username and password. Type: string (or Expression with resultType
   * string).
   */
  authSource?: any;
  /**
   * The TCP port number that the MongoDB server uses to listen for client connections. The default
   * value is 27017. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  port?: any;
  /**
   * Specifies whether the connections to the server are encrypted using SSL. The default value is
   * false. Type: boolean (or Expression with resultType boolean).
   */
  enableSsl?: any;
  /**
   * Specifies whether to allow self-signed certificates from the server. The default value is
   * false. Type: boolean (or Expression with resultType boolean).
   */
  allowSelfSignedServerCert?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Linked service for Cassandra data source.
 */
export interface CassandraLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Cassandra";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * Host name for connection. Type: string (or Expression with resultType string).
   */
  host: any;
  /**
   * AuthenticationType to be used for connection. Type: string (or Expression with resultType
   * string).
   */
  authenticationType?: any;
  /**
   * The port for the connection. Type: integer (or Expression with resultType integer).
   */
  port?: any;
  /**
   * Username for authentication. Type: string (or Expression with resultType string).
   */
  username?: any;
  /**
   * Password for authentication.
   */
  password?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Contains the possible cases for WebLinkedServiceTypeProperties.
 */
export type WebLinkedServiceTypePropertiesUnion = WebLinkedServiceTypeProperties | WebClientCertificateAuthentication | WebBasicAuthentication | WebAnonymousAuthentication;

/**
 * Base definition of WebLinkedServiceTypeProperties, this typeProperties is polymorphic based on
 * authenticationType, so not flattened in SDK models.
 */
export interface WebLinkedServiceTypeProperties {
  /**
   * Polymorphic Discriminator
   */
  authenticationType: "WebLinkedServiceTypeProperties";
  /**
   * The URL of the web service endpoint, e.g. http://www.microsoft.com . Type: string (or
   * Expression with resultType string).
   */
  url: any;
}

/**
 * A WebLinkedService that uses client certificate based authentication to communicate with an HTTP
 * endpoint. This scheme follows mutual authentication; the server must also provide valid
 * credentials to the client.
 */
export interface WebClientCertificateAuthentication {
  /**
   * Polymorphic Discriminator
   */
  authenticationType: "ClientCertificate";
  /**
   * The URL of the web service endpoint, e.g. http://www.microsoft.com . Type: string (or
   * Expression with resultType string).
   */
  url: any;
  /**
   * Base64-encoded contents of a PFX file.
   */
  pfx: SecretBaseUnion;
  /**
   * Password for the PFX file.
   */
  password: SecretBaseUnion;
}

/**
 * A WebLinkedService that uses basic authentication to communicate with an HTTP endpoint.
 */
export interface WebBasicAuthentication {
  /**
   * Polymorphic Discriminator
   */
  authenticationType: "Basic";
  /**
   * The URL of the web service endpoint, e.g. http://www.microsoft.com . Type: string (or
   * Expression with resultType string).
   */
  url: any;
  /**
   * User name for Basic authentication. Type: string (or Expression with resultType string).
   */
  username: any;
  /**
   * The password for Basic authentication.
   */
  password: SecretBaseUnion;
}

/**
 * A WebLinkedService that uses anonymous authentication to communicate with an HTTP endpoint.
 */
export interface WebAnonymousAuthentication {
  /**
   * Polymorphic Discriminator
   */
  authenticationType: "Anonymous";
  /**
   * The URL of the web service endpoint, e.g. http://www.microsoft.com . Type: string (or
   * Expression with resultType string).
   */
  url: any;
}

/**
 * Web linked service.
 */
export interface WebLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Web";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * Web linked service properties.
   */
  typeProperties: WebLinkedServiceTypePropertiesUnion;
}

/**
 * Open Data Protocol (OData) linked service.
 */
export interface ODataLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "OData";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The URL of the OData service endpoint. Type: string (or Expression with resultType string).
   */
  url: any;
  /**
   * Type of authentication used to connect to the OData service. Possible values include: 'Basic',
   * 'Anonymous', 'Windows', 'AadServicePrincipal', 'ManagedServiceIdentity'
   */
  authenticationType?: ODataAuthenticationType;
  /**
   * User name of the OData service. Type: string (or Expression with resultType string).
   */
  userName?: any;
  /**
   * Password of the OData service.
   */
  password?: SecretBaseUnion;
  /**
   * Specify the tenant information (domain name or tenant ID) under which your application
   * resides. Type: string (or Expression with resultType string).
   */
  tenant?: any;
  /**
   * Specify the application id of your application registered in Azure Active Directory. Type:
   * string (or Expression with resultType string).
   */
  servicePrincipalId?: any;
  /**
   * Specify the resource you are requesting authorization to use Directory. Type: string (or
   * Expression with resultType string).
   */
  aadResourceId?: any;
  /**
   * Specify the credential type (key or cert) is used for service principal. Possible values
   * include: 'ServicePrincipalKey', 'ServicePrincipalCert'
   */
  aadServicePrincipalCredentialType?: ODataAadServicePrincipalCredentialType;
  /**
   * Specify the secret of your application registered in Azure Active Directory. Type: string (or
   * Expression with resultType string).
   */
  servicePrincipalKey?: SecretBaseUnion;
  /**
   * Specify the base64 encoded certificate of your application registered in Azure Active
   * Directory. Type: string (or Expression with resultType string).
   */
  servicePrincipalEmbeddedCert?: SecretBaseUnion;
  /**
   * Specify the password of your certificate if your certificate has a password and you are using
   * AadServicePrincipal authentication. Type: string (or Expression with resultType string).
   */
  servicePrincipalEmbeddedCertPassword?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Hadoop Distributed File System (HDFS) linked service.
 */
export interface HdfsLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Hdfs";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The URL of the HDFS service endpoint, e.g. http://myhostname:50070/webhdfs/v1 . Type: string
   * (or Expression with resultType string).
   */
  url: any;
  /**
   * Type of authentication used to connect to the HDFS. Possible values are: Anonymous and
   * Windows. Type: string (or Expression with resultType string).
   */
  authenticationType?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
  /**
   * User name for Windows authentication. Type: string (or Expression with resultType string).
   */
  userName?: any;
  /**
   * Password for Windows authentication.
   */
  password?: SecretBaseUnion;
}

/**
 * Microsoft Access linked service.
 */
export interface MicrosoftAccessLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "MicrosoftAccess";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The non-access credential portion of the connection string as well as an optional encrypted
   * credential. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString: any;
  /**
   * Type of authentication used to connect to the Microsoft Access as ODBC data store. Possible
   * values are: Anonymous and Basic. Type: string (or Expression with resultType string).
   */
  authenticationType?: any;
  /**
   * The access credential portion of the connection string specified in driver-specific
   * property-value format.
   */
  credential?: SecretBaseUnion;
  /**
   * User name for Basic authentication. Type: string (or Expression with resultType string).
   */
  userName?: any;
  /**
   * Password for Basic authentication.
   */
  password?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Informix linked service.
 */
export interface InformixLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Informix";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The non-access credential portion of the connection string as well as an optional encrypted
   * credential. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString: any;
  /**
   * Type of authentication used to connect to the Informix as ODBC data store. Possible values
   * are: Anonymous and Basic. Type: string (or Expression with resultType string).
   */
  authenticationType?: any;
  /**
   * The access credential portion of the connection string specified in driver-specific
   * property-value format.
   */
  credential?: SecretBaseUnion;
  /**
   * User name for Basic authentication. Type: string (or Expression with resultType string).
   */
  userName?: any;
  /**
   * Password for Basic authentication.
   */
  password?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Open Database Connectivity (ODBC) linked service.
 */
export interface OdbcLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Odbc";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The non-access credential portion of the connection string as well as an optional encrypted
   * credential. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString: any;
  /**
   * Type of authentication used to connect to the ODBC data store. Possible values are: Anonymous
   * and Basic. Type: string (or Expression with resultType string).
   */
  authenticationType?: any;
  /**
   * The access credential portion of the connection string specified in driver-specific
   * property-value format.
   */
  credential?: SecretBaseUnion;
  /**
   * User name for Basic authentication. Type: string (or Expression with resultType string).
   */
  userName?: any;
  /**
   * Password for Basic authentication.
   */
  password?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Azure ML Service linked service.
 */
export interface AzureMLServiceLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureMLService";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * Azure ML Service workspace subscription ID. Type: string (or Expression with resultType
   * string).
   */
  subscriptionId: any;
  /**
   * Azure ML Service workspace resource group name. Type: string (or Expression with resultType
   * string).
   */
  resourceGroupName: any;
  /**
   * Azure ML Service workspace name. Type: string (or Expression with resultType string).
   */
  mlWorkspaceName: any;
  /**
   * The ID of the service principal used to authenticate against the endpoint of a published Azure
   * ML Service pipeline. Type: string (or Expression with resultType string).
   */
  servicePrincipalId?: any;
  /**
   * The key of the service principal used to authenticate against the endpoint of a published
   * Azure ML Service pipeline.
   */
  servicePrincipalKey?: SecretBaseUnion;
  /**
   * The name or ID of the tenant to which the service principal belongs. Type: string (or
   * Expression with resultType string).
   */
  tenant?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Azure ML Studio Web Service linked service.
 */
export interface AzureMLLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureML";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The Batch Execution REST URL for an Azure ML Studio Web Service endpoint. Type: string (or
   * Expression with resultType string).
   */
  mlEndpoint: any;
  /**
   * The API key for accessing the Azure ML model endpoint.
   */
  apiKey: SecretBaseUnion;
  /**
   * The Update Resource REST URL for an Azure ML Studio Web Service endpoint. Type: string (or
   * Expression with resultType string).
   */
  updateResourceEndpoint?: any;
  /**
   * The ID of the service principal used to authenticate against the ARM-based
   * updateResourceEndpoint of an Azure ML Studio web service. Type: string (or Expression with
   * resultType string).
   */
  servicePrincipalId?: any;
  /**
   * The key of the service principal used to authenticate against the ARM-based
   * updateResourceEndpoint of an Azure ML Studio web service.
   */
  servicePrincipalKey?: SecretBaseUnion;
  /**
   * The name or ID of the tenant to which the service principal belongs. Type: string (or
   * Expression with resultType string).
   */
  tenant?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Linked service for Teradata data source.
 */
export interface TeradataLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Teradata";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * Teradata ODBC connection string. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString?: any;
  /**
   * Server name for connection. Type: string (or Expression with resultType string).
   */
  server?: any;
  /**
   * AuthenticationType to be used for connection. Possible values include: 'Basic', 'Windows'
   */
  authenticationType?: TeradataAuthenticationType;
  /**
   * Username for authentication. Type: string (or Expression with resultType string).
   */
  username?: any;
  /**
   * Password for authentication.
   */
  password?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Linked service for DB2 data source.
 */
export interface Db2LinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Db2";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The connection string. It is mutually exclusive with server, database, authenticationType,
   * userName, packageCollection and certificateCommonName property. Type: string, SecureString or
   * AzureKeyVaultSecretReference.
   */
  connectionString?: any;
  /**
   * Server name for connection. It is mutually exclusive with connectionString property. Type:
   * string (or Expression with resultType string).
   */
  server?: any;
  /**
   * Database name for connection. It is mutually exclusive with connectionString property. Type:
   * string (or Expression with resultType string).
   */
  database?: any;
  /**
   * AuthenticationType to be used for connection. It is mutually exclusive with connectionString
   * property. Possible values include: 'Basic'
   */
  authenticationType?: Db2AuthenticationType;
  /**
   * Username for authentication. It is mutually exclusive with connectionString property. Type:
   * string (or Expression with resultType string).
   */
  username?: any;
  /**
   * Password for authentication.
   */
  password?: SecretBaseUnion;
  /**
   * Under where packages are created when querying database. It is mutually exclusive with
   * connectionString property. Type: string (or Expression with resultType string).
   */
  packageCollection?: any;
  /**
   * Certificate Common Name when TLS is enabled. It is mutually exclusive with connectionString
   * property. Type: string (or Expression with resultType string).
   */
  certificateCommonName?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. It is mutually exclusive with connectionString
   * property. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Linked service for Sybase data source.
 */
export interface SybaseLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Sybase";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * Server name for connection. Type: string (or Expression with resultType string).
   */
  server: any;
  /**
   * Database name for connection. Type: string (or Expression with resultType string).
   */
  database: any;
  /**
   * Schema name for connection. Type: string (or Expression with resultType string).
   */
  schema?: any;
  /**
   * AuthenticationType to be used for connection. Possible values include: 'Basic', 'Windows'
   */
  authenticationType?: SybaseAuthenticationType;
  /**
   * Username for authentication. Type: string (or Expression with resultType string).
   */
  username?: any;
  /**
   * Password for authentication.
   */
  password?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Linked service for PostgreSQL data source.
 */
export interface PostgreSqlLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "PostgreSql";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The connection string.
   */
  connectionString: any;
  /**
   * The Azure key vault secret reference of password in connection string.
   */
  password?: AzureKeyVaultSecretReference;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Linked service for MySQL data source.
 */
export interface MySqlLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "MySql";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The connection string.
   */
  connectionString: any;
  /**
   * The Azure key vault secret reference of password in connection string.
   */
  password?: AzureKeyVaultSecretReference;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Azure MySQL database linked service.
 */
export interface AzureMySqlLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureMySql";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The connection string. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString: any;
  /**
   * The Azure key vault secret reference of password in connection string.
   */
  password?: AzureKeyVaultSecretReference;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Oracle database.
 */
export interface OracleLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Oracle";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The connection string. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString: any;
  /**
   * The Azure key vault secret reference of password in connection string.
   */
  password?: AzureKeyVaultSecretReference;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Linked service for Google Cloud Storage.
 */
export interface GoogleCloudStorageLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "GoogleCloudStorage";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The access key identifier of the Google Cloud Storage Identity and Access Management (IAM)
   * user. Type: string (or Expression with resultType string).
   */
  accessKeyId?: any;
  /**
   * The secret access key of the Google Cloud Storage Identity and Access Management (IAM) user.
   */
  secretAccessKey?: SecretBaseUnion;
  /**
   * This value specifies the endpoint to access with the Google Cloud Storage Connector. This is
   * an optional property; change it only if you want to try a different service endpoint or want
   * to switch between https and http. Type: string (or Expression with resultType string).
   */
  serviceUrl?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Azure File Storage linked service.
 */
export interface AzureFileStorageLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureFileStorage";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * Host name of the server. Type: string (or Expression with resultType string).
   */
  host?: any;
  /**
   * User ID to logon the server. Type: string (or Expression with resultType string).
   */
  userId?: any;
  /**
   * Password to logon the server.
   */
  password?: SecretBaseUnion;
  /**
   * The connection string. It is mutually exclusive with sasUri property. Type: string,
   * SecureString or AzureKeyVaultSecretReference.
   */
  connectionString?: any;
  /**
   * The Azure key vault secret reference of accountKey in connection string.
   */
  accountKey?: AzureKeyVaultSecretReference;
  /**
   * SAS URI of the Azure File resource. It is mutually exclusive with connectionString property.
   * Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  sasUri?: any;
  /**
   * The Azure key vault secret reference of sasToken in sas uri.
   */
  sasToken?: AzureKeyVaultSecretReference;
  /**
   * The azure file share name. It is required when auth with accountKey/sasToken. Type: string (or
   * Expression with resultType string).
   */
  fileShare?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * File system linked service.
 */
export interface FileServerLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "FileServer";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * Host name of the server. Type: string (or Expression with resultType string).
   */
  host: any;
  /**
   * User ID to logon the server. Type: string (or Expression with resultType string).
   */
  userId?: any;
  /**
   * Password to logon the server.
   */
  password?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * HDInsight linked service.
 */
export interface HDInsightLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "HDInsight";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * HDInsight cluster URI. Type: string (or Expression with resultType string).
   */
  clusterUri: any;
  /**
   * HDInsight cluster user name. Type: string (or Expression with resultType string).
   */
  userName?: any;
  /**
   * HDInsight cluster password.
   */
  password?: SecretBaseUnion;
  /**
   * The Azure Storage linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * A reference to the Azure SQL linked service that points to the HCatalog database.
   */
  hcatalogLinkedServiceName?: LinkedServiceReference;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
  /**
   * Specify if the HDInsight is created with ESP (Enterprise Security Package). Type: Boolean.
   */
  isEspEnabled?: any;
  /**
   * Specify the FileSystem if the main storage for the HDInsight is ADLS Gen2. Type: string (or
   * Expression with resultType string).
   */
  fileSystem?: any;
}

/**
 * Common Data Service for Apps linked service.
 */
export interface CommonDataServiceForAppsLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "CommonDataServiceForApps";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The deployment type of the Common Data Service for Apps instance. 'Online' for Common Data
   * Service for Apps Online and 'OnPremisesWithIfd' for Common Data Service for Apps on-premises
   * with Ifd. Type: string (or Expression with resultType string). Possible values include:
   * 'Online', 'OnPremisesWithIfd'
   */
  deploymentType: DynamicsDeploymentType;
  /**
   * The host name of the on-premises Common Data Service for Apps server. The property is required
   * for on-prem and not allowed for online. Type: string (or Expression with resultType string).
   */
  hostName?: any;
  /**
   * The port of on-premises Common Data Service for Apps server. The property is required for
   * on-prem and not allowed for online. Default is 443. Type: integer (or Expression with
   * resultType integer), minimum: 0.
   */
  port?: any;
  /**
   * The URL to the Microsoft Common Data Service for Apps server. The property is required for
   * on-line and not allowed for on-prem. Type: string (or Expression with resultType string).
   */
  serviceUri?: any;
  /**
   * The organization name of the Common Data Service for Apps instance. The property is required
   * for on-prem and required for online when there are more than one Common Data Service for Apps
   * instances associated with the user. Type: string (or Expression with resultType string).
   */
  organizationName?: any;
  /**
   * The authentication type to connect to Common Data Service for Apps server. 'Office365' for
   * online scenario, 'Ifd' for on-premises with Ifd scenario. 'AADServicePrincipal' for
   * Server-To-Server authentication in online scenario. Type: string (or Expression with
   * resultType string). Possible values include: 'Office365', 'Ifd', 'AADServicePrincipal'
   */
  authenticationType: DynamicsAuthenticationType;
  /**
   * User name to access the Common Data Service for Apps instance. Type: string (or Expression
   * with resultType string).
   */
  username?: any;
  /**
   * Password to access the Common Data Service for Apps instance.
   */
  password?: SecretBaseUnion;
  /**
   * The client ID of the application in Azure Active Directory used for Server-To-Server
   * authentication. Type: string (or Expression with resultType string).
   */
  servicePrincipalId?: any;
  /**
   * The service principal credential type to use in Server-To-Server authentication.
   * 'ServicePrincipalKey' for key/secret, 'ServicePrincipalCert' for certificate. Type: string (or
   * Expression with resultType string).
   */
  servicePrincipalCredentialType?: any;
  /**
   * The credential of the service principal object in Azure Active Directory. If
   * servicePrincipalCredentialType is 'ServicePrincipalKey', servicePrincipalCredential can be
   * SecureString or AzureKeyVaultSecretReference. If servicePrincipalCredentialType is
   * 'ServicePrincipalCert', servicePrincipalCredential can only be AzureKeyVaultSecretReference.
   */
  servicePrincipalCredential?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Dynamics CRM linked service.
 */
export interface DynamicsCrmLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "DynamicsCrm";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The deployment type of the Dynamics CRM instance. 'Online' for Dynamics CRM Online and
   * 'OnPremisesWithIfd' for Dynamics CRM on-premises with Ifd. Type: string (or Expression with
   * resultType string). Possible values include: 'Online', 'OnPremisesWithIfd'
   */
  deploymentType: DynamicsDeploymentType;
  /**
   * The host name of the on-premises Dynamics CRM server. The property is required for on-prem and
   * not allowed for online. Type: string (or Expression with resultType string).
   */
  hostName?: any;
  /**
   * The port of on-premises Dynamics CRM server. The property is required for on-prem and not
   * allowed for online. Default is 443. Type: integer (or Expression with resultType integer),
   * minimum: 0.
   */
  port?: any;
  /**
   * The URL to the Microsoft Dynamics CRM server. The property is required for on-line and not
   * allowed for on-prem. Type: string (or Expression with resultType string).
   */
  serviceUri?: any;
  /**
   * The organization name of the Dynamics CRM instance. The property is required for on-prem and
   * required for online when there are more than one Dynamics CRM instances associated with the
   * user. Type: string (or Expression with resultType string).
   */
  organizationName?: any;
  /**
   * The authentication type to connect to Dynamics CRM server. 'Office365' for online scenario,
   * 'Ifd' for on-premises with Ifd scenario, 'AADServicePrincipal' for Server-To-Server
   * authentication in online scenario. Type: string (or Expression with resultType string).
   * Possible values include: 'Office365', 'Ifd', 'AADServicePrincipal'
   */
  authenticationType: DynamicsAuthenticationType;
  /**
   * User name to access the Dynamics CRM instance. Type: string (or Expression with resultType
   * string).
   */
  username?: any;
  /**
   * Password to access the Dynamics CRM instance.
   */
  password?: SecretBaseUnion;
  /**
   * The client ID of the application in Azure Active Directory used for Server-To-Server
   * authentication. Type: string (or Expression with resultType string).
   */
  servicePrincipalId?: any;
  /**
   * The service principal credential type to use in Server-To-Server authentication.
   * 'ServicePrincipalKey' for key/secret, 'ServicePrincipalCert' for certificate. Type: string (or
   * Expression with resultType string).
   */
  servicePrincipalCredentialType?: any;
  /**
   * The credential of the service principal object in Azure Active Directory. If
   * servicePrincipalCredentialType is 'ServicePrincipalKey', servicePrincipalCredential can be
   * SecureString or AzureKeyVaultSecretReference. If servicePrincipalCredentialType is
   * 'ServicePrincipalCert', servicePrincipalCredential can only be AzureKeyVaultSecretReference.
   */
  servicePrincipalCredential?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Dynamics linked service.
 */
export interface DynamicsLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "Dynamics";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The deployment type of the Dynamics instance. 'Online' for Dynamics Online and
   * 'OnPremisesWithIfd' for Dynamics on-premises with Ifd. Type: string (or Expression with
   * resultType string).
   */
  deploymentType: any;
  /**
   * The host name of the on-premises Dynamics server. The property is required for on-prem and not
   * allowed for online. Type: string (or Expression with resultType string).
   */
  hostName?: any;
  /**
   * The port of on-premises Dynamics server. The property is required for on-prem and not allowed
   * for online. Default is 443. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  port?: any;
  /**
   * The URL to the Microsoft Dynamics server. The property is required for on-line and not allowed
   * for on-prem. Type: string (or Expression with resultType string).
   */
  serviceUri?: any;
  /**
   * The organization name of the Dynamics instance. The property is required for on-prem and
   * required for online when there are more than one Dynamics instances associated with the user.
   * Type: string (or Expression with resultType string).
   */
  organizationName?: any;
  /**
   * The authentication type to connect to Dynamics server. 'Office365' for online scenario, 'Ifd'
   * for on-premises with Ifd scenario, 'AADServicePrincipal' for Server-To-Server authentication
   * in online scenario. Type: string (or Expression with resultType string).
   */
  authenticationType: any;
  /**
   * User name to access the Dynamics instance. Type: string (or Expression with resultType
   * string).
   */
  username?: any;
  /**
   * Password to access the Dynamics instance.
   */
  password?: SecretBaseUnion;
  /**
   * The client ID of the application in Azure Active Directory used for Server-To-Server
   * authentication. Type: string (or Expression with resultType string).
   */
  servicePrincipalId?: any;
  /**
   * The service principal credential type to use in Server-To-Server authentication.
   * 'ServicePrincipalKey' for key/secret, 'ServicePrincipalCert' for certificate. Type: string (or
   * Expression with resultType string).
   */
  servicePrincipalCredentialType?: any;
  /**
   * The credential of the service principal object in Azure Active Directory. If
   * servicePrincipalCredentialType is 'ServicePrincipalKey', servicePrincipalCredential can be
   * SecureString or AzureKeyVaultSecretReference. If servicePrincipalCredentialType is
   * 'ServicePrincipalCert', servicePrincipalCredential can only be AzureKeyVaultSecretReference.
   */
  servicePrincipalCredential?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Microsoft Azure Cosmos Database (CosmosDB) linked service.
 */
export interface CosmosDbLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "CosmosDb";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The connection string. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString?: any;
  /**
   * The endpoint of the Azure CosmosDB account. Type: string (or Expression with resultType
   * string)
   */
  accountEndpoint?: any;
  /**
   * The name of the database. Type: string (or Expression with resultType string)
   */
  database?: any;
  /**
   * The account key of the Azure CosmosDB account. Type: SecureString or
   * AzureKeyVaultSecretReference.
   */
  accountKey?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Azure Key Vault linked service.
 */
export interface AzureKeyVaultLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureKeyVault";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The base URL of the Azure Key Vault. e.g. https://myakv.vault.azure.net Type: string (or
   * Expression with resultType string).
   */
  baseUrl: any;
}

/**
 * Azure Batch linked service.
 */
export interface AzureBatchLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureBatch";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The Azure Batch account name. Type: string (or Expression with resultType string).
   */
  accountName: any;
  /**
   * The Azure Batch account access key.
   */
  accessKey?: SecretBaseUnion;
  /**
   * The Azure Batch URI. Type: string (or Expression with resultType string).
   */
  batchUri: any;
  /**
   * The Azure Batch pool name. Type: string (or Expression with resultType string).
   */
  poolName: any;
  /**
   * The Azure Storage linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Azure SQL Managed Instance linked service.
 */
export interface AzureSqlMILinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureSqlMI";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The connection string. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString: any;
  /**
   * The Azure key vault secret reference of password in connection string.
   */
  password?: AzureKeyVaultSecretReference;
  /**
   * The ID of the service principal used to authenticate against Azure SQL Managed Instance. Type:
   * string (or Expression with resultType string).
   */
  servicePrincipalId?: any;
  /**
   * The key of the service principal used to authenticate against Azure SQL Managed Instance.
   */
  servicePrincipalKey?: SecretBaseUnion;
  /**
   * The name or ID of the tenant to which the service principal belongs. Type: string (or
   * Expression with resultType string).
   */
  tenant?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Microsoft Azure SQL Database linked service.
 */
export interface AzureSqlDatabaseLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureSqlDatabase";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The connection string. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString: any;
  /**
   * The Azure key vault secret reference of password in connection string.
   */
  password?: AzureKeyVaultSecretReference;
  /**
   * The ID of the service principal used to authenticate against Azure SQL Database. Type: string
   * (or Expression with resultType string).
   */
  servicePrincipalId?: any;
  /**
   * The key of the service principal used to authenticate against Azure SQL Database.
   */
  servicePrincipalKey?: SecretBaseUnion;
  /**
   * The name or ID of the tenant to which the service principal belongs. Type: string (or
   * Expression with resultType string).
   */
  tenant?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * SQL Server linked service.
 */
export interface SqlServerLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "SqlServer";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The connection string. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString: any;
  /**
   * The on-premises Windows authentication user name. Type: string (or Expression with resultType
   * string).
   */
  userName?: any;
  /**
   * The on-premises Windows authentication password.
   */
  password?: SecretBaseUnion;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * Azure SQL Data Warehouse linked service.
 */
export interface AzureSqlDWLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureSqlDW";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The connection string. Type: string, SecureString or AzureKeyVaultSecretReference. Type:
   * string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString: any;
  /**
   * The Azure key vault secret reference of password in connection string.
   */
  password?: AzureKeyVaultSecretReference;
  /**
   * The ID of the service principal used to authenticate against Azure SQL Data Warehouse. Type:
   * string (or Expression with resultType string).
   */
  servicePrincipalId?: any;
  /**
   * The key of the service principal used to authenticate against Azure SQL Data Warehouse.
   */
  servicePrincipalKey?: SecretBaseUnion;
  /**
   * The name or ID of the tenant to which the service principal belongs. Type: string (or
   * Expression with resultType string).
   */
  tenant?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: any;
}

/**
 * The azure table storage linked service.
 */
export interface AzureTableStorageLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureTableStorage";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The connection string. It is mutually exclusive with sasUri property. Type: string,
   * SecureString or AzureKeyVaultSecretReference.
   */
  connectionString?: any;
  /**
   * The Azure key vault secret reference of accountKey in connection string.
   */
  accountKey?: AzureKeyVaultSecretReference;
  /**
   * SAS URI of the Azure Storage resource. It is mutually exclusive with connectionString
   * property. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  sasUri?: any;
  /**
   * The Azure key vault secret reference of sasToken in sas uri.
   */
  sasToken?: AzureKeyVaultSecretReference;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: string;
}

/**
 * The azure blob storage linked service.
 */
export interface AzureBlobStorageLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureBlobStorage";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The connection string. It is mutually exclusive with sasUri, serviceEndpoint property. Type:
   * string, SecureString or AzureKeyVaultSecretReference.
   */
  connectionString?: any;
  /**
   * The Azure key vault secret reference of accountKey in connection string.
   */
  accountKey?: AzureKeyVaultSecretReference;
  /**
   * SAS URI of the Azure Blob Storage resource. It is mutually exclusive with connectionString,
   * serviceEndpoint property. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  sasUri?: any;
  /**
   * The Azure key vault secret reference of sasToken in sas uri.
   */
  sasToken?: AzureKeyVaultSecretReference;
  /**
   * Blob service endpoint of the Azure Blob Storage resource. It is mutually exclusive with
   * connectionString, sasUri property.
   */
  serviceEndpoint?: string;
  /**
   * The ID of the service principal used to authenticate against Azure SQL Data Warehouse. Type:
   * string (or Expression with resultType string).
   */
  servicePrincipalId?: any;
  /**
   * The key of the service principal used to authenticate against Azure SQL Data Warehouse.
   */
  servicePrincipalKey?: SecretBaseUnion;
  /**
   * The name or ID of the tenant to which the service principal belongs. Type: string (or
   * Expression with resultType string).
   */
  tenant?: any;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: string;
}

/**
 * The storage account linked service.
 */
export interface AzureStorageLinkedService {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureStorage";
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
  /**
   * Linked service description.
   */
  description?: string;
  /**
   * Parameters for linked service.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the linked service.
   */
  annotations?: any[];
  /**
   * The connection string. It is mutually exclusive with sasUri property. Type: string,
   * SecureString or AzureKeyVaultSecretReference.
   */
  connectionString?: any;
  /**
   * The Azure key vault secret reference of accountKey in connection string.
   */
  accountKey?: AzureKeyVaultSecretReference;
  /**
   * SAS URI of the Azure Storage resource. It is mutually exclusive with connectionString
   * property. Type: string, SecureString or AzureKeyVaultSecretReference.
   */
  sasUri?: any;
  /**
   * The Azure key vault secret reference of sasToken in sas uri.
   */
  sasToken?: AzureKeyVaultSecretReference;
  /**
   * The encrypted credential used for authentication. Credentials are encrypted using the
   * integration runtime credential manager. Type: string (or Expression with resultType string).
   */
  encryptedCredential?: string;
}

/**
 * The sharepoint online list resource dataset.
 */
export interface SharePointOnlineListResourceDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "SharePointOnlineListResource";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The name of the SharePoint Online list. Type: string (or Expression with resultType string).
   */
  listName?: any;
}

/**
 * The snowflake dataset.
 */
export interface SnowflakeDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "SnowflakeTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The schema name of the Snowflake database. Type: string (or Expression with resultType
   * string).
   */
  snowflakeDatasetSchema?: any;
  /**
   * The table name of the Snowflake database. Type: string (or Expression with resultType string).
   */
  table?: any;
}

/**
 * Google AdWords service dataset.
 */
export interface GoogleAdWordsObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "GoogleAdWordsObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * The Azure Data Explorer (Kusto) dataset.
 */
export interface AzureDataExplorerTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureDataExplorerTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name of the Azure Data Explorer database. Type: string (or Expression with
   * resultType string).
   */
  table?: any;
}

/**
 * Oracle Service Cloud dataset.
 */
export interface OracleServiceCloudObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "OracleServiceCloudObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * The path of the Dynamics AX OData entity.
 */
export interface DynamicsAXResourceDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "DynamicsAXResource";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The path of the Dynamics AX OData entity. Type: string (or Expression with resultType string).
   */
  path: any;
}

/**
 * Responsys dataset.
 */
export interface ResponsysObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "ResponsysObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * Salesforce Marketing Cloud dataset.
 */
export interface SalesforceMarketingCloudObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "SalesforceMarketingCloudObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * Vertica dataset.
 */
export interface VerticaTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "VerticaTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * This property will be retired. Please consider using schema + table properties instead.
   */
  tableName?: any;
  /**
   * The table name of the Vertica. Type: string (or Expression with resultType string).
   */
  table?: any;
  /**
   * The schema name of the Vertica. Type: string (or Expression with resultType string).
   */
  verticaTableDatasetSchema?: any;
}

/**
 * Netezza dataset.
 */
export interface NetezzaTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "NetezzaTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * This property will be retired. Please consider using schema + table properties instead.
   */
  tableName?: any;
  /**
   * The table name of the Netezza. Type: string (or Expression with resultType string).
   */
  table?: any;
  /**
   * The schema name of the Netezza. Type: string (or Expression with resultType string).
   */
  netezzaTableDatasetSchema?: any;
}

/**
 * Zoho server dataset.
 */
export interface ZohoObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "ZohoObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * Xero Service dataset.
 */
export interface XeroObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "XeroObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * Square Service dataset.
 */
export interface SquareObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "SquareObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * Spark Server dataset.
 */
export interface SparkObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "SparkObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * This property will be retired. Please consider using schema + table properties instead.
   */
  tableName?: any;
  /**
   * The table name of the Spark. Type: string (or Expression with resultType string).
   */
  table?: any;
  /**
   * The schema name of the Spark. Type: string (or Expression with resultType string).
   */
  sparkObjectDatasetSchema?: any;
}

/**
 * Shopify Service dataset.
 */
export interface ShopifyObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "ShopifyObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * ServiceNow server dataset.
 */
export interface ServiceNowObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "ServiceNowObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * QuickBooks server dataset.
 */
export interface QuickBooksObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "QuickBooksObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * Presto server dataset.
 */
export interface PrestoObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "PrestoObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * This property will be retired. Please consider using schema + table properties instead.
   */
  tableName?: any;
  /**
   * The table name of the Presto. Type: string (or Expression with resultType string).
   */
  table?: any;
  /**
   * The schema name of the Presto. Type: string (or Expression with resultType string).
   */
  prestoObjectDatasetSchema?: any;
}

/**
 * Phoenix server dataset.
 */
export interface PhoenixObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "PhoenixObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * This property will be retired. Please consider using schema + table properties instead.
   */
  tableName?: any;
  /**
   * The table name of the Phoenix. Type: string (or Expression with resultType string).
   */
  table?: any;
  /**
   * The schema name of the Phoenix. Type: string (or Expression with resultType string).
   */
  phoenixObjectDatasetSchema?: any;
}

/**
 * Paypal Service dataset.
 */
export interface PaypalObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "PaypalObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * Marketo server dataset.
 */
export interface MarketoObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "MarketoObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * Azure Database for MariaDB dataset.
 */
export interface AzureMariaDBTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureMariaDBTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * MariaDB server dataset.
 */
export interface MariaDBTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "MariaDBTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * Magento server dataset.
 */
export interface MagentoObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "MagentoObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * Jira Service dataset.
 */
export interface JiraObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "JiraObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * Impala server dataset.
 */
export interface ImpalaObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "ImpalaObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * This property will be retired. Please consider using schema + table properties instead.
   */
  tableName?: any;
  /**
   * The table name of the Impala. Type: string (or Expression with resultType string).
   */
  table?: any;
  /**
   * The schema name of the Impala. Type: string (or Expression with resultType string).
   */
  impalaObjectDatasetSchema?: any;
}

/**
 * Hubspot Service dataset.
 */
export interface HubspotObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "HubspotObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * Hive Server dataset.
 */
export interface HiveObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "HiveObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * This property will be retired. Please consider using schema + table properties instead.
   */
  tableName?: any;
  /**
   * The table name of the Hive. Type: string (or Expression with resultType string).
   */
  table?: any;
  /**
   * The schema name of the Hive. Type: string (or Expression with resultType string).
   */
  hiveObjectDatasetSchema?: any;
}

/**
 * HBase server dataset.
 */
export interface HBaseObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "HBaseObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * Greenplum Database dataset.
 */
export interface GreenplumTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "GreenplumTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * This property will be retired. Please consider using schema + table properties instead.
   */
  tableName?: any;
  /**
   * The table name of Greenplum. Type: string (or Expression with resultType string).
   */
  table?: any;
  /**
   * The schema name of Greenplum. Type: string (or Expression with resultType string).
   */
  greenplumTableDatasetSchema?: any;
}

/**
 * Google BigQuery service dataset.
 */
export interface GoogleBigQueryObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "GoogleBigQueryObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * This property will be retired. Please consider using database + table properties instead.
   */
  tableName?: any;
  /**
   * The table name of the Google BigQuery. Type: string (or Expression with resultType string).
   */
  table?: any;
  /**
   * The database name of the Google BigQuery. Type: string (or Expression with resultType string).
   */
  dataset?: any;
}

/**
 * Eloqua server dataset.
 */
export interface EloquaObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "EloquaObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * Drill server dataset.
 */
export interface DrillTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "DrillTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * This property will be retired. Please consider using schema + table properties instead.
   */
  tableName?: any;
  /**
   * The table name of the Drill. Type: string (or Expression with resultType string).
   */
  table?: any;
  /**
   * The schema name of the Drill. Type: string (or Expression with resultType string).
   */
  drillTableDatasetSchema?: any;
}

/**
 * Couchbase server dataset.
 */
export interface CouchbaseTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "CouchbaseTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * Concur Service dataset.
 */
export interface ConcurObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "ConcurObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * Azure PostgreSQL dataset.
 */
export interface AzurePostgreSqlTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "AzurePostgreSqlTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name of the Azure PostgreSQL database which includes both schema and table. Type:
   * string (or Expression with resultType string).
   */
  tableName?: any;
  /**
   * The table name of the Azure PostgreSQL database. Type: string (or Expression with resultType
   * string).
   */
  table?: any;
  /**
   * The schema name of the Azure PostgreSQL database. Type: string (or Expression with resultType
   * string).
   */
  azurePostgreSqlTableDatasetSchema?: any;
}

/**
 * Amazon Marketplace Web Service dataset.
 */
export interface AmazonMWSObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "AmazonMWSObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * Contains the possible cases for DatasetCompression.
 */
export type DatasetCompressionUnion = DatasetCompression | DatasetZipDeflateCompression | DatasetDeflateCompression | DatasetGZipCompression | DatasetBZip2Compression;

/**
 * The compression method used on a dataset.
 */
export interface DatasetCompression {
  /**
   * Polymorphic Discriminator
   */
  type: "DatasetCompression";
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * The ZipDeflate compression method used on a dataset.
 */
export interface DatasetZipDeflateCompression {
  /**
   * Polymorphic Discriminator
   */
  type: "ZipDeflate";
  /**
   * The ZipDeflate compression level.
   */
  level?: any;
}

/**
 * The Deflate compression method used on a dataset.
 */
export interface DatasetDeflateCompression {
  /**
   * Polymorphic Discriminator
   */
  type: "Deflate";
  /**
   * The Deflate compression level.
   */
  level?: any;
}

/**
 * The GZip compression method used on a dataset.
 */
export interface DatasetGZipCompression {
  /**
   * Polymorphic Discriminator
   */
  type: "GZip";
  /**
   * The GZip compression level.
   */
  level?: any;
}

/**
 * The BZip2 compression method used on a dataset.
 */
export interface DatasetBZip2Compression {
  /**
   * Polymorphic Discriminator
   */
  type: "BZip2";
}

/**
 * Contains the possible cases for DatasetStorageFormat.
 */
export type DatasetStorageFormatUnion = DatasetStorageFormat | ParquetFormat | OrcFormat | AvroFormat | JsonFormat | TextFormat;

/**
 * The format definition of a storage.
 */
export interface DatasetStorageFormat {
  /**
   * Polymorphic Discriminator
   */
  type: "DatasetStorageFormat";
  /**
   * Serializer. Type: string (or Expression with resultType string).
   */
  serializer?: any;
  /**
   * Deserializer. Type: string (or Expression with resultType string).
   */
  deserializer?: any;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * The data stored in Parquet format.
 */
export interface ParquetFormat {
  /**
   * Polymorphic Discriminator
   */
  type: "ParquetFormat";
  /**
   * Serializer. Type: string (or Expression with resultType string).
   */
  serializer?: any;
  /**
   * Deserializer. Type: string (or Expression with resultType string).
   */
  deserializer?: any;
}

/**
 * The data stored in Optimized Row Columnar (ORC) format.
 */
export interface OrcFormat {
  /**
   * Polymorphic Discriminator
   */
  type: "OrcFormat";
  /**
   * Serializer. Type: string (or Expression with resultType string).
   */
  serializer?: any;
  /**
   * Deserializer. Type: string (or Expression with resultType string).
   */
  deserializer?: any;
}

/**
 * The data stored in Avro format.
 */
export interface AvroFormat {
  /**
   * Polymorphic Discriminator
   */
  type: "AvroFormat";
  /**
   * Serializer. Type: string (or Expression with resultType string).
   */
  serializer?: any;
  /**
   * Deserializer. Type: string (or Expression with resultType string).
   */
  deserializer?: any;
}

/**
 * The data stored in JSON format.
 */
export interface JsonFormat {
  /**
   * Polymorphic Discriminator
   */
  type: "JsonFormat";
  /**
   * Serializer. Type: string (or Expression with resultType string).
   */
  serializer?: any;
  /**
   * Deserializer. Type: string (or Expression with resultType string).
   */
  deserializer?: any;
  /**
   * File pattern of JSON. To be more specific, the way of separating a collection of JSON objects.
   * The default value is 'setOfObjects'. It is case-sensitive.
   */
  filePattern?: any;
  /**
   * The character used to separate nesting levels. Default value is '.' (dot). Type: string (or
   * Expression with resultType string).
   */
  nestingSeparator?: any;
  /**
   * The code page name of the preferred encoding. If not provided, the default value is 'utf-8',
   * unless the byte order mark (BOM) denotes another Unicode encoding. The full list of supported
   * values can be found in the 'Name' column of the table of encodings in the following reference:
   * https://go.microsoft.com/fwlink/?linkid=861078. Type: string (or Expression with resultType
   * string).
   */
  encodingName?: any;
  /**
   * The JSONPath of the JSON array element to be flattened. Example: "$.ArrayPath". Type: string
   * (or Expression with resultType string).
   */
  jsonNodeReference?: any;
  /**
   * The JSONPath definition for each column mapping with a customized column name to extract data
   * from JSON file. For fields under root object, start with "$"; for fields inside the array
   * chosen by jsonNodeReference property, start from the array element. Example: {"Column1":
   * "$.Column1Path", "Column2": "Column2PathInArray"}. Type: object (or Expression with resultType
   * object).
   */
  jsonPathDefinition?: any;
}

/**
 * The data stored in text format.
 */
export interface TextFormat {
  /**
   * Polymorphic Discriminator
   */
  type: "TextFormat";
  /**
   * Serializer. Type: string (or Expression with resultType string).
   */
  serializer?: any;
  /**
   * Deserializer. Type: string (or Expression with resultType string).
   */
  deserializer?: any;
  /**
   * The column delimiter. Type: string (or Expression with resultType string).
   */
  columnDelimiter?: any;
  /**
   * The row delimiter. Type: string (or Expression with resultType string).
   */
  rowDelimiter?: any;
  /**
   * The escape character. Type: string (or Expression with resultType string).
   */
  escapeChar?: any;
  /**
   * The quote character. Type: string (or Expression with resultType string).
   */
  quoteChar?: any;
  /**
   * The null value string. Type: string (or Expression with resultType string).
   */
  nullValue?: any;
  /**
   * The code page name of the preferred encoding. If miss, the default value is utf-8,
   * unless BOM denotes another Unicode encoding. Refer to the Name column of the table in
   * the following link to set supported values:
   * https://msdn.microsoft.com/library/system.text.encoding.aspx. Type: string (or Expression with
   * resultType string).
   */
  encodingName?: any;
  /**
   * Treat empty column values in the text file as null. The default value is true. Type: boolean
   * (or Expression with resultType boolean).
   */
  treatEmptyAsNull?: any;
  /**
   * The number of lines/rows to be skipped when parsing text files. The default value is 0. Type:
   * integer (or Expression with resultType integer).
   */
  skipLineCount?: any;
  /**
   * When used as input, treat the first row of data as headers. When used as output,write the
   * headers into the output as the first row of data. The default value is false. Type: boolean
   * (or Expression with resultType boolean).
   */
  firstRowAsHeader?: any;
}

/**
 * A file in an HTTP web server.
 */
export interface HttpDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "HttpFile";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The relative URL based on the URL in the HttpLinkedService refers to an HTTP file Type: string
   * (or Expression with resultType string).
   */
  relativeUrl?: any;
  /**
   * The HTTP method for the HTTP request. Type: string (or Expression with resultType string).
   */
  requestMethod?: any;
  /**
   * The body for the HTTP request. Type: string (or Expression with resultType string).
   */
  requestBody?: any;
  /**
   * The headers for the HTTP Request. e.g. request-header-name-1:request-header-value-1
   * ...
   * request-header-name-n:request-header-value-n Type: string (or Expression with resultType
   * string).
   */
  additionalHeaders?: any;
  /**
   * The format of files.
   */
  format?: DatasetStorageFormatUnion;
  /**
   * The data compression method used on files.
   */
  compression?: DatasetCompressionUnion;
}

/**
 * The Azure Search Index.
 */
export interface AzureSearchIndexDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureSearchIndex";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The name of the Azure Search Index. Type: string (or Expression with resultType string).
   */
  indexName: any;
}

/**
 * The dataset points to a HTML table in the web page.
 */
export interface WebTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "WebTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The zero-based index of the table in the web page. Type: integer (or Expression with
   * resultType integer), minimum: 0.
   */
  index: any;
  /**
   * The relative URL to the web page from the linked service URL. Type: string (or Expression with
   * resultType string).
   */
  path?: any;
}

/**
 * SAP Table Resource properties.
 */
export interface SapTableResourceDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "SapTableResource";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The name of the SAP Table. Type: string (or Expression with resultType string).
   */
  tableName: any;
}

/**
 * A Rest service dataset.
 */
export interface RestResourceDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "RestResource";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The relative URL to the resource that the RESTful API provides. Type: string (or Expression
   * with resultType string).
   */
  relativeUrl?: any;
  /**
   * The HTTP method used to call the RESTful API. The default is GET. Type: string (or Expression
   * with resultType string).
   */
  requestMethod?: any;
  /**
   * The HTTP request body to the RESTful API if requestMethod is POST. Type: string (or Expression
   * with resultType string).
   */
  requestBody?: any;
  /**
   * The additional HTTP headers in the request to the RESTful API. Type: string (or Expression
   * with resultType string).
   */
  additionalHeaders?: any;
  /**
   * The pagination rules to compose next page requests. Type: string (or Expression with
   * resultType string).
   */
  paginationRules?: any;
}

/**
 * The on-premises SQL Server dataset.
 */
export interface SqlServerTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "SqlServerTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * This property will be retired. Please consider using schema + table properties instead.
   */
  tableName?: any;
  /**
   * The schema name of the SQL Server dataset. Type: string (or Expression with resultType
   * string).
   */
  sqlServerTableDatasetSchema?: any;
  /**
   * The table name of the SQL Server dataset. Type: string (or Expression with resultType string).
   */
  table?: any;
}

/**
 * Sap Business Warehouse Open Hub Destination Table properties.
 */
export interface SapOpenHubTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "SapOpenHubTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The name of the Open Hub Destination with destination type as Database Table. Type: string (or
   * Expression with resultType string).
   */
  openHubDestinationName: any;
  /**
   * Whether to exclude the records of the last request. The default value is true. Type: boolean
   * (or Expression with resultType boolean).
   */
  excludeLastRequest?: any;
  /**
   * The ID of request for delta loading. Once it is set, only data with requestId larger than the
   * value of this property will be retrieved. The default value is 0. Type: integer (or Expression
   * with resultType integer ).
   */
  baseRequestId?: any;
}

/**
 * SAP HANA Table properties.
 */
export interface SapHanaTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "SapHanaTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The schema name of SAP HANA. Type: string (or Expression with resultType string).
   */
  sapHanaTableDatasetSchema?: any;
  /**
   * The table name of SAP HANA. Type: string (or Expression with resultType string).
   */
  table?: any;
}

/**
 * The path of the SAP ECC OData entity.
 */
export interface SapEccResourceDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "SapEccResource";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The path of the SAP ECC OData entity. Type: string (or Expression with resultType string).
   */
  path: any;
}

/**
 * The path of the SAP Cloud for Customer OData entity.
 */
export interface SapCloudForCustomerResourceDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "SapCloudForCustomerResource";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The path of the SAP Cloud for Customer OData entity. Type: string (or Expression with
   * resultType string).
   */
  path: any;
}

/**
 * The SAP BW cube dataset.
 */
export interface SapBwCubeDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "SapBwCube";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
}

/**
 * The Sybase table dataset.
 */
export interface SybaseTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "SybaseTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The Sybase table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * The Salesforce Service Cloud object dataset.
 */
export interface SalesforceServiceCloudObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "SalesforceServiceCloudObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The Salesforce Service Cloud object API name. Type: string (or Expression with resultType
   * string).
   */
  objectApiName?: any;
}

/**
 * The Salesforce object dataset.
 */
export interface SalesforceObjectDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "SalesforceObject";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The Salesforce object API name. Type: string (or Expression with resultType string).
   */
  objectApiName?: any;
}

/**
 * The Microsoft Access table dataset.
 */
export interface MicrosoftAccessTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "MicrosoftAccessTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The Microsoft Access table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * The PostgreSQL table dataset.
 */
export interface PostgreSqlTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "PostgreSqlTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * This property will be retired. Please consider using schema + table properties instead.
   */
  tableName?: any;
  /**
   * The PostgreSQL table name. Type: string (or Expression with resultType string).
   */
  table?: any;
  /**
   * The PostgreSQL schema name. Type: string (or Expression with resultType string).
   */
  postgreSqlTableDatasetSchema?: any;
}

/**
 * The MySQL table dataset.
 */
export interface MySqlTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "MySqlTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The MySQL table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * The ODBC table dataset.
 */
export interface OdbcTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "OdbcTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The ODBC table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * The Informix table dataset.
 */
export interface InformixTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "InformixTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The Informix table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * The relational table dataset.
 */
export interface RelationalTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "RelationalTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The relational table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
}

/**
 * The Db2 table dataset.
 */
export interface Db2TableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "Db2Table";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * This property will be retired. Please consider using schema + table properties instead.
   */
  tableName?: any;
  /**
   * The Db2 schema name. Type: string (or Expression with resultType string).
   */
  db2TableDatasetSchema?: any;
  /**
   * The Db2 table name. Type: string (or Expression with resultType string).
   */
  table?: any;
}

/**
 * The Amazon Redshift table dataset.
 */
export interface AmazonRedshiftTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "AmazonRedshiftTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * This property will be retired. Please consider using schema + table properties instead.
   */
  tableName?: any;
  /**
   * The Amazon Redshift table name. Type: string (or Expression with resultType string).
   */
  table?: any;
  /**
   * The Amazon Redshift schema name. Type: string (or Expression with resultType string).
   */
  amazonRedshiftTableDatasetSchema?: any;
}

/**
 * The Azure MySQL database dataset.
 */
export interface AzureMySqlTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureMySqlTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The Azure MySQL database table name. Type: string (or Expression with resultType string).
   */
  tableName?: any;
  /**
   * The name of Azure MySQL database table. Type: string (or Expression with resultType string).
   */
  table?: any;
}

/**
 * The Teradata database dataset.
 */
export interface TeradataTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "TeradataTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The database name of Teradata. Type: string (or Expression with resultType string).
   */
  database?: any;
  /**
   * The table name of Teradata. Type: string (or Expression with resultType string).
   */
  table?: any;
}

/**
 * The on-premises Oracle database dataset.
 */
export interface OracleTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "OracleTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * This property will be retired. Please consider using schema + table properties instead.
   */
  tableName?: any;
  /**
   * The schema name of the on-premises Oracle database. Type: string (or Expression with
   * resultType string).
   */
  oracleTableDatasetSchema?: any;
  /**
   * The table name of the on-premises Oracle database. Type: string (or Expression with resultType
   * string).
   */
  table?: any;
}

/**
 * The Open Data Protocol (OData) resource dataset.
 */
export interface ODataResourceDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "ODataResource";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The OData resource path. Type: string (or Expression with resultType string).
   */
  path?: any;
}

/**
 * The CosmosDB (MongoDB API) database dataset.
 */
export interface CosmosDbMongoDbApiCollectionDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "CosmosDbMongoDbApiCollection";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The collection name of the CosmosDB (MongoDB API) database. Type: string (or Expression with
   * resultType string).
   */
  collection: any;
}

/**
 * The MongoDB database dataset.
 */
export interface MongoDbV2CollectionDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "MongoDbV2Collection";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The collection name of the MongoDB database. Type: string (or Expression with resultType
   * string).
   */
  collection: any;
}

/**
 * The MongoDB database dataset.
 */
export interface MongoDbCollectionDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "MongoDbCollection";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name of the MongoDB database. Type: string (or Expression with resultType string).
   */
  collectionName: any;
}

/**
 * An on-premises file system dataset.
 */
export interface FileShareDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "FileShare";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The path of the on-premises file system. Type: string (or Expression with resultType string).
   */
  folderPath?: any;
  /**
   * The name of the on-premises file system. Type: string (or Expression with resultType string).
   */
  fileName?: any;
  /**
   * The start of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeStart?: any;
  /**
   * The end of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeEnd?: any;
  /**
   * The format of the files.
   */
  format?: DatasetStorageFormatUnion;
  /**
   * Specify a filter to be used to select a subset of files in the folderPath rather than all
   * files. Type: string (or Expression with resultType string).
   */
  fileFilter?: any;
  /**
   * The data compression method used for the file system.
   */
  compression?: DatasetCompressionUnion;
}

/**
 * The Office365 account.
 */
export interface Office365Dataset {
  /**
   * Polymorphic Discriminator
   */
  type: "Office365Table";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * Name of the dataset to extract from Office 365. Type: string (or Expression with resultType
   * string).
   */
  tableName: any;
  /**
   * A predicate expression that can be used to filter the specific rows to extract from Office
   * 365. Type: string (or Expression with resultType string).
   */
  predicate?: any;
}

/**
 * The Azure Data Lake Storage Gen2 storage.
 */
export interface AzureBlobFSDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureBlobFSFile";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The path of the Azure Data Lake Storage Gen2 storage. Type: string (or Expression with
   * resultType string).
   */
  folderPath?: any;
  /**
   * The name of the Azure Data Lake Storage Gen2. Type: string (or Expression with resultType
   * string).
   */
  fileName?: any;
  /**
   * The format of the Azure Data Lake Storage Gen2 storage.
   */
  format?: DatasetStorageFormatUnion;
  /**
   * The data compression method used for the blob storage.
   */
  compression?: DatasetCompressionUnion;
}

/**
 * Azure Data Lake Store dataset.
 */
export interface AzureDataLakeStoreDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureDataLakeStoreFile";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * Path to the folder in the Azure Data Lake Store. Type: string (or Expression with resultType
   * string).
   */
  folderPath?: any;
  /**
   * The name of the file in the Azure Data Lake Store. Type: string (or Expression with resultType
   * string).
   */
  fileName?: any;
  /**
   * The format of the Data Lake Store.
   */
  format?: DatasetStorageFormatUnion;
  /**
   * The data compression method used for the item(s) in the Azure Data Lake Store.
   */
  compression?: DatasetCompressionUnion;
}

/**
 * The Common Data Service for Apps entity dataset.
 */
export interface CommonDataServiceForAppsEntityDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "CommonDataServiceForAppsEntity";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The logical name of the entity. Type: string (or Expression with resultType string).
   */
  entityName?: any;
}

/**
 * The Dynamics CRM entity dataset.
 */
export interface DynamicsCrmEntityDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "DynamicsCrmEntity";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The logical name of the entity. Type: string (or Expression with resultType string).
   */
  entityName?: any;
}

/**
 * The Dynamics entity dataset.
 */
export interface DynamicsEntityDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "DynamicsEntity";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The logical name of the entity. Type: string (or Expression with resultType string).
   */
  entityName?: any;
}

/**
 * Microsoft Azure Document Database Collection dataset.
 */
export interface DocumentDbCollectionDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "DocumentDbCollection";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * Document Database collection name. Type: string (or Expression with resultType string).
   */
  collectionName: any;
}

/**
 * Microsoft Azure CosmosDB (SQL API) Collection dataset.
 */
export interface CosmosDbSqlApiCollectionDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "CosmosDbSqlApiCollection";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * CosmosDB (SQL API) collection name. Type: string (or Expression with resultType string).
   */
  collectionName: any;
}

/**
 * The custom dataset.
 */
export interface CustomDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "CustomDataset";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * Custom dataset properties.
   */
  typeProperties?: any;
}

/**
 * The Cassandra database dataset.
 */
export interface CassandraTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "CassandraTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name of the Cassandra database. Type: string (or Expression with resultType string).
   */
  tableName?: any;
  /**
   * The keyspace of the Cassandra database. Type: string (or Expression with resultType string).
   */
  keyspace?: any;
}

/**
 * The Azure SQL Data Warehouse dataset.
 */
export interface AzureSqlDWTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureSqlDWTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * This property will be retired. Please consider using schema + table properties instead.
   */
  tableName?: any;
  /**
   * The schema name of the Azure SQL Data Warehouse. Type: string (or Expression with resultType
   * string).
   */
  azureSqlDWTableDatasetSchema?: any;
  /**
   * The table name of the Azure SQL Data Warehouse. Type: string (or Expression with resultType
   * string).
   */
  table?: any;
}

/**
 * The Azure SQL Managed Instance dataset.
 */
export interface AzureSqlMITableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureSqlMITable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * This property will be retired. Please consider using schema + table properties instead.
   */
  tableName?: any;
  /**
   * The schema name of the Azure SQL Managed Instance. Type: string (or Expression with resultType
   * string).
   */
  azureSqlMITableDatasetSchema?: any;
  /**
   * The table name of the Azure SQL Managed Instance dataset. Type: string (or Expression with
   * resultType string).
   */
  table?: any;
}

/**
 * The Azure SQL Server database dataset.
 */
export interface AzureSqlTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureSqlTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * This property will be retired. Please consider using schema + table properties instead.
   */
  tableName?: any;
  /**
   * The schema name of the Azure SQL database. Type: string (or Expression with resultType
   * string).
   */
  azureSqlTableDatasetSchema?: any;
  /**
   * The table name of the Azure SQL database. Type: string (or Expression with resultType string).
   */
  table?: any;
}

/**
 * The Azure Table storage dataset.
 */
export interface AzureTableDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureTable";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The table name of the Azure Table storage. Type: string (or Expression with resultType
   * string).
   */
  tableName: any;
}

/**
 * The Azure Blob storage.
 */
export interface AzureBlobDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureBlob";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The path of the Azure Blob storage. Type: string (or Expression with resultType string).
   */
  folderPath?: any;
  /**
   * The root of blob path. Type: string (or Expression with resultType string).
   */
  tableRootLocation?: any;
  /**
   * The name of the Azure Blob. Type: string (or Expression with resultType string).
   */
  fileName?: any;
  /**
   * The start of Azure Blob's modified datetime. Type: string (or Expression with resultType
   * string).
   */
  modifiedDatetimeStart?: any;
  /**
   * The end of Azure Blob's modified datetime. Type: string (or Expression with resultType
   * string).
   */
  modifiedDatetimeEnd?: any;
  /**
   * The format of the Azure Blob storage.
   */
  format?: DatasetStorageFormatUnion;
  /**
   * The data compression method used for the blob storage.
   */
  compression?: DatasetCompressionUnion;
}

/**
 * Contains the possible cases for DatasetLocation.
 */
export type DatasetLocationUnion = DatasetLocation | HdfsLocation | HttpServerLocation | SftpLocation | FtpServerLocation | GoogleCloudStorageLocation | AzureFileStorageLocation | FileServerLocation | AmazonS3Location | AzureDataLakeStoreLocation | AzureBlobFSLocation | AzureBlobStorageLocation;

/**
 * Dataset location.
 */
export interface DatasetLocation {
  /**
   * Polymorphic Discriminator
   */
  type: "DatasetLocation";
  /**
   * Specify the folder path of dataset. Type: string (or Expression with resultType string)
   */
  folderPath?: any;
  /**
   * Specify the file name of dataset. Type: string (or Expression with resultType string).
   */
  fileName?: any;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * The location of HDFS.
 */
export interface HdfsLocation {
  /**
   * Polymorphic Discriminator
   */
  type: "HdfsLocation";
  /**
   * Specify the folder path of dataset. Type: string (or Expression with resultType string)
   */
  folderPath?: any;
  /**
   * Specify the file name of dataset. Type: string (or Expression with resultType string).
   */
  fileName?: any;
}

/**
 * The location of http server.
 */
export interface HttpServerLocation {
  /**
   * Polymorphic Discriminator
   */
  type: "HttpServerLocation";
  /**
   * Specify the folder path of dataset. Type: string (or Expression with resultType string)
   */
  folderPath?: any;
  /**
   * Specify the file name of dataset. Type: string (or Expression with resultType string).
   */
  fileName?: any;
  /**
   * Specify the relativeUrl of http server. Type: string (or Expression with resultType string)
   */
  relativeUrl?: any;
}

/**
 * The location of SFTP dataset.
 */
export interface SftpLocation {
  /**
   * Polymorphic Discriminator
   */
  type: "SftpLocation";
  /**
   * Specify the folder path of dataset. Type: string (or Expression with resultType string)
   */
  folderPath?: any;
  /**
   * Specify the file name of dataset. Type: string (or Expression with resultType string).
   */
  fileName?: any;
}

/**
 * The location of ftp server dataset.
 */
export interface FtpServerLocation {
  /**
   * Polymorphic Discriminator
   */
  type: "FtpServerLocation";
  /**
   * Specify the folder path of dataset. Type: string (or Expression with resultType string)
   */
  folderPath?: any;
  /**
   * Specify the file name of dataset. Type: string (or Expression with resultType string).
   */
  fileName?: any;
}

/**
 * The location of Google Cloud Storage dataset.
 */
export interface GoogleCloudStorageLocation {
  /**
   * Polymorphic Discriminator
   */
  type: "GoogleCloudStorageLocation";
  /**
   * Specify the folder path of dataset. Type: string (or Expression with resultType string)
   */
  folderPath?: any;
  /**
   * Specify the file name of dataset. Type: string (or Expression with resultType string).
   */
  fileName?: any;
  /**
   * Specify the bucketName of Google Cloud Storage. Type: string (or Expression with resultType
   * string)
   */
  bucketName?: any;
  /**
   * Specify the version of Google Cloud Storage. Type: string (or Expression with resultType
   * string).
   */
  version?: any;
}

/**
 * The location of file server dataset.
 */
export interface AzureFileStorageLocation {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureFileStorageLocation";
  /**
   * Specify the folder path of dataset. Type: string (or Expression with resultType string)
   */
  folderPath?: any;
  /**
   * Specify the file name of dataset. Type: string (or Expression with resultType string).
   */
  fileName?: any;
}

/**
 * The location of file server dataset.
 */
export interface FileServerLocation {
  /**
   * Polymorphic Discriminator
   */
  type: "FileServerLocation";
  /**
   * Specify the folder path of dataset. Type: string (or Expression with resultType string)
   */
  folderPath?: any;
  /**
   * Specify the file name of dataset. Type: string (or Expression with resultType string).
   */
  fileName?: any;
}

/**
 * The location of amazon S3 dataset.
 */
export interface AmazonS3Location {
  /**
   * Polymorphic Discriminator
   */
  type: "AmazonS3Location";
  /**
   * Specify the folder path of dataset. Type: string (or Expression with resultType string)
   */
  folderPath?: any;
  /**
   * Specify the file name of dataset. Type: string (or Expression with resultType string).
   */
  fileName?: any;
  /**
   * Specify the bucketName of amazon S3. Type: string (or Expression with resultType string)
   */
  bucketName?: any;
  /**
   * Specify the version of amazon S3. Type: string (or Expression with resultType string).
   */
  version?: any;
}

/**
 * The location of azure data lake store dataset.
 */
export interface AzureDataLakeStoreLocation {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureDataLakeStoreLocation";
  /**
   * Specify the folder path of dataset. Type: string (or Expression with resultType string)
   */
  folderPath?: any;
  /**
   * Specify the file name of dataset. Type: string (or Expression with resultType string).
   */
  fileName?: any;
}

/**
 * The location of azure blobFS dataset.
 */
export interface AzureBlobFSLocation {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureBlobFSLocation";
  /**
   * Specify the folder path of dataset. Type: string (or Expression with resultType string)
   */
  folderPath?: any;
  /**
   * Specify the file name of dataset. Type: string (or Expression with resultType string).
   */
  fileName?: any;
  /**
   * Specify the fileSystem of azure blobFS. Type: string (or Expression with resultType string).
   */
  fileSystem?: any;
}

/**
 * The location of azure blob dataset.
 */
export interface AzureBlobStorageLocation {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureBlobStorageLocation";
  /**
   * Specify the folder path of dataset. Type: string (or Expression with resultType string)
   */
  folderPath?: any;
  /**
   * Specify the file name of dataset. Type: string (or Expression with resultType string).
   */
  fileName?: any;
  /**
   * Specify the container of azure blob. Type: string (or Expression with resultType string).
   */
  container?: any;
}

/**
 * Binary dataset.
 */
export interface BinaryDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "Binary";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The location of the Binary storage.
   */
  location: DatasetLocationUnion;
  /**
   * The data compression method used for the binary dataset.
   */
  compression?: DatasetCompressionUnion;
}

/**
 * ORC dataset.
 */
export interface OrcDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "Orc";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The location of the ORC data storage.
   */
  location: DatasetLocationUnion;
  /**
   * Possible values include: 'none', 'zlib', 'snappy'
   */
  orcCompressionCodec?: OrcCompressionCodec;
}

/**
 * Xml dataset.
 */
export interface XmlDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "Xml";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The location of the json data storage.
   */
  location: DatasetLocationUnion;
  /**
   * The code page name of the preferred encoding. If not specified, the default value is UTF-8,
   * unless BOM denotes another Unicode encoding. Refer to the name column of the table in the
   * following link to set supported values:
   * https://msdn.microsoft.com/library/system.text.encoding.aspx. Type: string (or Expression with
   * resultType string).
   */
  encodingName?: any;
  /**
   * The null value string. Type: string (or Expression with resultType string).
   */
  nullValue?: any;
  /**
   * The data compression method used for the json dataset.
   */
  compression?: DatasetCompressionUnion;
}

/**
 * Json dataset.
 */
export interface JsonDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "Json";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The location of the json data storage.
   */
  location: DatasetLocationUnion;
  /**
   * The code page name of the preferred encoding. If not specified, the default value is UTF-8,
   * unless BOM denotes another Unicode encoding. Refer to the name column of the table in the
   * following link to set supported values:
   * https://msdn.microsoft.com/library/system.text.encoding.aspx. Type: string (or Expression with
   * resultType string).
   */
  encodingName?: any;
  /**
   * The data compression method used for the json dataset.
   */
  compression?: DatasetCompressionUnion;
}

/**
 * Delimited text dataset.
 */
export interface DelimitedTextDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "DelimitedText";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The location of the delimited text storage.
   */
  location: DatasetLocationUnion;
  /**
   * The column delimiter. Type: string (or Expression with resultType string).
   */
  columnDelimiter?: any;
  /**
   * The row delimiter. Type: string (or Expression with resultType string).
   */
  rowDelimiter?: any;
  /**
   * The code page name of the preferred encoding. If miss, the default value is UTF-8, unless BOM
   * denotes another Unicode encoding. Refer to the name column of the table in the following link
   * to set supported values: https://msdn.microsoft.com/library/system.text.encoding.aspx. Type:
   * string (or Expression with resultType string).
   */
  encodingName?: any;
  compressionCodec?: any;
  /**
   * The data compression method used for DelimitedText.
   */
  compressionLevel?: any;
  /**
   * The quote character. Type: string (or Expression with resultType string).
   */
  quoteChar?: any;
  /**
   * The escape character. Type: string (or Expression with resultType string).
   */
  escapeChar?: any;
  /**
   * When used as input, treat the first row of data as headers. When used as output,write the
   * headers into the output as the first row of data. The default value is false. Type: boolean
   * (or Expression with resultType boolean).
   */
  firstRowAsHeader?: any;
  /**
   * The null value string. Type: string (or Expression with resultType string).
   */
  nullValue?: any;
}

/**
 * Parquet dataset.
 */
export interface ParquetDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "Parquet";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The location of the parquet storage.
   */
  location: DatasetLocationUnion;
  compressionCodec?: any;
}

/**
 * Excel dataset.
 */
export interface ExcelDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "Excel";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The location of the excel storage.
   */
  location: DatasetLocationUnion;
  /**
   * The sheet of excel file. Type: string (or Expression with resultType string).
   */
  sheetName: any;
  /**
   * The partial data of one sheet. Type: string (or Expression with resultType string).
   */
  range?: any;
  /**
   * When used as input, treat the first row of data as headers. When used as output,write the
   * headers into the output as the first row of data. The default value is false. Type: boolean
   * (or Expression with resultType boolean).
   */
  firstRowAsHeader?: any;
  /**
   * The data compression method used for the json dataset.
   */
  compression?: DatasetCompressionUnion;
  /**
   * The null value string. Type: string (or Expression with resultType string).
   */
  nullValue?: any;
}

/**
 * Avro dataset.
 */
export interface AvroDataset {
  /**
   * Polymorphic Discriminator
   */
  type: "Avro";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The location of the avro storage.
   */
  location: DatasetLocationUnion;
  /**
   * Possible values include: 'none', 'deflate', 'snappy', 'xz', 'bzip2'
   */
  avroCompressionCodec?: AvroCompressionCodec;
  avroCompressionLevel?: number;
}

/**
 * A single Amazon Simple Storage Service (S3) object or a set of S3 objects.
 */
export interface AmazonS3Dataset {
  /**
   * Polymorphic Discriminator
   */
  type: "AmazonS3Object";
  /**
   * Dataset description.
   */
  description?: string;
  /**
   * Columns that define the structure of the dataset. Type: array (or Expression with resultType
   * array), itemType: DatasetDataElement.
   */
  structure?: any;
  /**
   * Columns that define the physical type schema of the dataset. Type: array (or Expression with
   * resultType array), itemType: DatasetSchemaDataElement.
   */
  schema?: any;
  /**
   * Linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * Parameters for dataset.
   */
  parameters?: { [propertyName: string]: ParameterSpecification };
  /**
   * List of tags that can be used for describing the Dataset.
   */
  annotations?: any[];
  /**
   * The folder that this Dataset is in. If not specified, Dataset will appear at the root level.
   */
  folder?: DatasetFolder;
  /**
   * The name of the Amazon S3 bucket. Type: string (or Expression with resultType string).
   */
  bucketName: any;
  /**
   * The key of the Amazon S3 object. Type: string (or Expression with resultType string).
   */
  key?: any;
  /**
   * The prefix filter for the S3 object name. Type: string (or Expression with resultType string).
   */
  prefix?: any;
  /**
   * The version for the S3 object. Type: string (or Expression with resultType string).
   */
  version?: any;
  /**
   * The start of S3 object's modified datetime. Type: string (or Expression with resultType
   * string).
   */
  modifiedDatetimeStart?: any;
  /**
   * The end of S3 object's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeEnd?: any;
  /**
   * The format of files.
   */
  format?: DatasetStorageFormatUnion;
  /**
   * The data compression method used for the Amazon S3 object.
   */
  compression?: DatasetCompressionUnion;
}

/**
 * Trigger that allows the referenced pipeline to depend on other pipeline runs based on
 * runDimension Name/Value pairs. Upstream pipelines should declare the same runDimension Name and
 * their runs should have the values for those runDimensions. The referenced pipeline run would be
 * triggered if the values for the runDimension match for all upstream pipeline runs.
 */
export interface ChainingTrigger {
  /**
   * Polymorphic Discriminator
   */
  type: "ChainingTrigger";
  /**
   * Trigger description.
   */
  description?: string;
  /**
   * Indicates if trigger is running or not. Updated when Start/Stop APIs are called on the
   * Trigger. Possible values include: 'Started', 'Stopped', 'Disabled'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly runtimeState?: TriggerRuntimeState;
  /**
   * List of tags that can be used for describing the trigger.
   */
  annotations?: any[];
  /**
   * Pipeline for which runs are created when all upstream pipelines complete successfully.
   */
  pipelineProperty: TriggerPipelineReference;
  /**
   * Upstream Pipelines.
   */
  dependsOn: PipelineReference[];
  /**
   * Run Dimension property that needs to be emitted by upstream pipelines.
   */
  runDimension: string;
}

/**
 * Trigger that schedules pipeline reruns for all fixed time interval windows from a requested
 * start time to requested end time.
 */
export interface RerunTumblingWindowTrigger {
  /**
   * Polymorphic Discriminator
   */
  type: "RerunTumblingWindowTrigger";
  /**
   * Trigger description.
   */
  description?: string;
  /**
   * Indicates if trigger is running or not. Updated when Start/Stop APIs are called on the
   * Trigger. Possible values include: 'Started', 'Stopped', 'Disabled'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly runtimeState?: TriggerRuntimeState;
  /**
   * List of tags that can be used for describing the trigger.
   */
  annotations?: any[];
  /**
   * The parent trigger reference.
   */
  parentTrigger: any;
  /**
   * The start time for the time period for which restatement is initiated. Only UTC time is
   * currently supported.
   */
  requestedStartTime: Date;
  /**
   * The end time for the time period for which restatement is initiated. Only UTC time is
   * currently supported.
   */
  requestedEndTime: Date;
  /**
   * The max number of parallel time windows (ready for execution) for which a rerun is triggered.
   */
  rerunConcurrency: number;
}

/**
 * Contains the possible cases for DependencyReference.
 */
export type DependencyReferenceUnion = DependencyReference | SelfDependencyTumblingWindowTriggerReference | TriggerDependencyReferenceUnion;

/**
 * Referenced dependency.
 */
export interface DependencyReference {
  /**
   * Polymorphic Discriminator
   */
  type: "DependencyReference";
}

/**
 * Self referenced tumbling window trigger dependency.
 */
export interface SelfDependencyTumblingWindowTriggerReference {
  /**
   * Polymorphic Discriminator
   */
  type: "SelfDependencyTumblingWindowTriggerReference";
  /**
   * Timespan applied to the start time of a tumbling window when evaluating dependency.
   */
  offset: string;
  /**
   * The size of the window when evaluating the dependency. If undefined the frequency of the
   * tumbling window will be used.
   */
  size?: string;
}

/**
 * Trigger reference type.
 */
export interface TriggerReference {
  /**
   * Reference trigger name.
   */
  referenceName: string;
}

/**
 * Contains the possible cases for TriggerDependencyReference.
 */
export type TriggerDependencyReferenceUnion = TriggerDependencyReference | TumblingWindowTriggerDependencyReference;

/**
 * Trigger referenced dependency.
 */
export interface TriggerDependencyReference {
  /**
   * Polymorphic Discriminator
   */
  type: "TriggerDependencyReference";
  /**
   * Referenced trigger.
   */
  referenceTrigger: TriggerReference;
}

/**
 * Referenced tumbling window trigger dependency.
 */
export interface TumblingWindowTriggerDependencyReference {
  /**
   * Polymorphic Discriminator
   */
  type: "TumblingWindowTriggerDependencyReference";
  /**
   * Referenced trigger.
   */
  referenceTrigger: TriggerReference;
  /**
   * Timespan applied to the start time of a tumbling window when evaluating dependency.
   */
  offset?: string;
  /**
   * The size of the window when evaluating the dependency. If undefined the frequency of the
   * tumbling window will be used.
   */
  size?: string;
}

/**
 * Execution policy for an activity.
 */
export interface RetryPolicy {
  /**
   * Maximum ordinary retry attempts. Default is 0. Type: integer (or Expression with resultType
   * integer), minimum: 0.
   */
  count?: any;
  /**
   * Interval between retries in seconds. Default is 30.
   */
  intervalInSeconds?: number;
}

/**
 * Trigger that schedules pipeline runs for all fixed time interval windows from a start time
 * without gaps and also supports backfill scenarios (when start time is in the past).
 */
export interface TumblingWindowTrigger {
  /**
   * Polymorphic Discriminator
   */
  type: "TumblingWindowTrigger";
  /**
   * Trigger description.
   */
  description?: string;
  /**
   * Indicates if trigger is running or not. Updated when Start/Stop APIs are called on the
   * Trigger. Possible values include: 'Started', 'Stopped', 'Disabled'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly runtimeState?: TriggerRuntimeState;
  /**
   * List of tags that can be used for describing the trigger.
   */
  annotations?: any[];
  /**
   * Pipeline for which runs are created when an event is fired for trigger window that is ready.
   */
  pipelineProperty: TriggerPipelineReference;
  /**
   * The frequency of the time windows. Possible values include: 'Minute', 'Hour'
   */
  frequency: TumblingWindowFrequency;
  /**
   * The interval of the time windows. The minimum interval allowed is 15 Minutes.
   */
  interval: number;
  /**
   * The start time for the time period for the trigger during which events are fired for windows
   * that are ready. Only UTC time is currently supported.
   */
  startTime: Date;
  /**
   * The end time for the time period for the trigger during which events are fired for windows
   * that are ready. Only UTC time is currently supported.
   */
  endTime?: Date;
  /**
   * Specifies how long the trigger waits past due time before triggering new run. It doesn't alter
   * window start and end time. The default is 0. Type: string (or Expression with resultType
   * string), pattern: ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  delay?: any;
  /**
   * The max number of parallel time windows (ready for execution) for which a new run is
   * triggered.
   */
  maxConcurrency: number;
  /**
   * Retry policy that will be applied for failed pipeline runs.
   */
  retryPolicy?: RetryPolicy;
  /**
   * Triggers that this trigger depends on. Only tumbling window triggers are supported.
   */
  dependsOn?: DependencyReferenceUnion[];
}

/**
 * Contains the possible cases for MultiplePipelineTrigger.
 */
export type MultiplePipelineTriggerUnion = MultiplePipelineTrigger | BlobEventsTrigger | BlobTrigger | ScheduleTrigger;

/**
 * Base class for all triggers that support one to many model for trigger to pipeline.
 */
export interface MultiplePipelineTrigger {
  /**
   * Polymorphic Discriminator
   */
  type: "MultiplePipelineTrigger";
  /**
   * Trigger description.
   */
  description?: string;
  /**
   * Indicates if trigger is running or not. Updated when Start/Stop APIs are called on the
   * Trigger. Possible values include: 'Started', 'Stopped', 'Disabled'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly runtimeState?: TriggerRuntimeState;
  /**
   * List of tags that can be used for describing the trigger.
   */
  annotations?: any[];
  /**
   * Pipelines that need to be started.
   */
  pipelines?: TriggerPipelineReference[];
}

/**
 * Trigger that runs every time a Blob event occurs.
 */
export interface BlobEventsTrigger {
  /**
   * Polymorphic Discriminator
   */
  type: "BlobEventsTrigger";
  /**
   * Trigger description.
   */
  description?: string;
  /**
   * Indicates if trigger is running or not. Updated when Start/Stop APIs are called on the
   * Trigger. Possible values include: 'Started', 'Stopped', 'Disabled'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly runtimeState?: TriggerRuntimeState;
  /**
   * List of tags that can be used for describing the trigger.
   */
  annotations?: any[];
  /**
   * Pipelines that need to be started.
   */
  pipelines?: TriggerPipelineReference[];
  /**
   * The blob path must begin with the pattern provided for trigger to fire. For example,
   * '/records/blobs/december/' will only fire the trigger for blobs in the december folder under
   * the records container. At least one of these must be provided: blobPathBeginsWith,
   * blobPathEndsWith.
   */
  blobPathBeginsWith?: string;
  /**
   * The blob path must end with the pattern provided for trigger to fire. For example,
   * 'december/boxes.csv' will only fire the trigger for blobs named boxes in a december folder. At
   * least one of these must be provided: blobPathBeginsWith, blobPathEndsWith.
   */
  blobPathEndsWith?: string;
  /**
   * If set to true, blobs with zero bytes will be ignored.
   */
  ignoreEmptyBlobs?: boolean;
  /**
   * The type of events that cause this trigger to fire.
   */
  events: BlobEventTypes[];
  /**
   * The ARM resource ID of the Storage Account.
   */
  scope: string;
}

/**
 * Trigger that runs every time the selected Blob container changes.
 */
export interface BlobTrigger {
  /**
   * Polymorphic Discriminator
   */
  type: "BlobTrigger";
  /**
   * Trigger description.
   */
  description?: string;
  /**
   * Indicates if trigger is running or not. Updated when Start/Stop APIs are called on the
   * Trigger. Possible values include: 'Started', 'Stopped', 'Disabled'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly runtimeState?: TriggerRuntimeState;
  /**
   * List of tags that can be used for describing the trigger.
   */
  annotations?: any[];
  /**
   * Pipelines that need to be started.
   */
  pipelines?: TriggerPipelineReference[];
  /**
   * The path of the container/folder that will trigger the pipeline.
   */
  folderPath: string;
  /**
   * The max number of parallel files to handle when it is triggered.
   */
  maxConcurrency: number;
  /**
   * The Azure Storage linked service reference.
   */
  linkedService: LinkedServiceReference;
}

/**
 * The recurrence schedule occurrence.
 */
export interface RecurrenceScheduleOccurrence {
  /**
   * The day of the week. Possible values include: 'Sunday', 'Monday', 'Tuesday', 'Wednesday',
   * 'Thursday', 'Friday', 'Saturday'
   */
  day?: DayOfWeek;
  /**
   * The occurrence.
   */
  occurrence?: number;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * The recurrence schedule.
 */
export interface RecurrenceSchedule {
  /**
   * The minutes.
   */
  minutes?: number[];
  /**
   * The hours.
   */
  hours?: number[];
  /**
   * The days of the week.
   */
  weekDays?: DaysOfWeek[];
  /**
   * The month days.
   */
  monthDays?: number[];
  /**
   * The monthly occurrences.
   */
  monthlyOccurrences?: RecurrenceScheduleOccurrence[];
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * The workflow trigger recurrence.
 */
export interface ScheduleTriggerRecurrence {
  /**
   * The frequency. Possible values include: 'NotSpecified', 'Minute', 'Hour', 'Day', 'Week',
   * 'Month', 'Year'
   */
  frequency?: RecurrenceFrequency;
  /**
   * The interval.
   */
  interval?: number;
  /**
   * The start time.
   */
  startTime?: Date;
  /**
   * The end time.
   */
  endTime?: Date;
  /**
   * The time zone.
   */
  timeZone?: string;
  /**
   * The recurrence schedule.
   */
  schedule?: RecurrenceSchedule;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Trigger that creates pipeline runs periodically, on schedule.
 */
export interface ScheduleTrigger {
  /**
   * Polymorphic Discriminator
   */
  type: "ScheduleTrigger";
  /**
   * Trigger description.
   */
  description?: string;
  /**
   * Indicates if trigger is running or not. Updated when Start/Stop APIs are called on the
   * Trigger. Possible values include: 'Started', 'Stopped', 'Disabled'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly runtimeState?: TriggerRuntimeState;
  /**
   * List of tags that can be used for describing the trigger.
   */
  annotations?: any[];
  /**
   * Pipelines that need to be started.
   */
  pipelines?: TriggerPipelineReference[];
  /**
   * Recurrence schedule configuration.
   */
  recurrence: ScheduleTriggerRecurrence;
}

/**
 * Execution policy for an activity.
 */
export interface ActivityPolicy {
  /**
   * Specifies the timeout for the activity to run. The default timeout is 7 days. Type: string (or
   * Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  timeout?: any;
  /**
   * Maximum ordinary retry attempts. Default is 0. Type: integer (or Expression with resultType
   * integer), minimum: 0.
   */
  retry?: any;
  /**
   * Interval between each retry attempt (in seconds). The default is 30 sec.
   */
  retryIntervalInSeconds?: number;
  /**
   * When set to true, Input from activity is considered as secure and will not be logged to
   * monitoring.
   */
  secureInput?: boolean;
  /**
   * When set to true, Output from activity is considered as secure and will not be logged to
   * monitoring.
   */
  secureOutput?: boolean;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Compute properties for data flow activity.
 */
export interface ExecuteDataFlowActivityTypePropertiesCompute {
  /**
   * Compute type of the cluster which will execute data flow job. Possible values include:
   * 'General', 'MemoryOptimized', 'ComputeOptimized'
   */
  computeType?: DataFlowComputeType;
  /**
   * Core count of the cluster which will execute data flow job. Supported values are: 8, 16, 32,
   * 48, 80, 144 and 272.
   */
  coreCount?: number;
}

/**
 * Contains the possible cases for ExecutionActivity.
 */
export type ExecutionActivityUnion = ExecutionActivity | ExecuteDataFlowActivity | AzureFunctionActivity | DatabricksSparkPythonActivity | DatabricksSparkJarActivity | DatabricksNotebookActivity | DataLakeAnalyticsUSQLActivity | AzureMLExecutePipelineActivity | AzureMLUpdateResourceActivity | AzureMLBatchExecutionActivity | GetMetadataActivity | WebActivity | LookupActivity | AzureDataExplorerCommandActivity | DeleteActivity | SqlServerStoredProcedureActivity | CustomActivity | ExecuteSSISPackageActivity | HDInsightSparkActivity | HDInsightStreamingActivity | HDInsightMapReduceActivity | HDInsightPigActivity | HDInsightHiveActivity | CopyActivity;

/**
 * Base class for all execution activities.
 */
export interface ExecutionActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "Execution";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
}

/**
 * Execute data flow activity.
 */
export interface ExecuteDataFlowActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "ExecuteDataFlow";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * Data flow reference.
   */
  dataFlow: DataFlowReference;
  /**
   * Staging info for execute data flow activity.
   */
  staging?: DataFlowStagingInfo;
  /**
   * The integration runtime reference.
   */
  integrationRuntime?: IntegrationRuntimeReference;
  /**
   * Compute properties for data flow activity.
   */
  compute?: ExecuteDataFlowActivityTypePropertiesCompute;
}

/**
 * Azure Function activity.
 */
export interface AzureFunctionActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureFunctionActivity";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * Rest API method for target endpoint. Possible values include: 'GET', 'POST', 'PUT', 'DELETE',
   * 'OPTIONS', 'HEAD', 'TRACE'
   */
  method: AzureFunctionActivityMethod;
  /**
   * Name of the Function that the Azure Function Activity will call. Type: string (or Expression
   * with resultType string)
   */
  functionName: any;
  /**
   * Represents the headers that will be sent to the request. For example, to set the language and
   * type on a request: "headers" : { "Accept-Language": "en-us", "Content-Type":
   * "application/json" }. Type: string (or Expression with resultType string).
   */
  headers?: any;
  /**
   * Represents the payload that will be sent to the endpoint. Required for POST/PUT method, not
   * allowed for GET method Type: string (or Expression with resultType string).
   */
  body?: any;
}

/**
 * DatabricksSparkPython activity.
 */
export interface DatabricksSparkPythonActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "DatabricksSparkPython";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * The URI of the Python file to be executed. DBFS paths are supported. Type: string (or
   * Expression with resultType string).
   */
  pythonFile: any;
  /**
   * Command line parameters that will be passed to the Python file.
   */
  parameters?: any[];
  /**
   * A list of libraries to be installed on the cluster that will execute the job.
   */
  libraries?: { [propertyName: string]: any }[];
}

/**
 * DatabricksSparkJar activity.
 */
export interface DatabricksSparkJarActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "DatabricksSparkJar";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * The full name of the class containing the main method to be executed. This class must be
   * contained in a JAR provided as a library. Type: string (or Expression with resultType string).
   */
  mainClassName: any;
  /**
   * Parameters that will be passed to the main method.
   */
  parameters?: any[];
  /**
   * A list of libraries to be installed on the cluster that will execute the job.
   */
  libraries?: { [propertyName: string]: any }[];
}

/**
 * DatabricksNotebook activity.
 */
export interface DatabricksNotebookActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "DatabricksNotebook";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * The absolute path of the notebook to be run in the Databricks Workspace. This path must begin
   * with a slash. Type: string (or Expression with resultType string).
   */
  notebookPath: any;
  /**
   * Base parameters to be used for each run of this job.If the notebook takes a parameter that is
   * not specified, the default value from the notebook will be used.
   */
  baseParameters?: { [propertyName: string]: any };
  /**
   * A list of libraries to be installed on the cluster that will execute the job.
   */
  libraries?: { [propertyName: string]: any }[];
}

/**
 * Data Lake Analytics U-SQL activity.
 */
export interface DataLakeAnalyticsUSQLActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "DataLakeAnalyticsU-SQL";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * Case-sensitive path to folder that contains the U-SQL script. Type: string (or Expression with
   * resultType string).
   */
  scriptPath: any;
  /**
   * Script linked service reference.
   */
  scriptLinkedService: LinkedServiceReference;
  /**
   * The maximum number of nodes simultaneously used to run the job. Default value is 1. Type:
   * integer (or Expression with resultType integer), minimum: 1.
   */
  degreeOfParallelism?: any;
  /**
   * Determines which jobs out of all that are queued should be selected to run first. The lower
   * the number, the higher the priority. Default value is 1000. Type: integer (or Expression with
   * resultType integer), minimum: 1.
   */
  priority?: any;
  /**
   * Parameters for U-SQL job request.
   */
  parameters?: { [propertyName: string]: any };
  /**
   * Runtime version of the U-SQL engine to use. Type: string (or Expression with resultType
   * string).
   */
  runtimeVersion?: any;
  /**
   * Compilation mode of U-SQL. Must be one of these values : Semantic, Full and SingleBox. Type:
   * string (or Expression with resultType string).
   */
  compilationMode?: any;
}

/**
 * Azure ML Execute Pipeline activity.
 */
export interface AzureMLExecutePipelineActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureMLExecutePipeline";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * ID of the published Azure ML pipeline. Type: string (or Expression with resultType string).
   */
  mlPipelineId: any;
  /**
   * Run history experiment name of the pipeline run. This information will be passed in the
   * ExperimentName property of the published pipeline execution request. Type: string (or
   * Expression with resultType string).
   */
  experimentName?: any;
  /**
   * Key,Value pairs to be passed to the published Azure ML pipeline endpoint. Keys must match the
   * names of pipeline parameters defined in the published pipeline. Values will be passed in the
   * ParameterAssignments property of the published pipeline execution request. Type: object with
   * key value pairs (or Expression with resultType object).
   */
  mlPipelineParameters?: any;
  /**
   * The parent Azure ML Service pipeline run id. This information will be passed in the
   * ParentRunId property of the published pipeline execution request. Type: string (or Expression
   * with resultType string).
   */
  mlParentRunId?: any;
  /**
   * Whether to continue execution of other steps in the PipelineRun if a step fails. This
   * information will be passed in the continueOnStepFailure property of the published pipeline
   * execution request. Type: boolean (or Expression with resultType boolean).
   */
  continueOnStepFailure?: any;
}

/**
 * Azure ML Update Resource management activity.
 */
export interface AzureMLUpdateResourceActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureMLUpdateResource";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * Name of the Trained Model module in the Web Service experiment to be updated. Type: string (or
   * Expression with resultType string).
   */
  trainedModelName: any;
  /**
   * Name of Azure Storage linked service holding the .ilearner file that will be uploaded by the
   * update operation.
   */
  trainedModelLinkedServiceName: LinkedServiceReference;
  /**
   * The relative file path in trainedModelLinkedService to represent the .ilearner file that will
   * be uploaded by the update operation.  Type: string (or Expression with resultType string).
   */
  trainedModelFilePath: any;
}

/**
 * Azure ML WebService Input/Output file
 */
export interface AzureMLWebServiceFile {
  /**
   * The relative file path, including container name, in the Azure Blob Storage specified by the
   * LinkedService. Type: string (or Expression with resultType string).
   */
  filePath: any;
  /**
   * Reference to an Azure Storage LinkedService, where Azure ML WebService Input/Output file
   * located.
   */
  linkedServiceName: LinkedServiceReference;
}

/**
 * Azure ML Batch Execution activity.
 */
export interface AzureMLBatchExecutionActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureMLBatchExecution";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * Key,Value pairs to be passed to the Azure ML Batch Execution Service endpoint. Keys must match
   * the names of web service parameters defined in the published Azure ML web service. Values will
   * be passed in the GlobalParameters property of the Azure ML batch execution request.
   */
  globalParameters?: { [propertyName: string]: any };
  /**
   * Key,Value pairs, mapping the names of Azure ML endpoint's Web Service Outputs to
   * AzureMLWebServiceFile objects specifying the output Blob locations. This information will be
   * passed in the WebServiceOutputs property of the Azure ML batch execution request.
   */
  webServiceOutputs?: { [propertyName: string]: AzureMLWebServiceFile };
  /**
   * Key,Value pairs, mapping the names of Azure ML endpoint's Web Service Inputs to
   * AzureMLWebServiceFile objects specifying the input Blob locations.. This information will be
   * passed in the WebServiceInputs property of the Azure ML batch execution request.
   */
  webServiceInputs?: { [propertyName: string]: AzureMLWebServiceFile };
}

/**
 * Contains the possible cases for CompressionReadSettings.
 */
export type CompressionReadSettingsUnion = CompressionReadSettings | ZipDeflateReadSettings;

/**
 * Compression read settings.
 */
export interface CompressionReadSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "CompressionReadSettings";
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * The ZipDeflate compression read settings.
 */
export interface ZipDeflateReadSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "ZipDeflateReadSettings";
  /**
   * Preserve the zip file name as folder path. Type: boolean (or Expression with resultType
   * boolean).
   */
  preserveZipFileNameAsFolder?: any;
}

/**
 * Contains the possible cases for FormatReadSettings.
 */
export type FormatReadSettingsUnion = FormatReadSettings | BinaryReadSettings | XmlReadSettings | JsonReadSettings | DelimitedTextReadSettings;

/**
 * Format read settings.
 */
export interface FormatReadSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "FormatReadSettings";
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Binary read settings.
 */
export interface BinaryReadSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "BinaryReadSettings";
  /**
   * Compression settings.
   */
  compressionProperties?: CompressionReadSettingsUnion;
}

/**
 * Xml read settings.
 */
export interface XmlReadSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "XmlReadSettings";
  /**
   * Compression settings.
   */
  compressionProperties?: CompressionReadSettingsUnion;
  /**
   * Indicates what validation method is used when reading the xml files. Allowed values: 'none',
   * 'xsd', or 'dtd'. Type: string (or Expression with resultType string).
   */
  validationMode?: any;
  /**
   * Namespace uri to prefix mappings to override the prefixes in column names when namespace is
   * enabled, if no prefix is defined for a namespace uri, the prefix of xml element/attribute name
   * in the xml data file will be used. Example: "{"http://www.example.com/xml":"prefix"}" Type:
   * object (or Expression with resultType object).
   */
  namespacePrefixes?: any;
}

/**
 * Json read settings.
 */
export interface JsonReadSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "JsonReadSettings";
  /**
   * Compression settings.
   */
  compressionProperties?: CompressionReadSettingsUnion;
}

/**
 * Delimited text read settings.
 */
export interface DelimitedTextReadSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "DelimitedTextReadSettings";
  /**
   * Indicates the number of non-empty rows to skip when reading data from input files. Type:
   * integer (or Expression with resultType integer).
   */
  skipLineCount?: any;
  /**
   * Compression settings.
   */
  compressionProperties?: CompressionReadSettingsUnion;
}

/**
 * Distcp settings.
 */
export interface DistcpSettings {
  /**
   * Specifies the Yarn ResourceManager endpoint. Type: string (or Expression with resultType
   * string).
   */
  resourceManagerEndpoint: any;
  /**
   * Specifies an existing folder path which will be used to store temp Distcp command script. The
   * script file is generated by ADF and will be removed after Copy job finished. Type: string (or
   * Expression with resultType string).
   */
  tempScriptPath: any;
  /**
   * Specifies the Distcp options. Type: string (or Expression with resultType string).
   */
  distcpOptions?: any;
}

/**
 * Contains the possible cases for StoreReadSettings.
 */
export type StoreReadSettingsUnion = StoreReadSettings | HdfsReadSettings | HttpReadSettings | SftpReadSettings | FtpReadSettings | GoogleCloudStorageReadSettings | AzureFileStorageReadSettings | FileServerReadSettings | AmazonS3ReadSettings | AzureDataLakeStoreReadSettings | AzureBlobFSReadSettings | AzureBlobStorageReadSettings;

/**
 * Connector read setting.
 */
export interface StoreReadSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "StoreReadSettings";
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * HDFS read settings.
 */
export interface HdfsReadSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "HdfsReadSettings";
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * If true, files under the folder path will be read recursively. Default is true. Type: boolean
   * (or Expression with resultType boolean).
   */
  recursive?: any;
  /**
   * HDFS wildcardFolderPath. Type: string (or Expression with resultType string).
   */
  wildcardFolderPath?: any;
  /**
   * HDFS wildcardFileName. Type: string (or Expression with resultType string).
   */
  wildcardFileName?: any;
  /**
   * Point to a text file that lists each file (relative path to the path configured in the
   * dataset) that you want to copy. Type: string (or Expression with resultType string).
   */
  fileListPath?: any;
  /**
   * Indicates whether to enable partition discovery.
   */
  enablePartitionDiscovery?: boolean;
  /**
   * Specify the root path where partition discovery starts from. Type: string (or Expression with
   * resultType string).
   */
  partitionRootPath?: any;
  /**
   * The start of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeStart?: any;
  /**
   * The end of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeEnd?: any;
  /**
   * Specifies Distcp-related settings.
   */
  distcpSettings?: DistcpSettings;
}

/**
 * Sftp read settings.
 */
export interface HttpReadSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "HttpReadSettings";
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * The HTTP method used to call the RESTful API. The default is GET. Type: string (or Expression
   * with resultType string).
   */
  requestMethod?: any;
  /**
   * The HTTP request body to the RESTful API if requestMethod is POST. Type: string (or Expression
   * with resultType string).
   */
  requestBody?: any;
  /**
   * The additional HTTP headers in the request to the RESTful API. Type: string (or Expression
   * with resultType string).
   */
  additionalHeaders?: any;
  /**
   * Specifies the timeout for a HTTP client to get HTTP response from HTTP server.
   */
  requestTimeout?: any;
  /**
   * Indicates whether to enable partition discovery.
   */
  enablePartitionDiscovery?: boolean;
  /**
   * Specify the root path where partition discovery starts from. Type: string (or Expression with
   * resultType string).
   */
  partitionRootPath?: any;
}

/**
 * Sftp read settings.
 */
export interface SftpReadSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "SftpReadSettings";
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * If true, files under the folder path will be read recursively. Default is true. Type: boolean
   * (or Expression with resultType boolean).
   */
  recursive?: any;
  /**
   * Sftp wildcardFolderPath. Type: string (or Expression with resultType string).
   */
  wildcardFolderPath?: any;
  /**
   * Sftp wildcardFileName. Type: string (or Expression with resultType string).
   */
  wildcardFileName?: any;
  /**
   * Indicates whether to enable partition discovery.
   */
  enablePartitionDiscovery?: boolean;
  /**
   * Specify the root path where partition discovery starts from. Type: string (or Expression with
   * resultType string).
   */
  partitionRootPath?: any;
  /**
   * Point to a text file that lists each file (relative path to the path configured in the
   * dataset) that you want to copy. Type: string (or Expression with resultType string).
   */
  fileListPath?: any;
  /**
   * Indicates whether the source files need to be deleted after copy completion. Default is false.
   * Type: boolean (or Expression with resultType boolean).
   */
  deleteFilesAfterCompletion?: any;
  /**
   * The start of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeStart?: any;
  /**
   * The end of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeEnd?: any;
}

/**
 * Ftp read settings.
 */
export interface FtpReadSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "FtpReadSettings";
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * If true, files under the folder path will be read recursively. Default is true. Type: boolean
   * (or Expression with resultType boolean).
   */
  recursive?: any;
  /**
   * Ftp wildcardFolderPath. Type: string (or Expression with resultType string).
   */
  wildcardFolderPath?: any;
  /**
   * Ftp wildcardFileName. Type: string (or Expression with resultType string).
   */
  wildcardFileName?: any;
  /**
   * Indicates whether to enable partition discovery.
   */
  enablePartitionDiscovery?: boolean;
  /**
   * Specify the root path where partition discovery starts from. Type: string (or Expression with
   * resultType string).
   */
  partitionRootPath?: any;
  /**
   * Indicates whether the source files need to be deleted after copy completion. Default is false.
   * Type: boolean (or Expression with resultType boolean).
   */
  deleteFilesAfterCompletion?: any;
  /**
   * Point to a text file that lists each file (relative path to the path configured in the
   * dataset) that you want to copy. Type: string (or Expression with resultType string).
   */
  fileListPath?: any;
  /**
   * Specify whether to use binary transfer mode for FTP stores.
   */
  useBinaryTransfer?: boolean;
}

/**
 * Google Cloud Storage read settings.
 */
export interface GoogleCloudStorageReadSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "GoogleCloudStorageReadSettings";
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * If true, files under the folder path will be read recursively. Default is true. Type: boolean
   * (or Expression with resultType boolean).
   */
  recursive?: any;
  /**
   * Google Cloud Storage wildcardFolderPath. Type: string (or Expression with resultType string).
   */
  wildcardFolderPath?: any;
  /**
   * Google Cloud Storage wildcardFileName. Type: string (or Expression with resultType string).
   */
  wildcardFileName?: any;
  /**
   * The prefix filter for the Google Cloud Storage object name. Type: string (or Expression with
   * resultType string).
   */
  prefix?: any;
  /**
   * Point to a text file that lists each file (relative path to the path configured in the
   * dataset) that you want to copy. Type: string (or Expression with resultType string).
   */
  fileListPath?: any;
  /**
   * Indicates whether to enable partition discovery.
   */
  enablePartitionDiscovery?: boolean;
  /**
   * Specify the root path where partition discovery starts from. Type: string (or Expression with
   * resultType string).
   */
  partitionRootPath?: any;
  /**
   * Indicates whether the source files need to be deleted after copy completion. Default is false.
   * Type: boolean (or Expression with resultType boolean).
   */
  deleteFilesAfterCompletion?: any;
  /**
   * The start of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeStart?: any;
  /**
   * The end of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeEnd?: any;
}

/**
 * Azure File Storage read settings.
 */
export interface AzureFileStorageReadSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureFileStorageReadSettings";
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * If true, files under the folder path will be read recursively. Default is true. Type: boolean
   * (or Expression with resultType boolean).
   */
  recursive?: any;
  /**
   * Azure File Storage wildcardFolderPath. Type: string (or Expression with resultType string).
   */
  wildcardFolderPath?: any;
  /**
   * Azure File Storage wildcardFileName. Type: string (or Expression with resultType string).
   */
  wildcardFileName?: any;
  /**
   * The prefix filter for the Azure File name starting from root path. Type: string (or Expression
   * with resultType string).
   */
  prefix?: any;
  /**
   * Point to a text file that lists each file (relative path to the path configured in the
   * dataset) that you want to copy. Type: string (or Expression with resultType string).
   */
  fileListPath?: any;
  /**
   * Indicates whether to enable partition discovery.
   */
  enablePartitionDiscovery?: boolean;
  /**
   * Specify the root path where partition discovery starts from. Type: string (or Expression with
   * resultType string).
   */
  partitionRootPath?: any;
  /**
   * Indicates whether the source files need to be deleted after copy completion. Default is false.
   * Type: boolean (or Expression with resultType boolean).
   */
  deleteFilesAfterCompletion?: any;
  /**
   * The start of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeStart?: any;
  /**
   * The end of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeEnd?: any;
}

/**
 * File server read settings.
 */
export interface FileServerReadSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "FileServerReadSettings";
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * If true, files under the folder path will be read recursively. Default is true. Type: boolean
   * (or Expression with resultType boolean).
   */
  recursive?: any;
  /**
   * FileServer wildcardFolderPath. Type: string (or Expression with resultType string).
   */
  wildcardFolderPath?: any;
  /**
   * FileServer wildcardFileName. Type: string (or Expression with resultType string).
   */
  wildcardFileName?: any;
  /**
   * Point to a text file that lists each file (relative path to the path configured in the
   * dataset) that you want to copy. Type: string (or Expression with resultType string).
   */
  fileListPath?: any;
  /**
   * Indicates whether to enable partition discovery.
   */
  enablePartitionDiscovery?: boolean;
  /**
   * Specify the root path where partition discovery starts from. Type: string (or Expression with
   * resultType string).
   */
  partitionRootPath?: any;
  /**
   * Indicates whether the source files need to be deleted after copy completion. Default is false.
   * Type: boolean (or Expression with resultType boolean).
   */
  deleteFilesAfterCompletion?: any;
  /**
   * The start of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeStart?: any;
  /**
   * The end of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeEnd?: any;
  /**
   * Specify a filter to be used to select a subset of files in the folderPath rather than all
   * files. Type: string (or Expression with resultType string).
   */
  fileFilter?: any;
}

/**
 * Azure data lake store read settings.
 */
export interface AmazonS3ReadSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "AmazonS3ReadSettings";
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * If true, files under the folder path will be read recursively. Default is true. Type: boolean
   * (or Expression with resultType boolean).
   */
  recursive?: any;
  /**
   * AmazonS3 wildcardFolderPath. Type: string (or Expression with resultType string).
   */
  wildcardFolderPath?: any;
  /**
   * AmazonS3 wildcardFileName. Type: string (or Expression with resultType string).
   */
  wildcardFileName?: any;
  /**
   * The prefix filter for the S3 object name. Type: string (or Expression with resultType string).
   */
  prefix?: any;
  /**
   * Point to a text file that lists each file (relative path to the path configured in the
   * dataset) that you want to copy. Type: string (or Expression with resultType string).
   */
  fileListPath?: any;
  /**
   * Indicates whether to enable partition discovery.
   */
  enablePartitionDiscovery?: boolean;
  /**
   * Specify the root path where partition discovery starts from. Type: string (or Expression with
   * resultType string).
   */
  partitionRootPath?: any;
  /**
   * Indicates whether the source files need to be deleted after copy completion. Default is false.
   * Type: boolean (or Expression with resultType boolean).
   */
  deleteFilesAfterCompletion?: any;
  /**
   * The start of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeStart?: any;
  /**
   * The end of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeEnd?: any;
}

/**
 * Azure data lake store read settings.
 */
export interface AzureDataLakeStoreReadSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureDataLakeStoreReadSettings";
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * If true, files under the folder path will be read recursively. Default is true. Type: boolean
   * (or Expression with resultType boolean).
   */
  recursive?: any;
  /**
   * ADLS wildcardFolderPath. Type: string (or Expression with resultType string).
   */
  wildcardFolderPath?: any;
  /**
   * ADLS wildcardFileName. Type: string (or Expression with resultType string).
   */
  wildcardFileName?: any;
  /**
   * Point to a text file that lists each file (relative path to the path configured in the
   * dataset) that you want to copy. Type: string (or Expression with resultType string).
   */
  fileListPath?: any;
  /**
   * Lists files after the value (exclusive) based on file/folder names lexicographical order.
   * Applies under the folderPath in data set, and filter files/sub-folders under the folderPath.
   * Type: string (or Expression with resultType string).
   */
  listAfter?: any;
  /**
   * Lists files before the value (inclusive) based on file/folder names lexicographical order.
   * Applies under the folderPath in data set, and filter files/sub-folders under the folderPath.
   * Type: string (or Expression with resultType string).
   */
  listBefore?: any;
  /**
   * Indicates whether to enable partition discovery.
   */
  enablePartitionDiscovery?: boolean;
  /**
   * Specify the root path where partition discovery starts from. Type: string (or Expression with
   * resultType string).
   */
  partitionRootPath?: any;
  /**
   * Indicates whether the source files need to be deleted after copy completion. Default is false.
   * Type: boolean (or Expression with resultType boolean).
   */
  deleteFilesAfterCompletion?: any;
  /**
   * The start of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeStart?: any;
  /**
   * The end of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeEnd?: any;
}

/**
 * Azure blobFS read settings.
 */
export interface AzureBlobFSReadSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureBlobFSReadSettings";
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * If true, files under the folder path will be read recursively. Default is true. Type: boolean
   * (or Expression with resultType boolean).
   */
  recursive?: any;
  /**
   * Azure blobFS wildcardFolderPath. Type: string (or Expression with resultType string).
   */
  wildcardFolderPath?: any;
  /**
   * Azure blobFS wildcardFileName. Type: string (or Expression with resultType string).
   */
  wildcardFileName?: any;
  /**
   * Point to a text file that lists each file (relative path to the path configured in the
   * dataset) that you want to copy. Type: string (or Expression with resultType string).
   */
  fileListPath?: any;
  /**
   * Indicates whether to enable partition discovery.
   */
  enablePartitionDiscovery?: boolean;
  /**
   * Specify the root path where partition discovery starts from. Type: string (or Expression with
   * resultType string).
   */
  partitionRootPath?: any;
  /**
   * Indicates whether the source files need to be deleted after copy completion. Default is false.
   * Type: boolean (or Expression with resultType boolean).
   */
  deleteFilesAfterCompletion?: any;
  /**
   * The start of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeStart?: any;
  /**
   * The end of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeEnd?: any;
}

/**
 * Azure blob read settings.
 */
export interface AzureBlobStorageReadSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureBlobStorageReadSettings";
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * If true, files under the folder path will be read recursively. Default is true. Type: boolean
   * (or Expression with resultType boolean).
   */
  recursive?: any;
  /**
   * Azure blob wildcardFolderPath. Type: string (or Expression with resultType string).
   */
  wildcardFolderPath?: any;
  /**
   * Azure blob wildcardFileName. Type: string (or Expression with resultType string).
   */
  wildcardFileName?: any;
  /**
   * The prefix filter for the Azure Blob name. Type: string (or Expression with resultType
   * string).
   */
  prefix?: any;
  /**
   * Point to a text file that lists each file (relative path to the path configured in the
   * dataset) that you want to copy. Type: string (or Expression with resultType string).
   */
  fileListPath?: any;
  /**
   * Indicates whether to enable partition discovery.
   */
  enablePartitionDiscovery?: boolean;
  /**
   * Specify the root path where partition discovery starts from. Type: string (or Expression with
   * resultType string).
   */
  partitionRootPath?: any;
  /**
   * Indicates whether the source files need to be deleted after copy completion. Default is false.
   * Type: boolean (or Expression with resultType boolean).
   */
  deleteFilesAfterCompletion?: any;
  /**
   * The start of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeStart?: any;
  /**
   * The end of file's modified datetime. Type: string (or Expression with resultType string).
   */
  modifiedDatetimeEnd?: any;
}

/**
 * Activity to get metadata of dataset
 */
export interface GetMetadataActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "GetMetadata";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * GetMetadata activity dataset reference.
   */
  dataset: DatasetReference;
  /**
   * Fields of metadata to get from dataset.
   */
  fieldList?: any[];
  /**
   * GetMetadata activity store settings.
   */
  storeSettings?: StoreReadSettingsUnion;
  /**
   * GetMetadata activity format settings.
   */
  formatSettings?: FormatReadSettingsUnion;
}

/**
 * Web activity authentication properties.
 */
export interface WebActivityAuthentication {
  /**
   * Web activity authentication (Basic/ClientCertificate/MSI)
   */
  type: string;
  /**
   * Base64-encoded contents of a PFX file.
   */
  pfx?: SecretBaseUnion;
  /**
   * Web activity authentication user name for basic authentication.
   */
  username?: string;
  /**
   * Password for the PFX file or basic authentication.
   */
  password?: SecretBaseUnion;
  /**
   * Resource for which Azure Auth token will be requested when using MSI Authentication.
   */
  resource?: string;
}

/**
 * Web activity.
 */
export interface WebActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "WebActivity";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * Rest API method for target endpoint. Possible values include: 'GET', 'POST', 'PUT', 'DELETE'
   */
  method: WebActivityMethod;
  /**
   * Web activity target endpoint and path. Type: string (or Expression with resultType string).
   */
  url: any;
  /**
   * Represents the headers that will be sent to the request. For example, to set the language and
   * type on a request: "headers" : { "Accept-Language": "en-us", "Content-Type":
   * "application/json" }. Type: string (or Expression with resultType string).
   */
  headers?: any;
  /**
   * Represents the payload that will be sent to the endpoint. Required for POST/PUT method, not
   * allowed for GET method Type: string (or Expression with resultType string).
   */
  body?: any;
  /**
   * Authentication method used for calling the endpoint.
   */
  authentication?: WebActivityAuthentication;
  /**
   * List of datasets passed to web endpoint.
   */
  datasets?: DatasetReference[];
  /**
   * List of linked services passed to web endpoint.
   */
  linkedServices?: LinkedServiceReference[];
  /**
   * The integration runtime reference.
   */
  connectVia?: IntegrationRuntimeReference;
}

/**
 * Contains the possible cases for CopySource.
 */
export type CopySourceUnion = CopySource | SharePointOnlineListSource | SnowflakeSource | HttpSource | AzureBlobFSSource | AzureDataLakeStoreSource | Office365Source | CosmosDbMongoDbApiSource | MongoDbV2Source | MongoDbSource | WebSource | OracleSource | AzureDataExplorerSource | HdfsSource | FileSystemSource | RestSource | SalesforceServiceCloudSource | ODataSource | MicrosoftAccessSource | RelationalSource | CommonDataServiceForAppsSource | DynamicsCrmSource | DynamicsSource | CosmosDbSqlApiSource | DocumentDbCollectionSource | BlobSource | TabularSourceUnion | BinarySource | OrcSource | XmlSource | JsonSource | DelimitedTextSource | ParquetSource | ExcelSource | AvroSource;

/**
 * A copy activity source.
 */
export interface CopySource {
  /**
   * Polymorphic Discriminator
   */
  type: "CopySource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * A copy activity source for sharePoint online list source.
 */
export interface SharePointOnlineListSource {
  /**
   * Polymorphic Discriminator
   */
  type: "SharePointOnlineListSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * The OData query to filter the data in SharePoint Online list. For example, "$top=1". Type:
   * string (or Expression with resultType string).
   */
  query?: any;
  /**
   * The wait time to get a response from SharePoint Online. Default value is 5 minutes (00:05:00).
   * Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  httpRequestTimeout?: any;
}

/**
 * Contains the possible cases for ExportSettings.
 */
export type ExportSettingsUnion = ExportSettings | SnowflakeExportCopyCommand;

/**
 * Export command settings.
 */
export interface ExportSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "ExportSettings";
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Snowflake export command settings.
 */
export interface SnowflakeExportCopyCommand {
  /**
   * Polymorphic Discriminator
   */
  type: "SnowflakeExportCopyCommand";
  /**
   * Additional copy options directly passed to snowflake Copy Command. Type: key value pairs
   * (value should be string type) (or Expression with resultType object). Example:
   * "additionalCopyOptions": { "DATE_FORMAT": "MM/DD/YYYY", "TIME_FORMAT": "'HH24:MI:SS.FF'" }
   */
  additionalCopyOptions?: { [propertyName: string]: any };
  /**
   * Additional format options directly passed to snowflake Copy Command. Type: key value pairs
   * (value should be string type) (or Expression with resultType object). Example:
   * "additionalFormatOptions": { "OVERWRITE": "TRUE", "MAX_FILE_SIZE": "'FALSE'" }
   */
  additionalFormatOptions?: { [propertyName: string]: any };
}

/**
 * A copy activity snowflake source.
 */
export interface SnowflakeSource {
  /**
   * Polymorphic Discriminator
   */
  type: "SnowflakeSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Snowflake Sql query. Type: string (or Expression with resultType string).
   */
  query?: any;
  /**
   * Snowflake export settings.
   */
  exportSettings?: SnowflakeExportCopyCommand;
}

/**
 * A copy activity source for an HTTP file.
 */
export interface HttpSource {
  /**
   * Polymorphic Discriminator
   */
  type: "HttpSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Specifies the timeout for a HTTP client to get HTTP response from HTTP server. The default
   * value is equivalent to System.Net.HttpWebRequest.Timeout. Type: string (or Expression with
   * resultType string), pattern: ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  httpRequestTimeout?: any;
}

/**
 * A copy activity Azure BlobFS source.
 */
export interface AzureBlobFSSource {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureBlobFSSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Treat empty as null. Type: boolean (or Expression with resultType boolean).
   */
  treatEmptyAsNull?: any;
  /**
   * Number of header lines to skip from each blob. Type: integer (or Expression with resultType
   * integer).
   */
  skipHeaderLineCount?: any;
  /**
   * If true, files under the folder path will be read recursively. Default is true. Type: boolean
   * (or Expression with resultType boolean).
   */
  recursive?: any;
}

/**
 * A copy activity Azure Data Lake source.
 */
export interface AzureDataLakeStoreSource {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureDataLakeStoreSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * If true, files under the folder path will be read recursively. Default is true. Type: boolean
   * (or Expression with resultType boolean).
   */
  recursive?: any;
}

/**
 * A copy activity source for an Office 365 service.
 */
export interface Office365Source {
  /**
   * Polymorphic Discriminator
   */
  type: "Office365Source";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * The groups containing all the users. Type: array of strings (or Expression with resultType
   * array of strings).
   */
  allowedGroups?: any;
  /**
   * The user scope uri. Type: string (or Expression with resultType string).
   */
  userScopeFilterUri?: any;
  /**
   * The Column to apply the <paramref name="StartTime"/> and <paramref name="EndTime"/>. Type:
   * string (or Expression with resultType string).
   */
  dateFilterColumn?: any;
  /**
   * Start time of the requested range for this dataset. Type: string (or Expression with
   * resultType string).
   */
  startTime?: any;
  /**
   * End time of the requested range for this dataset. Type: string (or Expression with resultType
   * string).
   */
  endTime?: any;
  /**
   * The columns to be read out from the Office 365 table. Type: array of objects (or Expression
   * with resultType array of objects). Example: [ { "name": "Id" }, { "name": "CreatedDateTime" }
   * ]
   */
  outputColumns?: any;
}

/**
 * Specify the column name and value of additional columns.
 */
export interface AdditionalColumns {
  /**
   * Additional column name. Type: string (or Expression with resultType string).
   */
  name?: any;
  /**
   * Additional column value. Type: string (or Expression with resultType string).
   */
  value?: any;
}

/**
 * Cursor methods for Mongodb query
 */
export interface MongoDbCursorMethodsProperties {
  /**
   * Specifies the fields to return in the documents that match the query filter. To return all
   * fields in the matching documents, omit this parameter. Type: string (or Expression with
   * resultType string).
   */
  project?: any;
  /**
   * Specifies the order in which the query returns matching documents. Type: string (or Expression
   * with resultType string). Type: string (or Expression with resultType string).
   */
  sort?: any;
  /**
   * Specifies the how many documents skipped and where MongoDB begins returning results. This
   * approach may be useful in implementing paginated results. Type: integer (or Expression with
   * resultType integer).
   */
  skip?: any;
  /**
   * Specifies the maximum number of documents the server returns. limit() is analogous to the
   * LIMIT statement in a SQL database. Type: integer (or Expression with resultType integer).
   */
  limit?: any;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * A copy activity source for a CosmosDB (MongoDB API) database.
 */
export interface CosmosDbMongoDbApiSource {
  /**
   * Polymorphic Discriminator
   */
  type: "CosmosDbMongoDbApiSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Specifies selection filter using query operators. To return all documents in a collection,
   * omit this parameter or pass an empty document ({}). Type: string (or Expression with
   * resultType string).
   */
  filter?: any;
  /**
   * Cursor methods for Mongodb query.
   */
  cursorMethods?: MongoDbCursorMethodsProperties;
  /**
   * Specifies the number of documents to return in each batch of the response from MongoDB
   * instance. In most cases, modifying the batch size will not affect the user or the application.
   * This property's main purpose is to avoid hit the limitation of response size. Type: integer
   * (or Expression with resultType integer).
   */
  batchSize?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity source for a MongoDB database.
 */
export interface MongoDbV2Source {
  /**
   * Polymorphic Discriminator
   */
  type: "MongoDbV2Source";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Specifies selection filter using query operators. To return all documents in a collection,
   * omit this parameter or pass an empty document ({}). Type: string (or Expression with
   * resultType string).
   */
  filter?: any;
  /**
   * Cursor methods for Mongodb query
   */
  cursorMethods?: MongoDbCursorMethodsProperties;
  /**
   * Specifies the number of documents to return in each batch of the response from MongoDB
   * instance. In most cases, modifying the batch size will not affect the user or the application.
   * This property's main purpose is to avoid hit the limitation of response size. Type: integer
   * (or Expression with resultType integer).
   */
  batchSize?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity source for a MongoDB database.
 */
export interface MongoDbSource {
  /**
   * Polymorphic Discriminator
   */
  type: "MongoDbSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Database query. Should be a SQL-92 query expression. Type: string (or Expression with
   * resultType string).
   */
  query?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity source for web page table.
 */
export interface WebSource {
  /**
   * Polymorphic Discriminator
   */
  type: "WebSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * The settings that will be leveraged for Oracle source partitioning.
 */
export interface OraclePartitionSettings {
  /**
   * Names of the physical partitions of Oracle table.
   */
  partitionNames?: any;
  /**
   * The name of the column in integer type that will be used for proceeding range partitioning.
   * Type: string (or Expression with resultType string).
   */
  partitionColumnName?: any;
  /**
   * The maximum value of column specified in partitionColumnName that will be used for proceeding
   * range partitioning. Type: string (or Expression with resultType string).
   */
  partitionUpperBound?: any;
  /**
   * The minimum value of column specified in partitionColumnName that will be used for proceeding
   * range partitioning. Type: string (or Expression with resultType string).
   */
  partitionLowerBound?: any;
}

/**
 * A copy activity Oracle source.
 */
export interface OracleSource {
  /**
   * Polymorphic Discriminator
   */
  type: "OracleSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Oracle reader query. Type: string (or Expression with resultType string).
   */
  oracleReaderQuery?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * The partition mechanism that will be used for Oracle read in parallel. Possible values
   * include: 'None', 'PhysicalPartitionsOfTable', 'DynamicRange'
   */
  partitionOption?: OraclePartitionOption;
  /**
   * The settings that will be leveraged for Oracle source partitioning.
   */
  partitionSettings?: OraclePartitionSettings;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity Azure Data Explorer (Kusto) source.
 */
export interface AzureDataExplorerSource {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureDataExplorerSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Database query. Should be a Kusto Query Language (KQL) query. Type: string (or Expression with
   * resultType string).
   */
  query: any;
  /**
   * The name of the Boolean option that controls whether truncation is applied to result-sets that
   * go beyond a certain row-count limit.
   */
  noTruncation?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9]))..
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity HDFS source.
 */
export interface HdfsSource {
  /**
   * Polymorphic Discriminator
   */
  type: "HdfsSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * If true, files under the folder path will be read recursively. Default is true. Type: boolean
   * (or Expression with resultType boolean).
   */
  recursive?: any;
  /**
   * Specifies Distcp-related settings.
   */
  distcpSettings?: DistcpSettings;
}

/**
 * A copy activity file system source.
 */
export interface FileSystemSource {
  /**
   * Polymorphic Discriminator
   */
  type: "FileSystemSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * If true, files under the folder path will be read recursively. Default is true. Type: boolean
   * (or Expression with resultType boolean).
   */
  recursive?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity Rest service source.
 */
export interface RestSource {
  /**
   * Polymorphic Discriminator
   */
  type: "RestSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * The HTTP method used to call the RESTful API. The default is GET. Type: string (or Expression
   * with resultType string).
   */
  requestMethod?: any;
  /**
   * The HTTP request body to the RESTful API if requestMethod is POST. Type: string (or Expression
   * with resultType string).
   */
  requestBody?: any;
  /**
   * The additional HTTP headers in the request to the RESTful API. Type: string (or Expression
   * with resultType string).
   */
  additionalHeaders?: any;
  /**
   * The pagination rules to compose next page requests. Type: string (or Expression with
   * resultType string).
   */
  paginationRules?: any;
  /**
   * The timeout (TimeSpan) to get an HTTP response. It is the timeout to get a response, not the
   * timeout to read response data. Default value: 00:01:40. Type: string (or Expression with
   * resultType string), pattern: ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  httpRequestTimeout?: any;
  /**
   * The time to await before sending next page request.
   */
  requestInterval?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity Salesforce Service Cloud source.
 */
export interface SalesforceServiceCloudSource {
  /**
   * Polymorphic Discriminator
   */
  type: "SalesforceServiceCloudSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Database query. Type: string (or Expression with resultType string).
   */
  query?: any;
  /**
   * The read behavior for the operation. Default is Query. Possible values include: 'Query',
   * 'QueryAll'
   */
  readBehavior?: SalesforceSourceReadBehavior;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity source for OData source.
 */
export interface ODataSource {
  /**
   * Polymorphic Discriminator
   */
  type: "ODataSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * OData query. For example, "$top=1". Type: string (or Expression with resultType string).
   */
  query?: any;
  /**
   * The timeout (TimeSpan) to get an HTTP response. It is the timeout to get a response, not the
   * timeout to read response data. Default value: 00:05:00. Type: string (or Expression with
   * resultType string), pattern: ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  httpRequestTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity source for Microsoft Access.
 */
export interface MicrosoftAccessSource {
  /**
   * Polymorphic Discriminator
   */
  type: "MicrosoftAccessSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Database query. Type: string (or Expression with resultType string).
   */
  query?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity source for various relational databases.
 */
export interface RelationalSource {
  /**
   * Polymorphic Discriminator
   */
  type: "RelationalSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Database query. Type: string (or Expression with resultType string).
   */
  query?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity Common Data Service for Apps source.
 */
export interface CommonDataServiceForAppsSource {
  /**
   * Polymorphic Discriminator
   */
  type: "CommonDataServiceForAppsSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * FetchXML is a proprietary query language that is used in Microsoft Common Data Service for
   * Apps (online & on-premises). Type: string (or Expression with resultType string).
   */
  query?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity Dynamics CRM source.
 */
export interface DynamicsCrmSource {
  /**
   * Polymorphic Discriminator
   */
  type: "DynamicsCrmSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * FetchXML is a proprietary query language that is used in Microsoft Dynamics CRM (online &
   * on-premises). Type: string (or Expression with resultType string).
   */
  query?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity Dynamics source.
 */
export interface DynamicsSource {
  /**
   * Polymorphic Discriminator
   */
  type: "DynamicsSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * FetchXML is a proprietary query language that is used in Microsoft Dynamics (online &
   * on-premises). Type: string (or Expression with resultType string).
   */
  query?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity Azure CosmosDB (SQL API) Collection source.
 */
export interface CosmosDbSqlApiSource {
  /**
   * Polymorphic Discriminator
   */
  type: "CosmosDbSqlApiSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * SQL API query. Type: string (or Expression with resultType string).
   */
  query?: any;
  /**
   * Page size of the result. Type: integer (or Expression with resultType integer).
   */
  pageSize?: any;
  /**
   * Preferred regions. Type: array of strings (or Expression with resultType array of strings).
   */
  preferredRegions?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity Document Database Collection source.
 */
export interface DocumentDbCollectionSource {
  /**
   * Polymorphic Discriminator
   */
  type: "DocumentDbCollectionSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Documents query. Type: string (or Expression with resultType string).
   */
  query?: any;
  /**
   * Nested properties separator. Type: string (or Expression with resultType string).
   */
  nestingSeparator?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity Azure Blob source.
 */
export interface BlobSource {
  /**
   * Polymorphic Discriminator
   */
  type: "BlobSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Treat empty as null. Type: boolean (or Expression with resultType boolean).
   */
  treatEmptyAsNull?: any;
  /**
   * Number of header lines to skip from each blob. Type: integer (or Expression with resultType
   * integer).
   */
  skipHeaderLineCount?: any;
  /**
   * If true, files under the folder path will be read recursively. Default is true. Type: boolean
   * (or Expression with resultType boolean).
   */
  recursive?: any;
}

/**
 * The Amazon S3 settings needed for the interim Amazon S3 when copying from Amazon Redshift with
 * unload. With this, data from Amazon Redshift source will be unloaded into S3 first and then
 * copied into the targeted sink from the interim S3.
 */
export interface RedshiftUnloadSettings {
  /**
   * The name of the Amazon S3 linked service which will be used for the unload operation when
   * copying from the Amazon Redshift source.
   */
  s3LinkedServiceName: LinkedServiceReference;
  /**
   * The bucket of the interim Amazon S3 which will be used to store the unloaded data from Amazon
   * Redshift source. The bucket must be in the same region as the Amazon Redshift source. Type:
   * string (or Expression with resultType string).
   */
  bucketName: any;
}

/**
 * Contains the possible cases for TabularSource.
 */
export type TabularSourceUnion = TabularSource | AmazonRedshiftSource | GoogleAdWordsSource | OracleServiceCloudSource | DynamicsAXSource | ResponsysSource | SalesforceMarketingCloudSource | VerticaSource | NetezzaSource | ZohoSource | XeroSource | SquareSource | SparkSource | ShopifySource | ServiceNowSource | QuickBooksSource | PrestoSource | PhoenixSource | PaypalSource | MarketoSource | AzureMariaDBSource | MariaDBSource | MagentoSource | JiraSource | ImpalaSource | HubspotSource | HiveSource | HBaseSource | GreenplumSource | GoogleBigQuerySource | EloquaSource | DrillSource | CouchbaseSource | ConcurSource | AzurePostgreSqlSource | AmazonMWSSource | CassandraSource | TeradataSource | AzureMySqlSource | SqlDWSource | SqlMISource | AzureSqlSource | SqlServerSource | SqlSource | SapTableSource | SapOpenHubSource | SapHanaSource | SapEccSource | SapCloudForCustomerSource | SalesforceSource | SapBwSource | SybaseSource | PostgreSqlSource | MySqlSource | OdbcSource | Db2Source | InformixSource | AzureTableSource;

/**
 * Copy activity sources of tabular type.
 */
export interface TabularSource {
  /**
   * Polymorphic Discriminator
   */
  type: "TabularSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity source for Amazon Redshift Source.
 */
export interface AmazonRedshiftSource {
  /**
   * Polymorphic Discriminator
   */
  type: "AmazonRedshiftSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * Database query. Type: string (or Expression with resultType string).
   */
  query?: any;
  /**
   * The Amazon S3 settings needed for the interim Amazon S3 when copying from Amazon Redshift with
   * unload. With this, data from Amazon Redshift source will be unloaded into S3 first and then
   * copied into the targeted sink from the interim S3.
   */
  redshiftUnloadSettings?: RedshiftUnloadSettings;
}

/**
 * A copy activity Google AdWords service source.
 */
export interface GoogleAdWordsSource {
  /**
   * Polymorphic Discriminator
   */
  type: "GoogleAdWordsSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Oracle Service Cloud source.
 */
export interface OracleServiceCloudSource {
  /**
   * Polymorphic Discriminator
   */
  type: "OracleServiceCloudSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Dynamics AX source.
 */
export interface DynamicsAXSource {
  /**
   * Polymorphic Discriminator
   */
  type: "DynamicsAXSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
  /**
   * The timeout (TimeSpan) to get an HTTP response. It is the timeout to get a response, not the
   * timeout to read response data. Default value: 00:05:00. Type: string (or Expression with
   * resultType string), pattern: ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  httpRequestTimeout?: any;
}

/**
 * A copy activity Responsys source.
 */
export interface ResponsysSource {
  /**
   * Polymorphic Discriminator
   */
  type: "ResponsysSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Salesforce Marketing Cloud source.
 */
export interface SalesforceMarketingCloudSource {
  /**
   * Polymorphic Discriminator
   */
  type: "SalesforceMarketingCloudSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Vertica source.
 */
export interface VerticaSource {
  /**
   * Polymorphic Discriminator
   */
  type: "VerticaSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * The settings that will be leveraged for Netezza source partitioning.
 */
export interface NetezzaPartitionSettings {
  /**
   * The name of the column in integer type that will be used for proceeding range partitioning.
   * Type: string (or Expression with resultType string).
   */
  partitionColumnName?: any;
  /**
   * The maximum value of column specified in partitionColumnName that will be used for proceeding
   * range partitioning. Type: string (or Expression with resultType string).
   */
  partitionUpperBound?: any;
  /**
   * The minimum value of column specified in partitionColumnName that will be used for proceeding
   * range partitioning. Type: string (or Expression with resultType string).
   */
  partitionLowerBound?: any;
}

/**
 * A copy activity Netezza source.
 */
export interface NetezzaSource {
  /**
   * Polymorphic Discriminator
   */
  type: "NetezzaSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
  /**
   * The partition mechanism that will be used for Netezza read in parallel. Possible values
   * include: 'None', 'DataSlice', 'DynamicRange'
   */
  partitionOption?: NetezzaPartitionOption;
  /**
   * The settings that will be leveraged for Netezza source partitioning.
   */
  partitionSettings?: NetezzaPartitionSettings;
}

/**
 * A copy activity Zoho server source.
 */
export interface ZohoSource {
  /**
   * Polymorphic Discriminator
   */
  type: "ZohoSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Xero Service source.
 */
export interface XeroSource {
  /**
   * Polymorphic Discriminator
   */
  type: "XeroSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Square Service source.
 */
export interface SquareSource {
  /**
   * Polymorphic Discriminator
   */
  type: "SquareSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Spark Server source.
 */
export interface SparkSource {
  /**
   * Polymorphic Discriminator
   */
  type: "SparkSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Shopify Service source.
 */
export interface ShopifySource {
  /**
   * Polymorphic Discriminator
   */
  type: "ShopifySource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity ServiceNow server source.
 */
export interface ServiceNowSource {
  /**
   * Polymorphic Discriminator
   */
  type: "ServiceNowSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity QuickBooks server source.
 */
export interface QuickBooksSource {
  /**
   * Polymorphic Discriminator
   */
  type: "QuickBooksSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Presto server source.
 */
export interface PrestoSource {
  /**
   * Polymorphic Discriminator
   */
  type: "PrestoSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Phoenix server source.
 */
export interface PhoenixSource {
  /**
   * Polymorphic Discriminator
   */
  type: "PhoenixSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Paypal Service source.
 */
export interface PaypalSource {
  /**
   * Polymorphic Discriminator
   */
  type: "PaypalSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Marketo server source.
 */
export interface MarketoSource {
  /**
   * Polymorphic Discriminator
   */
  type: "MarketoSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Azure MariaDB source.
 */
export interface AzureMariaDBSource {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureMariaDBSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity MariaDB server source.
 */
export interface MariaDBSource {
  /**
   * Polymorphic Discriminator
   */
  type: "MariaDBSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Magento server source.
 */
export interface MagentoSource {
  /**
   * Polymorphic Discriminator
   */
  type: "MagentoSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Jira Service source.
 */
export interface JiraSource {
  /**
   * Polymorphic Discriminator
   */
  type: "JiraSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Impala server source.
 */
export interface ImpalaSource {
  /**
   * Polymorphic Discriminator
   */
  type: "ImpalaSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Hubspot Service source.
 */
export interface HubspotSource {
  /**
   * Polymorphic Discriminator
   */
  type: "HubspotSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Hive Server source.
 */
export interface HiveSource {
  /**
   * Polymorphic Discriminator
   */
  type: "HiveSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity HBase server source.
 */
export interface HBaseSource {
  /**
   * Polymorphic Discriminator
   */
  type: "HBaseSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Greenplum Database source.
 */
export interface GreenplumSource {
  /**
   * Polymorphic Discriminator
   */
  type: "GreenplumSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Google BigQuery service source.
 */
export interface GoogleBigQuerySource {
  /**
   * Polymorphic Discriminator
   */
  type: "GoogleBigQuerySource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Eloqua server source.
 */
export interface EloquaSource {
  /**
   * Polymorphic Discriminator
   */
  type: "EloquaSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Drill server source.
 */
export interface DrillSource {
  /**
   * Polymorphic Discriminator
   */
  type: "DrillSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Couchbase server source.
 */
export interface CouchbaseSource {
  /**
   * Polymorphic Discriminator
   */
  type: "CouchbaseSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Concur Service source.
 */
export interface ConcurSource {
  /**
   * Polymorphic Discriminator
   */
  type: "ConcurSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Azure PostgreSQL source.
 */
export interface AzurePostgreSqlSource {
  /**
   * Polymorphic Discriminator
   */
  type: "AzurePostgreSqlSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Amazon Marketplace Web Service source.
 */
export interface AmazonMWSSource {
  /**
   * Polymorphic Discriminator
   */
  type: "AmazonMWSSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * A query to retrieve data from source. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity source for a Cassandra database.
 */
export interface CassandraSource {
  /**
   * Polymorphic Discriminator
   */
  type: "CassandraSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * Database query. Should be a SQL-92 query expression or Cassandra Query Language (CQL) command.
   * Type: string (or Expression with resultType string).
   */
  query?: any;
  /**
   * The consistency level specifies how many Cassandra servers must respond to a read request
   * before returning data to the client application. Cassandra checks the specified number of
   * Cassandra servers for data to satisfy the read request. Must be one of
   * cassandraSourceReadConsistencyLevels. The default value is 'ONE'. It is case-insensitive.
   * Possible values include: 'ALL', 'EACH_QUORUM', 'QUORUM', 'LOCAL_QUORUM', 'ONE', 'TWO',
   * 'THREE', 'LOCAL_ONE', 'SERIAL', 'LOCAL_SERIAL'
   */
  consistencyLevel?: CassandraSourceReadConsistencyLevels;
}

/**
 * The settings that will be leveraged for teradata source partitioning.
 */
export interface TeradataPartitionSettings {
  /**
   * The name of the column that will be used for proceeding range or hash partitioning. Type:
   * string (or Expression with resultType string).
   */
  partitionColumnName?: any;
  /**
   * The maximum value of column specified in partitionColumnName that will be used for proceeding
   * range partitioning. Type: string (or Expression with resultType string).
   */
  partitionUpperBound?: any;
  /**
   * The minimum value of column specified in partitionColumnName that will be used for proceeding
   * range partitioning. Type: string (or Expression with resultType string).
   */
  partitionLowerBound?: any;
}

/**
 * A copy activity Teradata source.
 */
export interface TeradataSource {
  /**
   * Polymorphic Discriminator
   */
  type: "TeradataSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * Teradata query. Type: string (or Expression with resultType string).
   */
  query?: any;
  /**
   * The partition mechanism that will be used for teradata read in parallel. Possible values
   * include: 'None', 'Hash', 'DynamicRange'
   */
  partitionOption?: TeradataPartitionOption;
  /**
   * The settings that will be leveraged for teradata source partitioning.
   */
  partitionSettings?: TeradataPartitionSettings;
}

/**
 * A copy activity Azure MySQL source.
 */
export interface AzureMySqlSource {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureMySqlSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * Database query. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * The settings that will be leveraged for Sql source partitioning.
 */
export interface SqlPartitionSettings {
  /**
   * The name of the column in integer or datetime type that will be used for proceeding
   * partitioning. If not specified, the primary key of the table is auto-detected and used as the
   * partition column. Type: string (or Expression with resultType string).
   */
  partitionColumnName?: any;
  /**
   * The maximum value of the partition column for partition range splitting. This value is used to
   * decide the partition stride, not for filtering the rows in table. All rows in the table or
   * query result will be partitioned and copied. Type: string (or Expression with resultType
   * string).
   */
  partitionUpperBound?: any;
  /**
   * The minimum value of the partition column for partition range splitting. This value is used to
   * decide the partition stride, not for filtering the rows in table. All rows in the table or
   * query result will be partitioned and copied. Type: string (or Expression with resultType
   * string).
   */
  partitionLowerBound?: any;
}

/**
 * A copy activity SQL Data Warehouse source.
 */
export interface SqlDWSource {
  /**
   * Polymorphic Discriminator
   */
  type: "SqlDWSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * SQL Data Warehouse reader query. Type: string (or Expression with resultType string).
   */
  sqlReaderQuery?: any;
  /**
   * Name of the stored procedure for a SQL Data Warehouse source. This cannot be used at the same
   * time as SqlReaderQuery. Type: string (or Expression with resultType string).
   */
  sqlReaderStoredProcedureName?: any;
  /**
   * Value and type setting for stored procedure parameters. Example: "{Parameter1: {value: "1",
   * type: "int"}}". Type: object (or Expression with resultType object), itemType:
   * StoredProcedureParameter.
   */
  storedProcedureParameters?: any;
  /**
   * The partition mechanism that will be used for Sql read in parallel. Possible values include:
   * 'None', 'PhysicalPartitionsOfTable', 'DynamicRange'
   */
  partitionOption?: SqlPartitionOption;
  /**
   * The settings that will be leveraged for Sql source partitioning.
   */
  partitionSettings?: SqlPartitionSettings;
}

/**
 * SQL stored procedure parameter.
 */
export interface StoredProcedureParameter {
  /**
   * Stored procedure parameter value. Type: string (or Expression with resultType string).
   */
  value?: any;
  /**
   * Stored procedure parameter type. Possible values include: 'String', 'Int', 'Int64', 'Decimal',
   * 'Guid', 'Boolean', 'Date'
   */
  type?: StoredProcedureParameterType;
}

/**
 * A copy activity Azure SQL Managed Instance source.
 */
export interface SqlMISource {
  /**
   * Polymorphic Discriminator
   */
  type: "SqlMISource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * SQL reader query. Type: string (or Expression with resultType string).
   */
  sqlReaderQuery?: any;
  /**
   * Name of the stored procedure for a Azure SQL Managed Instance source. This cannot be used at
   * the same time as SqlReaderQuery. Type: string (or Expression with resultType string).
   */
  sqlReaderStoredProcedureName?: any;
  /**
   * Value and type setting for stored procedure parameters. Example: "{Parameter1: {value: "1",
   * type: "int"}}".
   */
  storedProcedureParameters?: { [propertyName: string]: StoredProcedureParameter };
  /**
   * Which additional types to produce.
   */
  produceAdditionalTypes?: any;
  /**
   * The partition mechanism that will be used for Sql read in parallel. Possible values include:
   * 'None', 'PhysicalPartitionsOfTable', 'DynamicRange'
   */
  partitionOption?: SqlPartitionOption;
  /**
   * The settings that will be leveraged for Sql source partitioning.
   */
  partitionSettings?: SqlPartitionSettings;
}

/**
 * A copy activity Azure SQL source.
 */
export interface AzureSqlSource {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureSqlSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * SQL reader query. Type: string (or Expression with resultType string).
   */
  sqlReaderQuery?: any;
  /**
   * Name of the stored procedure for a SQL Database source. This cannot be used at the same time
   * as SqlReaderQuery. Type: string (or Expression with resultType string).
   */
  sqlReaderStoredProcedureName?: any;
  /**
   * Value and type setting for stored procedure parameters. Example: "{Parameter1: {value: "1",
   * type: "int"}}".
   */
  storedProcedureParameters?: { [propertyName: string]: StoredProcedureParameter };
  /**
   * Which additional types to produce.
   */
  produceAdditionalTypes?: any;
  /**
   * The partition mechanism that will be used for Sql read in parallel. Possible values include:
   * 'None', 'PhysicalPartitionsOfTable', 'DynamicRange'
   */
  partitionOption?: SqlPartitionOption;
  /**
   * The settings that will be leveraged for Sql source partitioning.
   */
  partitionSettings?: SqlPartitionSettings;
}

/**
 * A copy activity SQL server source.
 */
export interface SqlServerSource {
  /**
   * Polymorphic Discriminator
   */
  type: "SqlServerSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * SQL reader query. Type: string (or Expression with resultType string).
   */
  sqlReaderQuery?: any;
  /**
   * Name of the stored procedure for a SQL Database source. This cannot be used at the same time
   * as SqlReaderQuery. Type: string (or Expression with resultType string).
   */
  sqlReaderStoredProcedureName?: any;
  /**
   * Value and type setting for stored procedure parameters. Example: "{Parameter1: {value: "1",
   * type: "int"}}".
   */
  storedProcedureParameters?: { [propertyName: string]: StoredProcedureParameter };
  /**
   * Which additional types to produce.
   */
  produceAdditionalTypes?: any;
  /**
   * The partition mechanism that will be used for Sql read in parallel. Possible values include:
   * 'None', 'PhysicalPartitionsOfTable', 'DynamicRange'
   */
  partitionOption?: SqlPartitionOption;
  /**
   * The settings that will be leveraged for Sql source partitioning.
   */
  partitionSettings?: SqlPartitionSettings;
}

/**
 * A copy activity SQL source.
 */
export interface SqlSource {
  /**
   * Polymorphic Discriminator
   */
  type: "SqlSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * SQL reader query. Type: string (or Expression with resultType string).
   */
  sqlReaderQuery?: any;
  /**
   * Name of the stored procedure for a SQL Database source. This cannot be used at the same time
   * as SqlReaderQuery. Type: string (or Expression with resultType string).
   */
  sqlReaderStoredProcedureName?: any;
  /**
   * Value and type setting for stored procedure parameters. Example: "{Parameter1: {value: "1",
   * type: "int"}}".
   */
  storedProcedureParameters?: { [propertyName: string]: StoredProcedureParameter };
  /**
   * Specifies the transaction locking behavior for the SQL source. Allowed values:
   * ReadCommitted/ReadUncommitted/RepeatableRead/Serializable/Snapshot. The default value is
   * ReadCommitted. Type: string (or Expression with resultType string).
   */
  isolationLevel?: any;
  /**
   * The partition mechanism that will be used for Sql read in parallel. Possible values include:
   * 'None', 'PhysicalPartitionsOfTable', 'DynamicRange'
   */
  partitionOption?: SqlPartitionOption;
  /**
   * The settings that will be leveraged for Sql source partitioning.
   */
  partitionSettings?: SqlPartitionSettings;
}

/**
 * The settings that will be leveraged for SAP table source partitioning.
 */
export interface SapTablePartitionSettings {
  /**
   * The name of the column that will be used for proceeding range partitioning. Type: string (or
   * Expression with resultType string).
   */
  partitionColumnName?: any;
  /**
   * The maximum value of column specified in partitionColumnName that will be used for proceeding
   * range partitioning. Type: string (or Expression with resultType string).
   */
  partitionUpperBound?: any;
  /**
   * The minimum value of column specified in partitionColumnName that will be used for proceeding
   * range partitioning. Type: string (or Expression with resultType string).
   */
  partitionLowerBound?: any;
  /**
   * The maximum value of partitions the table will be split into. Type: integer (or Expression
   * with resultType string).
   */
  maxPartitionsNumber?: any;
}

/**
 * A copy activity source for SAP Table source.
 */
export interface SapTableSource {
  /**
   * Polymorphic Discriminator
   */
  type: "SapTableSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * The number of rows to be retrieved. Type: integer(or Expression with resultType integer).
   */
  rowCount?: any;
  /**
   * The number of rows that will be skipped. Type: integer (or Expression with resultType
   * integer).
   */
  rowSkips?: any;
  /**
   * The fields of the SAP table that will be retrieved. For example, column0, column1. Type:
   * string (or Expression with resultType string).
   */
  rfcTableFields?: any;
  /**
   * The options for the filtering of the SAP Table. For example, COLUMN0 EQ SOME VALUE. Type:
   * string (or Expression with resultType string).
   */
  rfcTableOptions?: any;
  /**
   * Specifies the maximum number of rows that will be retrieved at a time when retrieving data
   * from SAP Table. Type: integer (or Expression with resultType integer).
   */
  batchSize?: any;
  /**
   * Specifies the custom RFC function module that will be used to read data from SAP Table. Type:
   * string (or Expression with resultType string).
   */
  customRfcReadTableFunctionModule?: any;
  /**
   * The partition mechanism that will be used for SAP table read in parallel. Possible values
   * include: 'None', 'PartitionOnInt', 'PartitionOnCalendarYear', 'PartitionOnCalendarMonth',
   * 'PartitionOnCalendarDate', 'PartitionOnTime'
   */
  partitionOption?: SapTablePartitionOption;
  /**
   * The settings that will be leveraged for SAP table source partitioning.
   */
  partitionSettings?: SapTablePartitionSettings;
}

/**
 * A copy activity source for SAP Business Warehouse Open Hub Destination source.
 */
export interface SapOpenHubSource {
  /**
   * Polymorphic Discriminator
   */
  type: "SapOpenHubSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * Whether to exclude the records of the last request. The default value is true. Type: boolean
   * (or Expression with resultType boolean).
   */
  excludeLastRequest?: any;
  /**
   * The ID of request for delta loading. Once it is set, only data with requestId larger than the
   * value of this property will be retrieved. The default value is 0. Type: integer (or Expression
   * with resultType integer ).
   */
  baseRequestId?: any;
}

/**
 * The settings that will be leveraged for SAP HANA source partitioning.
 */
export interface SapHanaPartitionSettings {
  /**
   * The name of the column that will be used for proceeding range partitioning. Type: string (or
   * Expression with resultType string).
   */
  partitionColumnName?: any;
}

/**
 * A copy activity source for SAP HANA source.
 */
export interface SapHanaSource {
  /**
   * Polymorphic Discriminator
   */
  type: "SapHanaSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * SAP HANA Sql query. Type: string (or Expression with resultType string).
   */
  query?: any;
  /**
   * The packet size of data read from SAP HANA. Type: integer(or Expression with resultType
   * integer).
   */
  packetSize?: any;
  /**
   * The partition mechanism that will be used for SAP HANA read in parallel. Possible values
   * include: 'None', 'PhysicalPartitionsOfTable', 'SapHanaDynamicRange'
   */
  partitionOption?: SapHanaPartitionOption;
  /**
   * The settings that will be leveraged for SAP HANA source partitioning.
   */
  partitionSettings?: SapHanaPartitionSettings;
}

/**
 * A copy activity source for SAP ECC source.
 */
export interface SapEccSource {
  /**
   * Polymorphic Discriminator
   */
  type: "SapEccSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * SAP ECC OData query. For example, "$top=1". Type: string (or Expression with resultType
   * string).
   */
  query?: any;
  /**
   * The timeout (TimeSpan) to get an HTTP response. It is the timeout to get a response, not the
   * timeout to read response data. Default value: 00:05:00. Type: string (or Expression with
   * resultType string), pattern: ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  httpRequestTimeout?: any;
}

/**
 * A copy activity source for SAP Cloud for Customer source.
 */
export interface SapCloudForCustomerSource {
  /**
   * Polymorphic Discriminator
   */
  type: "SapCloudForCustomerSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * SAP Cloud for Customer OData query. For example, "$top=1". Type: string (or Expression with
   * resultType string).
   */
  query?: any;
  /**
   * The timeout (TimeSpan) to get an HTTP response. It is the timeout to get a response, not the
   * timeout to read response data. Default value: 00:05:00. Type: string (or Expression with
   * resultType string), pattern: ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  httpRequestTimeout?: any;
}

/**
 * A copy activity Salesforce source.
 */
export interface SalesforceSource {
  /**
   * Polymorphic Discriminator
   */
  type: "SalesforceSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * Database query. Type: string (or Expression with resultType string).
   */
  query?: any;
  /**
   * The read behavior for the operation. Default is Query. Possible values include: 'Query',
   * 'QueryAll'
   */
  readBehavior?: SalesforceSourceReadBehavior;
}

/**
 * A copy activity source for SapBW server via MDX.
 */
export interface SapBwSource {
  /**
   * Polymorphic Discriminator
   */
  type: "SapBwSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * MDX query. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity source for Sybase databases.
 */
export interface SybaseSource {
  /**
   * Polymorphic Discriminator
   */
  type: "SybaseSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * Database query. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity source for PostgreSQL databases.
 */
export interface PostgreSqlSource {
  /**
   * Polymorphic Discriminator
   */
  type: "PostgreSqlSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * Database query. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity source for MySQL databases.
 */
export interface MySqlSource {
  /**
   * Polymorphic Discriminator
   */
  type: "MySqlSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * Database query. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity source for ODBC databases.
 */
export interface OdbcSource {
  /**
   * Polymorphic Discriminator
   */
  type: "OdbcSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * Database query. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity source for Db2 databases.
 */
export interface Db2Source {
  /**
   * Polymorphic Discriminator
   */
  type: "Db2Source";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * Database query. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity source for Informix.
 */
export interface InformixSource {
  /**
   * Polymorphic Discriminator
   */
  type: "InformixSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * Database query. Type: string (or Expression with resultType string).
   */
  query?: any;
}

/**
 * A copy activity Azure Table source.
 */
export interface AzureTableSource {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureTableSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Query timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  queryTimeout?: any;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
  /**
   * Azure Table source query. Type: string (or Expression with resultType string).
   */
  azureTableSourceQuery?: any;
  /**
   * Azure Table source ignore table not found. Type: boolean (or Expression with resultType
   * boolean).
   */
  azureTableSourceIgnoreTableNotFound?: any;
}

/**
 * A copy activity Binary source.
 */
export interface BinarySource {
  /**
   * Polymorphic Discriminator
   */
  type: "BinarySource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Binary store settings.
   */
  storeSettings?: StoreReadSettingsUnion;
  /**
   * Binary format settings.
   */
  formatSettings?: BinaryReadSettings;
}

/**
 * A copy activity ORC source.
 */
export interface OrcSource {
  /**
   * Polymorphic Discriminator
   */
  type: "OrcSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * ORC store settings.
   */
  storeSettings?: StoreReadSettingsUnion;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity Xml source.
 */
export interface XmlSource {
  /**
   * Polymorphic Discriminator
   */
  type: "XmlSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Xml store settings.
   */
  storeSettings?: StoreReadSettingsUnion;
  /**
   * Xml format settings.
   */
  formatSettings?: XmlReadSettings;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity Json source.
 */
export interface JsonSource {
  /**
   * Polymorphic Discriminator
   */
  type: "JsonSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Json store settings.
   */
  storeSettings?: StoreReadSettingsUnion;
  /**
   * Json format settings.
   */
  formatSettings?: JsonReadSettings;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity DelimitedText source.
 */
export interface DelimitedTextSource {
  /**
   * Polymorphic Discriminator
   */
  type: "DelimitedTextSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * DelimitedText store settings.
   */
  storeSettings?: StoreReadSettingsUnion;
  /**
   * DelimitedText format settings.
   */
  formatSettings?: DelimitedTextReadSettings;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity Parquet source.
 */
export interface ParquetSource {
  /**
   * Polymorphic Discriminator
   */
  type: "ParquetSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Parquet store settings.
   */
  storeSettings?: StoreReadSettingsUnion;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity excel source.
 */
export interface ExcelSource {
  /**
   * Polymorphic Discriminator
   */
  type: "ExcelSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Excel store settings.
   */
  storeSettings?: StoreReadSettingsUnion;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * A copy activity Avro source.
 */
export interface AvroSource {
  /**
   * Polymorphic Discriminator
   */
  type: "AvroSource";
  /**
   * Source retry count. Type: integer (or Expression with resultType integer).
   */
  sourceRetryCount?: any;
  /**
   * Source retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sourceRetryWait?: any;
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Avro store settings.
   */
  storeSettings?: StoreReadSettingsUnion;
  /**
   * Specifies the additional columns to be added to source data. Type: array of objects (or
   * Expression with resultType array of objects).
   */
  additionalColumns?: AdditionalColumns[];
}

/**
 * Lookup activity.
 */
export interface LookupActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "Lookup";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * Dataset-specific source properties, same as copy activity source.
   */
  source: CopySourceUnion;
  /**
   * Lookup activity dataset reference.
   */
  dataset: DatasetReference;
  /**
   * Whether to return first row or all rows. Default value is true. Type: boolean (or Expression
   * with resultType boolean).
   */
  firstRowOnly?: any;
}

/**
 * Azure Data Explorer command activity.
 */
export interface AzureDataExplorerCommandActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureDataExplorerCommand";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * A control command, according to the Azure Data Explorer command syntax. Type: string (or
   * Expression with resultType string).
   */
  command: any;
  /**
   * Control command timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9]))..)
   */
  commandTimeout?: any;
}

/**
 * Log storage settings.
 */
export interface LogStorageSettings {
  /**
   * Log storage linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * The path to storage for storing detailed logs of activity execution. Type: string (or
   * Expression with resultType string).
   */
  path?: any;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Delete activity.
 */
export interface DeleteActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "Delete";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * If true, files or sub-folders under current folder path will be deleted recursively. Default
   * is false. Type: boolean (or Expression with resultType boolean).
   */
  recursive?: any;
  /**
   * The max concurrent connections to connect data source at the same time.
   */
  maxConcurrentConnections?: number;
  /**
   * Whether to record detailed logs of delete-activity execution. Default value is false. Type:
   * boolean (or Expression with resultType boolean).
   */
  enableLogging?: any;
  /**
   * Log storage settings customer need to provide when enableLogging is true.
   */
  logStorageSettings?: LogStorageSettings;
  /**
   * Delete activity dataset reference.
   */
  dataset: DatasetReference;
  /**
   * Delete activity store settings.
   */
  storeSettings?: StoreReadSettingsUnion;
}

/**
 * SQL stored procedure activity type.
 */
export interface SqlServerStoredProcedureActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "SqlServerStoredProcedure";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * Stored procedure name. Type: string (or Expression with resultType string).
   */
  storedProcedureName: any;
  /**
   * Value and type setting for stored procedure parameters. Example: "{Parameter1: {value: "1",
   * type: "int"}}".
   */
  storedProcedureParameters?: { [propertyName: string]: StoredProcedureParameter };
}

/**
 * Reference objects for custom activity
 */
export interface CustomActivityReferenceObject {
  /**
   * Linked service references.
   */
  linkedServices?: LinkedServiceReference[];
  /**
   * Dataset references.
   */
  datasets?: DatasetReference[];
}

/**
 * Custom activity type.
 */
export interface CustomActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "Custom";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * Command for custom activity Type: string (or Expression with resultType string).
   */
  command: any;
  /**
   * Resource linked service reference.
   */
  resourceLinkedService?: LinkedServiceReference;
  /**
   * Folder path for resource files Type: string (or Expression with resultType string).
   */
  folderPath?: any;
  /**
   * Reference objects
   */
  referenceObjects?: CustomActivityReferenceObject;
  /**
   * User defined property bag. There is no restriction on the keys or values that can be used. The
   * user specified custom activity has the full responsibility to consume and interpret the
   * content defined.
   */
  extendedProperties?: { [propertyName: string]: any };
  /**
   * The retention time for the files submitted for custom activity. Type: double (or Expression
   * with resultType double).
   */
  retentionTimeInDays?: any;
}

/**
 * SSIS access credential.
 */
export interface SSISAccessCredential {
  /**
   * Domain for windows authentication.
   */
  domain: any;
  /**
   * UseName for windows authentication.
   */
  userName: any;
  /**
   * Password for windows authentication.
   */
  password: SecretBaseUnion;
}

/**
 * SSIS package execution log location
 */
export interface SSISLogLocation {
  /**
   * The SSIS package execution log path. Type: string (or Expression with resultType string).
   */
  logPath: any;
  /**
   * The package execution log access credential.
   */
  accessCredential?: SSISAccessCredential;
  /**
   * Specifies the interval to refresh log. The default interval is 5 minutes. Type: string (or
   * Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  logRefreshInterval?: any;
}

/**
 * SSIS property override.
 */
export interface SSISPropertyOverride {
  /**
   * SSIS package property override value. Type: string (or Expression with resultType string).
   */
  value: any;
  /**
   * Whether SSIS package property override value is sensitive data. Value will be encrypted in
   * SSISDB if it is true
   */
  isSensitive?: boolean;
}

/**
 * SSIS execution parameter.
 */
export interface SSISExecutionParameter {
  /**
   * SSIS package execution parameter value. Type: string (or Expression with resultType string).
   */
  value: any;
}

/**
 * SSIS package execution credential.
 */
export interface SSISExecutionCredential {
  /**
   * Domain for windows authentication.
   */
  domain: any;
  /**
   * UseName for windows authentication.
   */
  userName: any;
  /**
   * Password for windows authentication.
   */
  password: SecureString;
}

/**
 * SSIS embedded child package.
 */
export interface SSISChildPackage {
  /**
   * Path for embedded child package. Type: string (or Expression with resultType string).
   */
  packagePath: any;
  /**
   * Name for embedded child package.
   */
  packageName?: string;
  /**
   * Content for embedded child package. Type: string (or Expression with resultType string).
   */
  packageContent: any;
  /**
   * Last modified date for embedded child package.
   */
  packageLastModifiedDate?: string;
}

/**
 * SSIS package location.
 */
export interface SSISPackageLocation {
  /**
   * The SSIS package path. Type: string (or Expression with resultType string).
   */
  packagePath?: any;
  /**
   * The type of SSIS package location. Possible values include: 'SSISDB', 'File', 'InlinePackage',
   * 'PackageStore'
   */
  type?: SsisPackageLocationType;
  /**
   * Password of the package.
   */
  packagePassword?: SecretBaseUnion;
  /**
   * The package access credential.
   */
  accessCredential?: SSISAccessCredential;
  /**
   * The configuration file of the package execution. Type: string (or Expression with resultType
   * string).
   */
  configurationPath?: any;
  /**
   * The configuration file access credential.
   */
  configurationAccessCredential?: SSISAccessCredential;
  /**
   * The package name.
   */
  packageName?: string;
  /**
   * The embedded package content. Type: string (or Expression with resultType string).
   */
  packageContent?: any;
  /**
   * The embedded package last modified date.
   */
  packageLastModifiedDate?: string;
  /**
   * The embedded child package list.
   */
  childPackages?: SSISChildPackage[];
}

/**
 * Execute SSIS package activity.
 */
export interface ExecuteSSISPackageActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "ExecuteSSISPackage";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * SSIS package location.
   */
  packageLocation: SSISPackageLocation;
  /**
   * Specifies the runtime to execute SSIS package. The value should be "x86" or "x64". Type:
   * string (or Expression with resultType string).
   */
  runtime?: any;
  /**
   * The logging level of SSIS package execution. Type: string (or Expression with resultType
   * string).
   */
  loggingLevel?: any;
  /**
   * The environment path to execute the SSIS package. Type: string (or Expression with resultType
   * string).
   */
  environmentPath?: any;
  /**
   * The package execution credential.
   */
  executionCredential?: SSISExecutionCredential;
  /**
   * The integration runtime reference.
   */
  connectVia: IntegrationRuntimeReference;
  /**
   * The project level parameters to execute the SSIS package.
   */
  projectParameters?: { [propertyName: string]: SSISExecutionParameter };
  /**
   * The package level parameters to execute the SSIS package.
   */
  packageParameters?: { [propertyName: string]: SSISExecutionParameter };
  /**
   * The project level connection managers to execute the SSIS package.
   */
  projectConnectionManagers?: { [propertyName: string]: { [propertyName: string]: SSISExecutionParameter } };
  /**
   * The package level connection managers to execute the SSIS package.
   */
  packageConnectionManagers?: { [propertyName: string]: { [propertyName: string]: SSISExecutionParameter } };
  /**
   * The property overrides to execute the SSIS package.
   */
  propertyOverrides?: { [propertyName: string]: SSISPropertyOverride };
  /**
   * SSIS package execution log location.
   */
  logLocation?: SSISLogLocation;
}

/**
 * HDInsight Spark activity.
 */
export interface HDInsightSparkActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "HDInsightSpark";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * The root path in 'sparkJobLinkedService' for all the jobs files. Type: string (or Expression
   * with resultType string).
   */
  rootPath: any;
  /**
   * The relative path to the root folder of the code/package to be executed. Type: string (or
   * Expression with resultType string).
   */
  entryFilePath: any;
  /**
   * The user-specified arguments to HDInsightSparkActivity.
   */
  argumentsProperty?: any[];
  /**
   * Debug info option. Possible values include: 'None', 'Always', 'Failure'
   */
  getDebugInfo?: HDInsightActivityDebugInfoOption;
  /**
   * The storage linked service for uploading the entry file and dependencies, and for receiving
   * logs.
   */
  sparkJobLinkedService?: LinkedServiceReference;
  /**
   * The application's Java/Spark main class.
   */
  className?: string;
  /**
   * The user to impersonate that will execute the job. Type: string (or Expression with resultType
   * string).
   */
  proxyUser?: any;
  /**
   * Spark configuration property.
   */
  sparkConfig?: { [propertyName: string]: any };
}

/**
 * HDInsight streaming activity type.
 */
export interface HDInsightStreamingActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "HDInsightStreaming";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * Storage linked service references.
   */
  storageLinkedServices?: LinkedServiceReference[];
  /**
   * User specified arguments to HDInsightActivity.
   */
  argumentsProperty?: any[];
  /**
   * Debug info option. Possible values include: 'None', 'Always', 'Failure'
   */
  getDebugInfo?: HDInsightActivityDebugInfoOption;
  /**
   * Mapper executable name. Type: string (or Expression with resultType string).
   */
  mapper: any;
  /**
   * Reducer executable name. Type: string (or Expression with resultType string).
   */
  reducer: any;
  /**
   * Input blob path. Type: string (or Expression with resultType string).
   */
  input: any;
  /**
   * Output blob path. Type: string (or Expression with resultType string).
   */
  output: any;
  /**
   * Paths to streaming job files. Can be directories.
   */
  filePaths: any[];
  /**
   * Linked service reference where the files are located.
   */
  fileLinkedService?: LinkedServiceReference;
  /**
   * Combiner executable name. Type: string (or Expression with resultType string).
   */
  combiner?: any;
  /**
   * Command line environment values.
   */
  commandEnvironment?: any[];
  /**
   * Allows user to specify defines for streaming job request.
   */
  defines?: { [propertyName: string]: any };
}

/**
 * HDInsight MapReduce activity type.
 */
export interface HDInsightMapReduceActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "HDInsightMapReduce";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * Storage linked service references.
   */
  storageLinkedServices?: LinkedServiceReference[];
  /**
   * User specified arguments to HDInsightActivity.
   */
  argumentsProperty?: any[];
  /**
   * Debug info option. Possible values include: 'None', 'Always', 'Failure'
   */
  getDebugInfo?: HDInsightActivityDebugInfoOption;
  /**
   * Class name. Type: string (or Expression with resultType string).
   */
  className: any;
  /**
   * Jar path. Type: string (or Expression with resultType string).
   */
  jarFilePath: any;
  /**
   * Jar linked service reference.
   */
  jarLinkedService?: LinkedServiceReference;
  /**
   * Jar libs.
   */
  jarLibs?: any[];
  /**
   * Allows user to specify defines for the MapReduce job request.
   */
  defines?: { [propertyName: string]: any };
}

/**
 * HDInsight Pig activity type.
 */
export interface HDInsightPigActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "HDInsightPig";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * Storage linked service references.
   */
  storageLinkedServices?: LinkedServiceReference[];
  /**
   * User specified arguments to HDInsightActivity. Type: array (or Expression with resultType
   * array).
   */
  argumentsProperty?: any;
  /**
   * Debug info option. Possible values include: 'None', 'Always', 'Failure'
   */
  getDebugInfo?: HDInsightActivityDebugInfoOption;
  /**
   * Script path. Type: string (or Expression with resultType string).
   */
  scriptPath?: any;
  /**
   * Script linked service reference.
   */
  scriptLinkedService?: LinkedServiceReference;
  /**
   * Allows user to specify defines for Pig job request.
   */
  defines?: { [propertyName: string]: any };
}

/**
 * HDInsight Hive activity type.
 */
export interface HDInsightHiveActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "HDInsightHive";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * Storage linked service references.
   */
  storageLinkedServices?: LinkedServiceReference[];
  /**
   * User specified arguments to HDInsightActivity.
   */
  argumentsProperty?: any[];
  /**
   * Debug info option. Possible values include: 'None', 'Always', 'Failure'
   */
  getDebugInfo?: HDInsightActivityDebugInfoOption;
  /**
   * Script path. Type: string (or Expression with resultType string).
   */
  scriptPath?: any;
  /**
   * Script linked service reference.
   */
  scriptLinkedService?: LinkedServiceReference;
  /**
   * Allows user to specify defines for Hive job request.
   */
  defines?: { [propertyName: string]: any };
  /**
   * User specified arguments under hivevar namespace.
   */
  variables?: any[];
  /**
   * Query timeout value (in minutes).  Effective when the HDInsight cluster is with ESP
   * (Enterprise Security Package)
   */
  queryTimeout?: number;
}

/**
 * Skip error file.
 */
export interface SkipErrorFile {
  /**
   * Skip if file is deleted by other client during copy. Default is true. Type: boolean (or
   * Expression with resultType boolean).
   */
  fileMissing?: any;
  /**
   * Skip if source/sink file changed by other concurrent write. Default is false. Type: boolean
   * (or Expression with resultType boolean).
   */
  dataInconsistency?: any;
}

/**
 * Redirect incompatible row settings
 */
export interface RedirectIncompatibleRowSettings {
  /**
   * Name of the Azure Storage, Storage SAS, or Azure Data Lake Store linked service used for
   * redirecting incompatible row. Must be specified if redirectIncompatibleRowSettings is
   * specified. Type: string (or Expression with resultType string).
   */
  linkedServiceName: any;
  /**
   * The path for storing the redirect incompatible row data. Type: string (or Expression with
   * resultType string).
   */
  path?: any;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Staging settings.
 */
export interface StagingSettings {
  /**
   * Staging linked service reference.
   */
  linkedServiceName: LinkedServiceReference;
  /**
   * The path to storage for storing the interim data. Type: string (or Expression with resultType
   * string).
   */
  path?: any;
  /**
   * Specifies whether to use compression when copying data via an interim staging. Default value
   * is false. Type: boolean (or Expression with resultType boolean).
   */
  enableCompression?: any;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Contains the possible cases for CopySink.
 */
export type CopySinkUnion = CopySink | CosmosDbMongoDbApiSink | SalesforceServiceCloudSink | SalesforceSink | AzureDataExplorerSink | CommonDataServiceForAppsSink | DynamicsCrmSink | DynamicsSink | MicrosoftAccessSink | InformixSink | OdbcSink | AzureSearchIndexSink | AzureBlobFSSink | AzureDataLakeStoreSink | OracleSink | SnowflakeSink | SqlDWSink | SqlMISink | AzureSqlSink | SqlServerSink | SqlSink | CosmosDbSqlApiSink | DocumentDbCollectionSink | FileSystemSink | BlobSink | BinarySink | ParquetSink | AvroSink | AzureTableSink | AzureQueueSink | SapCloudForCustomerSink | AzureMySqlSink | AzurePostgreSqlSink | OrcSink | JsonSink | DelimitedTextSink;

/**
 * A copy activity sink.
 */
export interface CopySink {
  /**
   * Polymorphic Discriminator
   */
  type: "CopySink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * A copy activity sink for a CosmosDB (MongoDB API) database.
 */
export interface CosmosDbMongoDbApiSink {
  /**
   * Polymorphic Discriminator
   */
  type: "CosmosDbMongoDbApiSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Specifies whether the document with same key to be overwritten (upsert) rather than throw
   * exception (insert). The default value is "insert". Type: string (or Expression with resultType
   * string). Type: string (or Expression with resultType string).
   */
  writeBehavior?: any;
}

/**
 * A copy activity Salesforce Service Cloud sink.
 */
export interface SalesforceServiceCloudSink {
  /**
   * Polymorphic Discriminator
   */
  type: "SalesforceServiceCloudSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * The write behavior for the operation. Default is Insert. Possible values include: 'Insert',
   * 'Upsert'
   */
  writeBehavior?: SalesforceSinkWriteBehavior;
  /**
   * The name of the external ID field for upsert operation. Default value is 'Id' column. Type:
   * string (or Expression with resultType string).
   */
  externalIdFieldName?: any;
  /**
   * The flag indicating whether or not to ignore null values from input dataset (except key
   * fields) during write operation. Default value is false. If set it to true, it means ADF will
   * leave the data in the destination object unchanged when doing upsert/update operation and
   * insert defined default value when doing insert operation, versus ADF will update the data in
   * the destination object to NULL when doing upsert/update operation and insert NULL value when
   * doing insert operation. Type: boolean (or Expression with resultType boolean).
   */
  ignoreNullValues?: any;
}

/**
 * A copy activity Salesforce sink.
 */
export interface SalesforceSink {
  /**
   * Polymorphic Discriminator
   */
  type: "SalesforceSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * The write behavior for the operation. Default is Insert. Possible values include: 'Insert',
   * 'Upsert'
   */
  writeBehavior?: SalesforceSinkWriteBehavior;
  /**
   * The name of the external ID field for upsert operation. Default value is 'Id' column. Type:
   * string (or Expression with resultType string).
   */
  externalIdFieldName?: any;
  /**
   * The flag indicating whether or not to ignore null values from input dataset (except key
   * fields) during write operation. Default value is false. If set it to true, it means ADF will
   * leave the data in the destination object unchanged when doing upsert/update operation and
   * insert defined default value when doing insert operation, versus ADF will update the data in
   * the destination object to NULL when doing upsert/update operation and insert NULL value when
   * doing insert operation. Type: boolean (or Expression with resultType boolean).
   */
  ignoreNullValues?: any;
}

/**
 * A copy activity Azure Data Explorer sink.
 */
export interface AzureDataExplorerSink {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureDataExplorerSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * A name of a pre-created csv mapping that was defined on the target Kusto table. Type: string.
   */
  ingestionMappingName?: any;
  /**
   * An explicit column mapping description provided in a json format. Type: string.
   */
  ingestionMappingAsJson?: any;
  /**
   * If set to true, any aggregation will be skipped. Default is false. Type: boolean.
   */
  flushImmediately?: any;
}

/**
 * A copy activity Common Data Service for Apps sink.
 */
export interface CommonDataServiceForAppsSink {
  /**
   * Polymorphic Discriminator
   */
  type: "CommonDataServiceForAppsSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * The flag indicating whether to ignore null values from input dataset (except key fields)
   * during write operation. Default is false. Type: boolean (or Expression with resultType
   * boolean).
   */
  ignoreNullValues?: any;
  /**
   * The logical name of the alternate key which will be used when upserting records. Type: string
   * (or Expression with resultType string).
   */
  alternateKeyName?: any;
}

/**
 * A copy activity Dynamics CRM sink.
 */
export interface DynamicsCrmSink {
  /**
   * Polymorphic Discriminator
   */
  type: "DynamicsCrmSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * The flag indicating whether to ignore null values from input dataset (except key fields)
   * during write operation. Default is false. Type: boolean (or Expression with resultType
   * boolean).
   */
  ignoreNullValues?: any;
  /**
   * The logical name of the alternate key which will be used when upserting records. Type: string
   * (or Expression with resultType string).
   */
  alternateKeyName?: any;
}

/**
 * A copy activity Dynamics sink.
 */
export interface DynamicsSink {
  /**
   * Polymorphic Discriminator
   */
  type: "DynamicsSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * The flag indicating whether ignore null values from input dataset (except key fields) during
   * write operation. Default is false. Type: boolean (or Expression with resultType boolean).
   */
  ignoreNullValues?: any;
  /**
   * The logical name of the alternate key which will be used when upserting records. Type: string
   * (or Expression with resultType string).
   */
  alternateKeyName?: any;
}

/**
 * A copy activity Microsoft Access sink.
 */
export interface MicrosoftAccessSink {
  /**
   * Polymorphic Discriminator
   */
  type: "MicrosoftAccessSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * A query to execute before starting the copy. Type: string (or Expression with resultType
   * string).
   */
  preCopyScript?: any;
}

/**
 * A copy activity Informix sink.
 */
export interface InformixSink {
  /**
   * Polymorphic Discriminator
   */
  type: "InformixSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * A query to execute before starting the copy. Type: string (or Expression with resultType
   * string).
   */
  preCopyScript?: any;
}

/**
 * A copy activity ODBC sink.
 */
export interface OdbcSink {
  /**
   * Polymorphic Discriminator
   */
  type: "OdbcSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * A query to execute before starting the copy. Type: string (or Expression with resultType
   * string).
   */
  preCopyScript?: any;
}

/**
 * A copy activity Azure Search Index sink.
 */
export interface AzureSearchIndexSink {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureSearchIndexSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Specify the write behavior when upserting documents into Azure Search Index. Possible values
   * include: 'Merge', 'Upload'
   */
  writeBehavior?: AzureSearchIndexWriteBehaviorType;
}

/**
 * A copy activity Azure Data Lake Storage Gen2 sink.
 */
export interface AzureBlobFSSink {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureBlobFSSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * The type of copy behavior for copy sink.
   */
  copyBehavior?: any;
}

/**
 * A copy activity Azure Data Lake Store sink.
 */
export interface AzureDataLakeStoreSink {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureDataLakeStoreSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * The type of copy behavior for copy sink.
   */
  copyBehavior?: any;
  /**
   * Single File Parallel.
   */
  enableAdlsSingleFileParallel?: any;
}

/**
 * A copy activity Oracle sink.
 */
export interface OracleSink {
  /**
   * Polymorphic Discriminator
   */
  type: "OracleSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * SQL pre-copy script. Type: string (or Expression with resultType string).
   */
  preCopyScript?: any;
}

/**
 * Contains the possible cases for ImportSettings.
 */
export type ImportSettingsUnion = ImportSettings | SnowflakeImportCopyCommand;

/**
 * Import command settings.
 */
export interface ImportSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "ImportSettings";
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Snowflake import command settings.
 */
export interface SnowflakeImportCopyCommand {
  /**
   * Polymorphic Discriminator
   */
  type: "SnowflakeImportCopyCommand";
  /**
   * Additional copy options directly passed to snowflake Copy Command. Type: key value pairs
   * (value should be string type) (or Expression with resultType object). Example:
   * "additionalCopyOptions": { "DATE_FORMAT": "MM/DD/YYYY", "TIME_FORMAT": "'HH24:MI:SS.FF'" }
   */
  additionalCopyOptions?: { [propertyName: string]: any };
  /**
   * Additional format options directly passed to snowflake Copy Command. Type: key value pairs
   * (value should be string type) (or Expression with resultType object). Example:
   * "additionalFormatOptions": { "FORCE": "TRUE", "LOAD_UNCERTAIN_FILES": "'FALSE'" }
   */
  additionalFormatOptions?: { [propertyName: string]: any };
}

/**
 * A copy activity snowflake sink.
 */
export interface SnowflakeSink {
  /**
   * Polymorphic Discriminator
   */
  type: "SnowflakeSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * SQL pre-copy script. Type: string (or Expression with resultType string).
   */
  preCopyScript?: any;
  /**
   * Snowflake import settings.
   */
  importSettings?: SnowflakeImportCopyCommand;
}

/**
 * Default value.
 */
export interface DWCopyCommandDefaultValue {
  /**
   * Column name. Type: object (or Expression with resultType string).
   */
  columnName?: any;
  /**
   * The default value of the column. Type: object (or Expression with resultType string).
   */
  defaultValue?: any;
}

/**
 * DW Copy Command settings.
 */
export interface DWCopyCommandSettings {
  /**
   * Specifies the default values for each target column in SQL DW. The default values in the
   * property overwrite the DEFAULT constraint set in the DB, and identity column cannot have a
   * default value. Type: array of objects (or Expression with resultType array of objects).
   */
  defaultValues?: DWCopyCommandDefaultValue[];
  /**
   * Additional options directly passed to SQL DW in Copy Command. Type: key value pairs (value
   * should be string type) (or Expression with resultType object). Example: "additionalOptions": {
   * "MAXERRORS": "1000", "DATEFORMAT": "'ymd'" }
   */
  additionalOptions?: { [propertyName: string]: string };
}

/**
 * PolyBase settings.
 */
export interface PolybaseSettings {
  /**
   * Reject type. Possible values include: 'value', 'percentage'
   */
  rejectType?: PolybaseSettingsRejectType;
  /**
   * Specifies the value or the percentage of rows that can be rejected before the query fails.
   * Type: number (or Expression with resultType number), minimum: 0.
   */
  rejectValue?: any;
  /**
   * Determines the number of rows to attempt to retrieve before the PolyBase recalculates the
   * percentage of rejected rows. Type: integer (or Expression with resultType integer), minimum:
   * 0.
   */
  rejectSampleValue?: any;
  /**
   * Specifies how to handle missing values in delimited text files when PolyBase retrieves data
   * from the text file. Type: boolean (or Expression with resultType boolean).
   */
  useTypeDefault?: any;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * A copy activity SQL Data Warehouse sink.
 */
export interface SqlDWSink {
  /**
   * Polymorphic Discriminator
   */
  type: "SqlDWSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * SQL pre-copy script. Type: string (or Expression with resultType string).
   */
  preCopyScript?: any;
  /**
   * Indicates to use PolyBase to copy data into SQL Data Warehouse when applicable. Type: boolean
   * (or Expression with resultType boolean).
   */
  allowPolyBase?: any;
  /**
   * Specifies PolyBase-related settings when allowPolyBase is true.
   */
  polyBaseSettings?: PolybaseSettings;
  /**
   * Indicates to use Copy Command to copy data into SQL Data Warehouse. Type: boolean (or
   * Expression with resultType boolean).
   */
  allowCopyCommand?: any;
  /**
   * Specifies Copy Command related settings when allowCopyCommand is true.
   */
  copyCommandSettings?: DWCopyCommandSettings;
  /**
   * The option to handle sink table, such as autoCreate. For now only 'autoCreate' value is
   * supported. Type: string (or Expression with resultType string).
   */
  tableOption?: any;
}

/**
 * A copy activity Azure SQL Managed Instance sink.
 */
export interface SqlMISink {
  /**
   * Polymorphic Discriminator
   */
  type: "SqlMISink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * SQL writer stored procedure name. Type: string (or Expression with resultType string).
   */
  sqlWriterStoredProcedureName?: any;
  /**
   * SQL writer table type. Type: string (or Expression with resultType string).
   */
  sqlWriterTableType?: any;
  /**
   * SQL pre-copy script. Type: string (or Expression with resultType string).
   */
  preCopyScript?: any;
  /**
   * SQL stored procedure parameters.
   */
  storedProcedureParameters?: { [propertyName: string]: StoredProcedureParameter };
  /**
   * The stored procedure parameter name of the table type. Type: string (or Expression with
   * resultType string).
   */
  storedProcedureTableTypeParameterName?: any;
  /**
   * The option to handle sink table, such as autoCreate. For now only 'autoCreate' value is
   * supported. Type: string (or Expression with resultType string).
   */
  tableOption?: any;
}

/**
 * A copy activity Azure SQL sink.
 */
export interface AzureSqlSink {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureSqlSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * SQL writer stored procedure name. Type: string (or Expression with resultType string).
   */
  sqlWriterStoredProcedureName?: any;
  /**
   * SQL writer table type. Type: string (or Expression with resultType string).
   */
  sqlWriterTableType?: any;
  /**
   * SQL pre-copy script. Type: string (or Expression with resultType string).
   */
  preCopyScript?: any;
  /**
   * SQL stored procedure parameters.
   */
  storedProcedureParameters?: { [propertyName: string]: StoredProcedureParameter };
  /**
   * The stored procedure parameter name of the table type. Type: string (or Expression with
   * resultType string).
   */
  storedProcedureTableTypeParameterName?: any;
  /**
   * The option to handle sink table, such as autoCreate. For now only 'autoCreate' value is
   * supported. Type: string (or Expression with resultType string).
   */
  tableOption?: any;
}

/**
 * A copy activity SQL server sink.
 */
export interface SqlServerSink {
  /**
   * Polymorphic Discriminator
   */
  type: "SqlServerSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * SQL writer stored procedure name. Type: string (or Expression with resultType string).
   */
  sqlWriterStoredProcedureName?: any;
  /**
   * SQL writer table type. Type: string (or Expression with resultType string).
   */
  sqlWriterTableType?: any;
  /**
   * SQL pre-copy script. Type: string (or Expression with resultType string).
   */
  preCopyScript?: any;
  /**
   * SQL stored procedure parameters.
   */
  storedProcedureParameters?: { [propertyName: string]: StoredProcedureParameter };
  /**
   * The stored procedure parameter name of the table type. Type: string (or Expression with
   * resultType string).
   */
  storedProcedureTableTypeParameterName?: any;
  /**
   * The option to handle sink table, such as autoCreate. For now only 'autoCreate' value is
   * supported. Type: string (or Expression with resultType string).
   */
  tableOption?: any;
}

/**
 * A copy activity SQL sink.
 */
export interface SqlSink {
  /**
   * Polymorphic Discriminator
   */
  type: "SqlSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * SQL writer stored procedure name. Type: string (or Expression with resultType string).
   */
  sqlWriterStoredProcedureName?: any;
  /**
   * SQL writer table type. Type: string (or Expression with resultType string).
   */
  sqlWriterTableType?: any;
  /**
   * SQL pre-copy script. Type: string (or Expression with resultType string).
   */
  preCopyScript?: any;
  /**
   * SQL stored procedure parameters.
   */
  storedProcedureParameters?: { [propertyName: string]: StoredProcedureParameter };
  /**
   * The stored procedure parameter name of the table type. Type: string (or Expression with
   * resultType string).
   */
  storedProcedureTableTypeParameterName?: any;
  /**
   * The option to handle sink table, such as autoCreate. For now only 'autoCreate' value is
   * supported. Type: string (or Expression with resultType string).
   */
  tableOption?: any;
}

/**
 * A copy activity Azure CosmosDB (SQL API) Collection sink.
 */
export interface CosmosDbSqlApiSink {
  /**
   * Polymorphic Discriminator
   */
  type: "CosmosDbSqlApiSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Describes how to write data to Azure Cosmos DB. Type: string (or Expression with resultType
   * string). Allowed values: insert and upsert.
   */
  writeBehavior?: any;
}

/**
 * A copy activity Document Database Collection sink.
 */
export interface DocumentDbCollectionSink {
  /**
   * Polymorphic Discriminator
   */
  type: "DocumentDbCollectionSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Nested properties separator. Default is . (dot). Type: string (or Expression with resultType
   * string).
   */
  nestingSeparator?: any;
  /**
   * Describes how to write data to Azure Cosmos DB. Type: string (or Expression with resultType
   * string). Allowed values: insert and upsert.
   */
  writeBehavior?: any;
}

/**
 * A copy activity file system sink.
 */
export interface FileSystemSink {
  /**
   * Polymorphic Discriminator
   */
  type: "FileSystemSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * The type of copy behavior for copy sink.
   */
  copyBehavior?: any;
}

/**
 * A copy activity Azure Blob sink.
 */
export interface BlobSink {
  /**
   * Polymorphic Discriminator
   */
  type: "BlobSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Blob writer overwrite files. Type: boolean (or Expression with resultType boolean).
   */
  blobWriterOverwriteFiles?: any;
  /**
   * Blob writer date time format. Type: string (or Expression with resultType string).
   */
  blobWriterDateTimeFormat?: any;
  /**
   * Blob writer add header. Type: boolean (or Expression with resultType boolean).
   */
  blobWriterAddHeader?: any;
  /**
   * The type of copy behavior for copy sink.
   */
  copyBehavior?: any;
}

/**
 * Contains the possible cases for StoreWriteSettings.
 */
export type StoreWriteSettingsUnion = StoreWriteSettings | FileServerWriteSettings | AzureDataLakeStoreWriteSettings | AzureBlobFSWriteSettings | AzureBlobStorageWriteSettings | SftpWriteSettings;

/**
 * Connector write settings.
 */
export interface StoreWriteSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "StoreWriteSettings";
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * The type of copy behavior for copy sink.
   */
  copyBehavior?: any;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * File server write settings.
 */
export interface FileServerWriteSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "FileServerWriteSettings";
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * The type of copy behavior for copy sink.
   */
  copyBehavior?: any;
}

/**
 * Azure data lake store write settings.
 */
export interface AzureDataLakeStoreWriteSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureDataLakeStoreWriteSettings";
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * The type of copy behavior for copy sink.
   */
  copyBehavior?: any;
  /**
   * Specifies the expiry time of the written files. The time is applied to the UTC time zone in
   * the format of "2018-12-01T05:00:00Z". Default value is NULL. Type: integer (or Expression with
   * resultType integer).
   */
  expiryDateTime?: any;
}

/**
 * Azure blobFS write settings.
 */
export interface AzureBlobFSWriteSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureBlobFSWriteSettings";
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * The type of copy behavior for copy sink.
   */
  copyBehavior?: any;
  /**
   * Indicates the block size(MB) when writing data to blob. Type: integer (or Expression with
   * resultType integer).
   */
  blockSizeInMB?: any;
}

/**
 * Azure blob write settings.
 */
export interface AzureBlobStorageWriteSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureBlobStorageWriteSettings";
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * The type of copy behavior for copy sink.
   */
  copyBehavior?: any;
  /**
   * Indicates the block size(MB) when writing data to blob. Type: integer (or Expression with
   * resultType integer).
   */
  blockSizeInMB?: any;
}

/**
 * Sftp write settings.
 */
export interface SftpWriteSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "SftpWriteSettings";
  /**
   * The maximum concurrent connection count for the source data store. Type: integer (or
   * Expression with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * The type of copy behavior for copy sink.
   */
  copyBehavior?: any;
  /**
   * Specifies the timeout for writing each chunk to SFTP server. Default value: 01:00:00 (one
   * hour). Type: string (or Expression with resultType string).
   */
  operationTimeout?: any;
  /**
   * Upload to temporary file(s) and rename. Disable this option if your SFTP server doesn't
   * support rename operation. Type: boolean (or Expression with resultType boolean).
   */
  useTempFileRename?: any;
}

/**
 * A copy activity Binary sink.
 */
export interface BinarySink {
  /**
   * Polymorphic Discriminator
   */
  type: "BinarySink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Binary store settings.
   */
  storeSettings?: StoreWriteSettingsUnion;
}

/**
 * A copy activity Parquet sink.
 */
export interface ParquetSink {
  /**
   * Polymorphic Discriminator
   */
  type: "ParquetSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Parquet store settings.
   */
  storeSettings?: StoreWriteSettingsUnion;
}

/**
 * Contains the possible cases for FormatWriteSettings.
 */
export type FormatWriteSettingsUnion = FormatWriteSettings | JsonWriteSettings | DelimitedTextWriteSettings | AvroWriteSettings;

/**
 * Format write settings.
 */
export interface FormatWriteSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "FormatWriteSettings";
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Json write settings.
 */
export interface JsonWriteSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "JsonWriteSettings";
  /**
   * File pattern of JSON. This setting controls the way a collection of JSON objects will be
   * treated. The default value is 'setOfObjects'. It is case-sensitive. Possible values include:
   * 'setOfObjects', 'arrayOfObjects'
   */
  filePattern?: JsonWriteFilePattern;
}

/**
 * Delimited text write settings.
 */
export interface DelimitedTextWriteSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "DelimitedTextWriteSettings";
  /**
   * Indicates whether string values should always be enclosed with quotes. Type: boolean (or
   * Expression with resultType boolean).
   */
  quoteAllText?: any;
  /**
   * The file extension used to create the files. Type: string (or Expression with resultType
   * string).
   */
  fileExtension: any;
}

/**
 * Avro write settings.
 */
export interface AvroWriteSettings {
  /**
   * Polymorphic Discriminator
   */
  type: "AvroWriteSettings";
  /**
   * Top level record name in write result, which is required in AVRO spec.
   */
  recordName?: string;
  /**
   * Record namespace in the write result.
   */
  recordNamespace?: string;
}

/**
 * A copy activity Avro sink.
 */
export interface AvroSink {
  /**
   * Polymorphic Discriminator
   */
  type: "AvroSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Avro store settings.
   */
  storeSettings?: StoreWriteSettingsUnion;
  /**
   * Avro format settings.
   */
  formatSettings?: AvroWriteSettings;
}

/**
 * A copy activity Azure Table sink.
 */
export interface AzureTableSink {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureTableSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Azure Table default partition key value. Type: string (or Expression with resultType string).
   */
  azureTableDefaultPartitionKeyValue?: any;
  /**
   * Azure Table partition key name. Type: string (or Expression with resultType string).
   */
  azureTablePartitionKeyName?: any;
  /**
   * Azure Table row key name. Type: string (or Expression with resultType string).
   */
  azureTableRowKeyName?: any;
  /**
   * Azure Table insert type. Type: string (or Expression with resultType string).
   */
  azureTableInsertType?: any;
}

/**
 * A copy activity Azure Queue sink.
 */
export interface AzureQueueSink {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureQueueSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
}

/**
 * A copy activity SAP Cloud for Customer sink.
 */
export interface SapCloudForCustomerSink {
  /**
   * Polymorphic Discriminator
   */
  type: "SapCloudForCustomerSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * The write behavior for the operation. Default is 'Insert'. Possible values include: 'Insert',
   * 'Update'
   */
  writeBehavior?: SapCloudForCustomerSinkWriteBehavior;
  /**
   * The timeout (TimeSpan) to get an HTTP response. It is the timeout to get a response, not the
   * timeout to read response data. Default value: 00:05:00. Type: string (or Expression with
   * resultType string), pattern: ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  httpRequestTimeout?: any;
}

/**
 * A copy activity Azure MySql sink.
 */
export interface AzureMySqlSink {
  /**
   * Polymorphic Discriminator
   */
  type: "AzureMySqlSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * A query to execute before starting the copy. Type: string (or Expression with resultType
   * string).
   */
  preCopyScript?: any;
}

/**
 * A copy activity Azure PostgreSQL sink.
 */
export interface AzurePostgreSqlSink {
  /**
   * Polymorphic Discriminator
   */
  type: "AzurePostgreSqlSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * A query to execute before starting the copy. Type: string (or Expression with resultType
   * string).
   */
  preCopyScript?: any;
}

/**
 * A copy activity ORC sink.
 */
export interface OrcSink {
  /**
   * Polymorphic Discriminator
   */
  type: "OrcSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * ORC store settings.
   */
  storeSettings?: StoreWriteSettingsUnion;
}

/**
 * A copy activity Json sink.
 */
export interface JsonSink {
  /**
   * Polymorphic Discriminator
   */
  type: "JsonSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * Json store settings.
   */
  storeSettings?: StoreWriteSettingsUnion;
  /**
   * Json format settings.
   */
  formatSettings?: JsonWriteSettings;
}

/**
 * A copy activity DelimitedText sink.
 */
export interface DelimitedTextSink {
  /**
   * Polymorphic Discriminator
   */
  type: "DelimitedTextSink";
  /**
   * Write batch size. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  writeBatchSize?: any;
  /**
   * Write batch timeout. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  writeBatchTimeout?: any;
  /**
   * Sink retry count. Type: integer (or Expression with resultType integer).
   */
  sinkRetryCount?: any;
  /**
   * Sink retry wait. Type: string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  sinkRetryWait?: any;
  /**
   * The maximum concurrent connection count for the sink data store. Type: integer (or Expression
   * with resultType integer).
   */
  maxConcurrentConnections?: any;
  /**
   * DelimitedText store settings.
   */
  storeSettings?: StoreWriteSettingsUnion;
  /**
   * DelimitedText format settings.
   */
  formatSettings?: DelimitedTextWriteSettings;
}

/**
 * Copy activity.
 */
export interface CopyActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "Copy";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Linked service reference.
   */
  linkedServiceName?: LinkedServiceReference;
  /**
   * Activity policy.
   */
  policy?: ActivityPolicy;
  /**
   * Copy activity source.
   */
  source: CopySourceUnion;
  /**
   * Copy activity sink.
   */
  sink: CopySinkUnion;
  /**
   * Copy activity translator. If not specified, tabular translator is used.
   */
  translator?: any;
  /**
   * Specifies whether to copy data via an interim staging. Default value is false. Type: boolean
   * (or Expression with resultType boolean).
   */
  enableStaging?: any;
  /**
   * Specifies interim staging settings when EnableStaging is true.
   */
  stagingSettings?: StagingSettings;
  /**
   * Maximum number of concurrent sessions opened on the source or sink to avoid overloading the
   * data store. Type: integer (or Expression with resultType integer), minimum: 0.
   */
  parallelCopies?: any;
  /**
   * Maximum number of data integration units that can be used to perform this data movement. Type:
   * integer (or Expression with resultType integer), minimum: 0.
   */
  dataIntegrationUnits?: any;
  /**
   * Whether to skip incompatible row. Default value is false. Type: boolean (or Expression with
   * resultType boolean).
   */
  enableSkipIncompatibleRow?: any;
  /**
   * Redirect incompatible row settings when EnableSkipIncompatibleRow is true.
   */
  redirectIncompatibleRowSettings?: RedirectIncompatibleRowSettings;
  /**
   * Log storage settings customer need to provide when enabling session log.
   */
  logStorageSettings?: LogStorageSettings;
  /**
   * Preserve Rules.
   */
  preserveRules?: any[];
  /**
   * Preserve rules.
   */
  preserve?: any[];
  /**
   * Whether to enable Data Consistency validation. Type: boolean (or Expression with resultType
   * boolean).
   */
  validateDataConsistency?: any;
  /**
   * Specify the fault tolerance for data consistency.
   */
  skipErrorFile?: SkipErrorFile;
  /**
   * List of inputs for the activity.
   */
  inputs?: DatasetReference[];
  /**
   * List of outputs for the activity.
   */
  outputs?: DatasetReference[];
}

/**
 * Contains the possible cases for ControlActivity.
 */
export type ControlActivityUnion = ControlActivity | WebHookActivity | AppendVariableActivity | SetVariableActivity | FilterActivity | ValidationActivity | UntilActivity | WaitActivity | ForEachActivity | SwitchActivity | IfConditionActivity | ExecutePipelineActivity;

/**
 * Base class for all control activities like IfCondition, ForEach , Until.
 */
export interface ControlActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "Container";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
}

/**
 * WebHook activity.
 */
export interface WebHookActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "WebHook";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * WebHook activity target endpoint and path. Type: string (or Expression with resultType
   * string).
   */
  url: any;
  /**
   * The timeout within which the webhook should be called back. If there is no value specified, it
   * defaults to 10 minutes. Type: string. Pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  timeout?: string;
  /**
   * Represents the headers that will be sent to the request. For example, to set the language and
   * type on a request: "headers" : { "Accept-Language": "en-us", "Content-Type":
   * "application/json" }. Type: string (or Expression with resultType string).
   */
  headers?: any;
  /**
   * Represents the payload that will be sent to the endpoint. Required for POST/PUT method, not
   * allowed for GET method Type: string (or Expression with resultType string).
   */
  body?: any;
  /**
   * Authentication method used for calling the endpoint.
   */
  authentication?: WebActivityAuthentication;
  /**
   * When set to true, statusCode,outputanderrorincallbackrequestbodywill be consumedby
   * activity.Theactivitycan be marked as failed bysettingstatusCode>=400 in callback
   * request. Default is false. Type: boolean (or Expression with resultType boolean).
   */
  reportStatusOnCallBack?: any;
}

/**
 * Append value for a Variable of type Array.
 */
export interface AppendVariableActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "AppendVariable";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Name of the variable whose value needs to be appended to.
   */
  variableName?: string;
  /**
   * Value to be appended. Could be a static value or Expression
   */
  value?: any;
}

/**
 * Set value for a Variable.
 */
export interface SetVariableActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "SetVariable";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Name of the variable whose value needs to be set.
   */
  variableName?: string;
  /**
   * Value to be set. Could be a static value or Expression
   */
  value?: any;
}

/**
 * Filter and return results from input array based on the conditions.
 */
export interface FilterActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "Filter";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Input array on which filter should be applied.
   */
  items: Expression;
  /**
   * Condition to be used for filtering the input.
   */
  condition: Expression;
}

/**
 * This activity verifies that an external resource exists.
 */
export interface ValidationActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "Validation";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Specifies the timeout for the activity to run. If there is no value specified, it takes the
   * value of TimeSpan.FromDays(7) which is 1 week as default. Type: string (or Expression with
   * resultType string), pattern: ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  timeout?: any;
  /**
   * A delay in seconds between validation attempts. If no value is specified, 10 seconds will be
   * used as the default. Type: integer (or Expression with resultType integer).
   */
  sleep?: any;
  /**
   * Can be used if dataset points to a file. The file must be greater than or equal in size to the
   * value specified. Type: integer (or Expression with resultType integer).
   */
  minimumSize?: any;
  /**
   * Can be used if dataset points to a folder. If set to true, the folder must have at least one
   * file. If set to false, the folder must be empty. Type: boolean (or Expression with resultType
   * boolean).
   */
  childItems?: any;
  /**
   * Validation activity dataset reference.
   */
  dataset: DatasetReference;
}

/**
 * This activity executes inner activities until the specified boolean expression results to true
 * or timeout is reached, whichever is earlier.
 */
export interface UntilActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "Until";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * An expression that would evaluate to Boolean. The loop will continue until this expression
   * evaluates to true
   */
  expression: Expression;
  /**
   * Specifies the timeout for the activity to run. If there is no value specified, it takes the
   * value of TimeSpan.FromDays(7) which is 1 week as default. Type: string (or Expression with
   * resultType string), pattern: ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])). Type:
   * string (or Expression with resultType string), pattern:
   * ((\d+)\.)?(\d\d):(60|([0-5][0-9])):(60|([0-5][0-9])).
   */
  timeout?: any;
  /**
   * List of activities to execute.
   */
  activities: ActivityUnion[];
}

/**
 * This activity suspends pipeline execution for the specified interval.
 */
export interface WaitActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "Wait";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Duration in seconds.
   */
  waitTimeInSeconds: any;
}

/**
 * This activity is used for iterating over a collection and execute given activities.
 */
export interface ForEachActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "ForEach";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Should the loop be executed in sequence or in parallel (max 50)
   */
  isSequential?: boolean;
  /**
   * Batch count to be used for controlling the number of parallel execution (when isSequential is
   * set to false).
   */
  batchCount?: number;
  /**
   * Collection to iterate.
   */
  items: Expression;
  /**
   * List of activities to execute .
   */
  activities: ActivityUnion[];
}

/**
 * Switch cases with have a value and corresponding activities.
 */
export interface SwitchCase {
  /**
   * Expected value that satisfies the expression result of the 'on' property.
   */
  value?: string;
  /**
   * List of activities to execute for satisfied case condition.
   */
  activities?: ActivityUnion[];
}

/**
 * This activity evaluates an expression and executes activities under the cases property that
 * correspond to the expression evaluation expected in the equals property.
 */
export interface SwitchActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "Switch";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * An expression that would evaluate to a string or integer. This is used to determine the block
   * of activities in cases that will be executed.
   */
  on: Expression;
  /**
   * List of cases that correspond to expected values of the 'on' property. This is an optional
   * property and if not provided, the activity will execute activities provided in
   * defaultActivities.
   */
  cases?: SwitchCase[];
  /**
   * List of activities to execute if no case condition is satisfied. This is an optional property
   * and if not provided, the activity will exit without any action.
   */
  defaultActivities?: ActivityUnion[];
}

/**
 * This activity evaluates a boolean expression and executes either the activities under the
 * ifTrueActivities property or the ifFalseActivities property depending on the result of the
 * expression.
 */
export interface IfConditionActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "IfCondition";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * An expression that would evaluate to Boolean. This is used to determine the block of
   * activities (ifTrueActivities or ifFalseActivities) that will be executed.
   */
  expression: Expression;
  /**
   * List of activities to execute if expression is evaluated to true. This is an optional property
   * and if not provided, the activity will exit without any action.
   */
  ifTrueActivities?: ActivityUnion[];
  /**
   * List of activities to execute if expression is evaluated to false. This is an optional
   * property and if not provided, the activity will exit without any action.
   */
  ifFalseActivities?: ActivityUnion[];
}

/**
 * Execute pipeline activity.
 */
export interface ExecutePipelineActivity {
  /**
   * Polymorphic Discriminator
   */
  type: "ExecutePipeline";
  /**
   * Activity name.
   */
  name: string;
  /**
   * Activity description.
   */
  description?: string;
  /**
   * Activity depends on condition.
   */
  dependsOn?: ActivityDependency[];
  /**
   * Activity user properties.
   */
  userProperties?: UserProperty[];
  /**
   * Pipeline reference.
   */
  pipelineProperty: PipelineReference;
  /**
   * Pipeline parameters.
   */
  parameters?: { [propertyName: string]: any };
  /**
   * Defines whether activity execution will wait for the dependent pipeline execution to finish.
   * Default is false.
   */
  waitOnCompletion?: boolean;
}

/**
 * The linked integration runtime information.
 */
export interface LinkedIntegrationRuntime {
  /**
   * The name of the linked integration runtime.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly name?: string;
  /**
   * The subscription ID for which the linked integration runtime belong to.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly subscriptionId?: string;
  /**
   * The name of the data factory for which the linked integration runtime belong to.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly dataFactoryName?: string;
  /**
   * The location of the data factory for which the linked integration runtime belong to.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly dataFactoryLocation?: string;
  /**
   * The creating time of the linked integration runtime.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly createTime?: Date;
}

/**
 * Properties of Self-hosted integration runtime node.
 */
export interface SelfHostedIntegrationRuntimeNode {
  /**
   * Name of the integration runtime node.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly nodeName?: string;
  /**
   * Machine name of the integration runtime node.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly machineName?: string;
  /**
   * URI for the host machine of the integration runtime.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly hostServiceUri?: string;
  /**
   * Status of the integration runtime node. Possible values include: 'NeedRegistration', 'Online',
   * 'Limited', 'Offline', 'Upgrading', 'Initializing', 'InitializeFailed'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly status?: SelfHostedIntegrationRuntimeNodeStatus;
  /**
   * The integration runtime capabilities dictionary
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly capabilities?: { [propertyName: string]: string };
  /**
   * Status of the integration runtime node version.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly versionStatus?: string;
  /**
   * Version of the integration runtime node.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly version?: string;
  /**
   * The time at which the integration runtime node was registered in ISO8601 format.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly registerTime?: Date;
  /**
   * The most recent time at which the integration runtime was connected in ISO8601 format.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly lastConnectTime?: Date;
  /**
   * The time at which the integration runtime will expire in ISO8601 format.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly expiryTime?: Date;
  /**
   * The time the node last started up.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly lastStartTime?: Date;
  /**
   * The integration runtime node last stop time.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly lastStopTime?: Date;
  /**
   * The result of the last integration runtime node update. Possible values include: 'None',
   * 'Succeed', 'Fail'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly lastUpdateResult?: IntegrationRuntimeUpdateResult;
  /**
   * The last time for the integration runtime node update start.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly lastStartUpdateTime?: Date;
  /**
   * The last time for the integration runtime node update end.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly lastEndUpdateTime?: Date;
  /**
   * Indicates whether this node is the active dispatcher for integration runtime requests.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly isActiveDispatcher?: boolean;
  /**
   * Maximum concurrent jobs on the integration runtime node.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly concurrentJobsLimit?: number;
  /**
   * The maximum concurrent jobs in this integration runtime.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly maxConcurrentJobs?: number;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Self-hosted integration runtime status.
 */
export interface SelfHostedIntegrationRuntimeStatus {
  /**
   * Polymorphic Discriminator
   */
  type: "SelfHosted";
  /**
   * The data factory name which the integration runtime belong to.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly dataFactoryName?: string;
  /**
   * The state of integration runtime. Possible values include: 'Initial', 'Stopped', 'Started',
   * 'Starting', 'Stopping', 'NeedRegistration', 'Online', 'Limited', 'Offline', 'AccessDenied'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly state?: IntegrationRuntimeState;
  /**
   * The time at which the integration runtime was created, in ISO8601 format.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly createTime?: Date;
  /**
   * The task queue id of the integration runtime.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly taskQueueId?: string;
  /**
   * It is used to set the encryption mode for node-node communication channel (when more than 2
   * self-hosted integration runtime nodes exist). Possible values include: 'NotSet',
   * 'SslEncrypted', 'NotEncrypted'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly internalChannelEncryption?: IntegrationRuntimeInternalChannelEncryptionMode;
  /**
   * Version of the integration runtime.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly version?: string;
  /**
   * The list of nodes for this integration runtime.
   */
  nodes?: SelfHostedIntegrationRuntimeNode[];
  /**
   * The date at which the integration runtime will be scheduled to update, in ISO8601 format.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly scheduledUpdateDate?: Date;
  /**
   * The time in the date scheduled by service to update the integration runtime, e.g., PT03H is 3
   * hours
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly updateDelayOffset?: string;
  /**
   * The local time zone offset in hours.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly localTimeZoneOffset?: string;
  /**
   * Object with additional information about integration runtime capabilities.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly capabilities?: { [propertyName: string]: string };
  /**
   * The URLs for the services used in integration runtime backend service.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly serviceUrls?: string[];
  /**
   * Whether Self-hosted integration runtime auto update has been turned on. Possible values
   * include: 'On', 'Off'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly autoUpdate?: IntegrationRuntimeAutoUpdate;
  /**
   * Status of the integration runtime version.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly versionStatus?: string;
  /**
   * The list of linked integration runtimes that are created to share with this integration
   * runtime.
   */
  links?: LinkedIntegrationRuntime[];
  /**
   * The version that the integration runtime is going to update to.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly pushedVersion?: string;
  /**
   * The latest version on download center.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly latestVersion?: string;
  /**
   * The estimated time when the self-hosted integration runtime will be updated.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly autoUpdateETA?: Date;
}

/**
 * Properties of managed integration runtime operation result.
 */
export interface ManagedIntegrationRuntimeOperationResult {
  /**
   * The operation type. Could be start or stop.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly type?: string;
  /**
   * The start time of the operation.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly startTime?: Date;
  /**
   * The operation result.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly result?: string;
  /**
   * The error code.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly errorCode?: string;
  /**
   * Managed integration runtime error parameters.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly parameters?: string[];
  /**
   * The activity id for the operation request.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly activityId?: string;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Error definition for managed integration runtime.
 */
export interface ManagedIntegrationRuntimeError {
  /**
   * The time when the error occurred.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly time?: Date;
  /**
   * Error code.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly code?: string;
  /**
   * Managed integration runtime error parameters.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly parameters?: string[];
  /**
   * Error message.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly message?: string;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Properties of integration runtime node.
 */
export interface ManagedIntegrationRuntimeNode {
  /**
   * The managed integration runtime node id.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly nodeId?: string;
  /**
   * The managed integration runtime node status. Possible values include: 'Starting', 'Available',
   * 'Recycling', 'Unavailable'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly status?: ManagedIntegrationRuntimeNodeStatus;
  /**
   * The errors that occurred on this integration runtime node.
   */
  errors?: ManagedIntegrationRuntimeError[];
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Managed integration runtime status.
 */
export interface ManagedIntegrationRuntimeStatus {
  /**
   * Polymorphic Discriminator
   */
  type: "Managed";
  /**
   * The data factory name which the integration runtime belong to.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly dataFactoryName?: string;
  /**
   * The state of integration runtime. Possible values include: 'Initial', 'Stopped', 'Started',
   * 'Starting', 'Stopping', 'NeedRegistration', 'Online', 'Limited', 'Offline', 'AccessDenied'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly state?: IntegrationRuntimeState;
  /**
   * The time at which the integration runtime was created, in ISO8601 format.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly createTime?: Date;
  /**
   * The list of nodes for managed integration runtime.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly nodes?: ManagedIntegrationRuntimeNode[];
  /**
   * The errors that occurred on this integration runtime.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly otherErrors?: ManagedIntegrationRuntimeError[];
  /**
   * The last operation result that occurred on this integration runtime.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly lastOperation?: ManagedIntegrationRuntimeOperationResult;
}

/**
 * Contains the possible cases for LinkedIntegrationRuntimeType.
 */
export type LinkedIntegrationRuntimeTypeUnion = LinkedIntegrationRuntimeType | LinkedIntegrationRuntimeRbacAuthorization | LinkedIntegrationRuntimeKeyAuthorization;

/**
 * The base definition of a linked integration runtime.
 */
export interface LinkedIntegrationRuntimeType {
  /**
   * Polymorphic Discriminator
   */
  authorizationType: "LinkedIntegrationRuntimeType";
}

/**
 * The role based access control (RBAC) authorization type integration runtime.
 */
export interface LinkedIntegrationRuntimeRbacAuthorization {
  /**
   * Polymorphic Discriminator
   */
  authorizationType: "RBAC";
  /**
   * The resource identifier of the integration runtime to be shared.
   */
  resourceId: string;
}

/**
 * The key authorization type integration runtime.
 */
export interface LinkedIntegrationRuntimeKeyAuthorization {
  /**
   * Polymorphic Discriminator
   */
  authorizationType: "Key";
  /**
   * The key used for authorization.
   */
  key: SecureString;
}

/**
 * Self-hosted integration runtime.
 */
export interface SelfHostedIntegrationRuntime {
  /**
   * Polymorphic Discriminator
   */
  type: "SelfHosted";
  /**
   * Integration runtime description.
   */
  description?: string;
  linkedInfo?: LinkedIntegrationRuntimeTypeUnion;
}

/**
 * The entity reference.
 */
export interface EntityReference {
  /**
   * The type of this referenced entity. Possible values include: 'IntegrationRuntimeReference',
   * 'LinkedServiceReference'
   */
  type?: IntegrationRuntimeEntityReferenceType;
  /**
   * The name of this referenced entity.
   */
  referenceName?: string;
}

/**
 * Package store for the SSIS integration runtime.
 */
export interface PackageStore {
  /**
   * The name of the package store
   */
  name: string;
  /**
   * The package store linked service reference.
   */
  packageStoreLinkedService: EntityReference;
}

/**
 * Contains the possible cases for CustomSetupBase.
 */
export type CustomSetupBaseUnion = CustomSetupBase | ComponentSetup | EnvironmentVariableSetup | CmdkeySetup;

/**
 * The base definition of the custom setup.
 */
export interface CustomSetupBase {
  /**
   * Polymorphic Discriminator
   */
  type: "CustomSetupBase";
}

/**
 * The custom setup of installing 3rd party components.
 */
export interface ComponentSetup {
  /**
   * Polymorphic Discriminator
   */
  type: "ComponentSetup";
  /**
   * The name of the 3rd party component.
   */
  componentName: string;
  /**
   * The license key to activate the component.
   */
  licenseKey?: SecretBaseUnion;
}

/**
 * The custom setup of setting environment variable.
 */
export interface EnvironmentVariableSetup {
  /**
   * Polymorphic Discriminator
   */
  type: "EnvironmentVariableSetup";
  /**
   * The name of the environment variable.
   */
  variableName: string;
  /**
   * The value of the environment variable.
   */
  variableValue: string;
}

/**
 * The custom setup of running cmdkey commands.
 */
export interface CmdkeySetup {
  /**
   * Polymorphic Discriminator
   */
  type: "CmdkeySetup";
  /**
   * The server name of data source access.
   */
  targetName: any;
  /**
   * The user name of data source access.
   */
  userName: any;
  /**
   * The password of data source access.
   */
  password: SecretBaseUnion;
}

/**
 * Data proxy properties for a managed dedicated integration runtime.
 */
export interface IntegrationRuntimeDataProxyProperties {
  /**
   * The self-hosted integration runtime reference.
   */
  connectVia?: EntityReference;
  /**
   * The staging linked service reference.
   */
  stagingLinkedService?: EntityReference;
  /**
   * The path to contain the staged data in the Blob storage.
   */
  path?: string;
}

/**
 * Custom setup script properties for a managed dedicated integration runtime.
 */
export interface IntegrationRuntimeCustomSetupScriptProperties {
  /**
   * The URI of the Azure blob container that contains the custom setup script.
   */
  blobContainerUri?: string;
  /**
   * The SAS token of the Azure blob container.
   */
  sasToken?: SecureString;
}

/**
 * Catalog information for managed dedicated integration runtime.
 */
export interface IntegrationRuntimeSsisCatalogInfo {
  /**
   * The catalog database server URL.
   */
  catalogServerEndpoint?: string;
  /**
   * The administrator user name of catalog database.
   */
  catalogAdminUserName?: string;
  /**
   * The password of the administrator user account of the catalog database.
   */
  catalogAdminPassword?: SecureString;
  /**
   * The pricing tier for the catalog database. The valid values could be found in
   * https://azure.microsoft.com/en-us/pricing/details/sql-database/. Possible values include:
   * 'Basic', 'Standard', 'Premium', 'PremiumRS'
   */
  catalogPricingTier?: IntegrationRuntimeSsisCatalogPricingTier;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * SSIS properties for managed integration runtime.
 */
export interface IntegrationRuntimeSsisProperties {
  /**
   * Catalog information for managed dedicated integration runtime.
   */
  catalogInfo?: IntegrationRuntimeSsisCatalogInfo;
  /**
   * License type for bringing your own license scenario. Possible values include: 'BasePrice',
   * 'LicenseIncluded'
   */
  licenseType?: IntegrationRuntimeLicenseType;
  /**
   * Custom setup script properties for a managed dedicated integration runtime.
   */
  customSetupScriptProperties?: IntegrationRuntimeCustomSetupScriptProperties;
  /**
   * Data proxy properties for a managed dedicated integration runtime.
   */
  dataProxyProperties?: IntegrationRuntimeDataProxyProperties;
  /**
   * The edition for the SSIS Integration Runtime. Possible values include: 'Standard',
   * 'Enterprise'
   */
  edition?: IntegrationRuntimeEdition;
  /**
   * Custom setup without script properties for a SSIS integration runtime.
   */
  expressCustomSetupProperties?: CustomSetupBaseUnion[];
  /**
   * Package stores for the SSIS Integration Runtime.
   */
  packageStores?: PackageStore[];
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * VNet properties for managed integration runtime.
 */
export interface IntegrationRuntimeVNetProperties {
  /**
   * The ID of the VNet that this integration runtime will join.
   */
  vNetId?: string;
  /**
   * The name of the subnet this integration runtime will join.
   */
  subnet?: string;
  /**
   * Resource IDs of the public IP addresses that this integration runtime will use.
   */
  publicIPs?: string[];
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Data flow properties for managed integration runtime.
 */
export interface IntegrationRuntimeDataFlowProperties {
  /**
   * Compute type of the cluster which will execute data flow job. Possible values include:
   * 'General', 'MemoryOptimized', 'ComputeOptimized'
   */
  computeType?: DataFlowComputeType;
  /**
   * Core count of the cluster which will execute data flow job. Supported values are: 8, 16, 32,
   * 48, 80, 144 and 272.
   */
  coreCount?: number;
  /**
   * Time to live (in minutes) setting of the cluster which will execute data flow job.
   */
  timeToLive?: number;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * The compute resource properties for managed integration runtime.
 */
export interface IntegrationRuntimeComputeProperties {
  /**
   * The location for managed integration runtime. The supported regions could be found on
   * https://docs.microsoft.com/en-us/azure/data-factory/data-factory-data-movement-activities
   */
  location?: string;
  /**
   * The node size requirement to managed integration runtime.
   */
  nodeSize?: string;
  /**
   * The required number of nodes for managed integration runtime.
   */
  numberOfNodes?: number;
  /**
   * Maximum parallel executions count per node for managed integration runtime.
   */
  maxParallelExecutionsPerNode?: number;
  /**
   * Data flow properties for managed integration runtime.
   */
  dataFlowProperties?: IntegrationRuntimeDataFlowProperties;
  /**
   * VNet properties for managed integration runtime.
   */
  vNetProperties?: IntegrationRuntimeVNetProperties;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Managed integration runtime, including managed elastic and managed dedicated integration
 * runtimes.
 */
export interface ManagedIntegrationRuntime {
  /**
   * Polymorphic Discriminator
   */
  type: "Managed";
  /**
   * Integration runtime description.
   */
  description?: string;
  /**
   * Integration runtime state, only valid for managed dedicated integration runtime. Possible
   * values include: 'Initial', 'Stopped', 'Started', 'Starting', 'Stopping', 'NeedRegistration',
   * 'Online', 'Limited', 'Offline', 'AccessDenied'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly state?: IntegrationRuntimeState;
  /**
   * The compute resource for managed integration runtime.
   */
  computeProperties?: IntegrationRuntimeComputeProperties;
  /**
   * SSIS properties for managed integration runtime.
   */
  ssisProperties?: IntegrationRuntimeSsisProperties;
}

/**
 * The IP address of self-hosted integration runtime node.
 */
export interface IntegrationRuntimeNodeIpAddress {
  /**
   * The IP address of self-hosted integration runtime node.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly ipAddress?: string;
}

/**
 * Ssis variable.
 */
export interface SsisVariable {
  /**
   * Variable id.
   */
  id?: number;
  /**
   * Variable name.
   */
  name?: string;
  /**
   * Variable description.
   */
  description?: string;
  /**
   * Variable type.
   */
  dataType?: string;
  /**
   * Whether variable is sensitive.
   */
  sensitive?: boolean;
  /**
   * Variable value.
   */
  value?: string;
  /**
   * Variable sensitive value.
   */
  sensitiveValue?: string;
}

/**
 * Contains the possible cases for SsisObjectMetadata.
 */
export type SsisObjectMetadataUnion = SsisObjectMetadata | SsisEnvironment | SsisPackage | SsisProject | SsisFolder;

/**
 * SSIS object metadata.
 */
export interface SsisObjectMetadata {
  /**
   * Polymorphic Discriminator
   */
  type: "SsisObjectMetadata";
  /**
   * Metadata id.
   */
  id?: number;
  /**
   * Metadata name.
   */
  name?: string;
  /**
   * Metadata description.
   */
  description?: string;
}

/**
 * Ssis environment.
 */
export interface SsisEnvironment {
  /**
   * Polymorphic Discriminator
   */
  type: "Environment";
  /**
   * Metadata id.
   */
  id?: number;
  /**
   * Metadata name.
   */
  name?: string;
  /**
   * Metadata description.
   */
  description?: string;
  /**
   * Folder id which contains environment.
   */
  folderId?: number;
  /**
   * Variable in environment
   */
  variables?: SsisVariable[];
}

/**
 * Ssis parameter.
 */
export interface SsisParameter {
  /**
   * Parameter id.
   */
  id?: number;
  /**
   * Parameter name.
   */
  name?: string;
  /**
   * Parameter description.
   */
  description?: string;
  /**
   * Parameter type.
   */
  dataType?: string;
  /**
   * Whether parameter is required.
   */
  required?: boolean;
  /**
   * Whether parameter is sensitive.
   */
  sensitive?: boolean;
  /**
   * Design default value of parameter.
   */
  designDefaultValue?: string;
  /**
   * Default value of parameter.
   */
  defaultValue?: string;
  /**
   * Default sensitive value of parameter.
   */
  sensitiveDefaultValue?: string;
  /**
   * Parameter value type.
   */
  valueType?: string;
  /**
   * Parameter value set.
   */
  valueSet?: boolean;
  /**
   * Parameter reference variable.
   */
  variable?: string;
}

/**
 * Ssis Package.
 */
export interface SsisPackage {
  /**
   * Polymorphic Discriminator
   */
  type: "Package";
  /**
   * Metadata id.
   */
  id?: number;
  /**
   * Metadata name.
   */
  name?: string;
  /**
   * Metadata description.
   */
  description?: string;
  /**
   * Folder id which contains package.
   */
  folderId?: number;
  /**
   * Project version which contains package.
   */
  projectVersion?: number;
  /**
   * Project id which contains package.
   */
  projectId?: number;
  /**
   * Parameters in package
   */
  parameters?: SsisParameter[];
}

/**
 * Ssis environment reference.
 */
export interface SsisEnvironmentReference {
  /**
   * Environment reference id.
   */
  id?: number;
  /**
   * Environment folder name.
   */
  environmentFolderName?: string;
  /**
   * Environment name.
   */
  environmentName?: string;
  /**
   * Reference type
   */
  referenceType?: string;
}

/**
 * Ssis project.
 */
export interface SsisProject {
  /**
   * Polymorphic Discriminator
   */
  type: "Project";
  /**
   * Metadata id.
   */
  id?: number;
  /**
   * Metadata name.
   */
  name?: string;
  /**
   * Metadata description.
   */
  description?: string;
  /**
   * Folder id which contains project.
   */
  folderId?: number;
  /**
   * Project version.
   */
  version?: number;
  /**
   * Environment reference in project
   */
  environmentRefs?: SsisEnvironmentReference[];
  /**
   * Parameters in project
   */
  parameters?: SsisParameter[];
}

/**
 * Ssis folder.
 */
export interface SsisFolder {
  /**
   * Polymorphic Discriminator
   */
  type: "Folder";
  /**
   * Metadata id.
   */
  id?: number;
  /**
   * Metadata name.
   */
  name?: string;
  /**
   * Metadata description.
   */
  description?: string;
}

/**
 * A list of SSIS object metadata.
 */
export interface SsisObjectMetadataListResponse {
  /**
   * List of SSIS object metadata.
   */
  value?: SsisObjectMetadataUnion[];
  /**
   * The link to the next page of results, if any remaining results exist.
   */
  nextLink?: string;
}

/**
 * Monitoring data for integration runtime node.
 */
export interface IntegrationRuntimeNodeMonitoringData {
  /**
   * Name of the integration runtime node.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly nodeName?: string;
  /**
   * Available memory (MB) on the integration runtime node.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly availableMemoryInMB?: number;
  /**
   * CPU percentage on the integration runtime node.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly cpuUtilization?: number;
  /**
   * Maximum concurrent jobs on the integration runtime node.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly concurrentJobsLimit?: number;
  /**
   * The number of jobs currently running on the integration runtime node.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly concurrentJobsRunning?: number;
  /**
   * The maximum concurrent jobs in this integration runtime.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly maxConcurrentJobs?: number;
  /**
   * Sent bytes on the integration runtime node.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly sentBytes?: number;
  /**
   * Received bytes on the integration runtime node.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly receivedBytes?: number;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Get monitoring data response.
 */
export interface IntegrationRuntimeMonitoringData {
  /**
   * Integration runtime name.
   */
  name?: string;
  /**
   * Integration runtime node monitoring data.
   */
  nodes?: IntegrationRuntimeNodeMonitoringData[];
}

/**
 * The integration runtime authentication keys.
 */
export interface IntegrationRuntimeAuthKeys {
  /**
   * The primary integration runtime authentication key.
   */
  authKey1?: string;
  /**
   * The secondary integration runtime authentication key.
   */
  authKey2?: string;
}

/**
 * Parameters to regenerate the authentication key.
 */
export interface IntegrationRuntimeRegenerateKeyParameters {
  /**
   * The name of the authentication key to regenerate. Possible values include: 'authKey1',
   * 'authKey2'
   */
  keyName?: IntegrationRuntimeAuthKeyName;
}

/**
 * Connection information for encrypting the on-premises data source credentials.
 */
export interface IntegrationRuntimeConnectionInfo {
  /**
   * The token generated in service. Callers use this token to authenticate to integration runtime.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly serviceToken?: string;
  /**
   * The integration runtime SSL certificate thumbprint. Click-Once application uses it to do
   * server validation.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly identityCertThumbprint?: string;
  /**
   * The on-premises integration runtime host URL.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly hostServiceUri?: string;
  /**
   * The integration runtime version.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly version?: string;
  /**
   * The public key for encrypting a credential when transferring the credential to the integration
   * runtime.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly publicKey?: string;
  /**
   * Whether the identity certificate is expired.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly isIdentityCertExprired?: boolean;
  /**
   * Describes unknown properties. The value of an unknown property can be of "any" type.
   */
  [property: string]: any;
}

/**
 * Optional Parameters.
 */
export interface FactoriesCreateOrUpdateOptionalParams extends msRest.RequestOptionsBase {
  /**
   * ETag of the factory entity. Should only be specified for update, for which it should match
   * existing entity or can be * for unconditional update.
   */
  ifMatch?: string;
}

/**
 * Optional Parameters.
 */
export interface FactoriesGetOptionalParams extends msRest.RequestOptionsBase {
  /**
   * ETag of the factory entity. Should only be specified for get. If the ETag matches the existing
   * entity tag, or if * was provided, then no content will be returned.
   */
  ifNoneMatch?: string;
}

/**
 * Optional Parameters.
 */
export interface IntegrationRuntimesCreateOrUpdateOptionalParams extends msRest.RequestOptionsBase {
  /**
   * ETag of the integration runtime entity. Should only be specified for update, for which it
   * should match existing entity or can be * for unconditional update.
   */
  ifMatch?: string;
}

/**
 * Optional Parameters.
 */
export interface IntegrationRuntimesGetOptionalParams extends msRest.RequestOptionsBase {
  /**
   * ETag of the integration runtime entity. Should only be specified for get. If the ETag matches
   * the existing entity tag, or if * was provided, then no content will be returned.
   */
  ifNoneMatch?: string;
}

/**
 * Optional Parameters.
 */
export interface IntegrationRuntimeObjectMetadataGetOptionalParams extends msRest.RequestOptionsBase {
  /**
   * The parameters for getting a SSIS object metadata.
   */
  getMetadataRequest?: GetSsisObjectMetadataRequest;
}

/**
 * Optional Parameters.
 */
export interface LinkedServicesCreateOrUpdateOptionalParams extends msRest.RequestOptionsBase {
  /**
   * ETag of the linkedService entity.  Should only be specified for update, for which it should
   * match existing entity or can be * for unconditional update.
   */
  ifMatch?: string;
}

/**
 * Optional Parameters.
 */
export interface LinkedServicesGetOptionalParams extends msRest.RequestOptionsBase {
  /**
   * ETag of the linked service entity. Should only be specified for get. If the ETag matches the
   * existing entity tag, or if * was provided, then no content will be returned.
   */
  ifNoneMatch?: string;
}

/**
 * Optional Parameters.
 */
export interface DatasetsCreateOrUpdateOptionalParams extends msRest.RequestOptionsBase {
  /**
   * ETag of the dataset entity.  Should only be specified for update, for which it should match
   * existing entity or can be * for unconditional update.
   */
  ifMatch?: string;
}

/**
 * Optional Parameters.
 */
export interface DatasetsGetOptionalParams extends msRest.RequestOptionsBase {
  /**
   * ETag of the dataset entity. Should only be specified for get. If the ETag matches the existing
   * entity tag, or if * was provided, then no content will be returned.
   */
  ifNoneMatch?: string;
}

/**
 * Optional Parameters.
 */
export interface PipelinesCreateOrUpdateOptionalParams extends msRest.RequestOptionsBase {
  /**
   * ETag of the pipeline entity.  Should only be specified for update, for which it should match
   * existing entity or can be * for unconditional update.
   */
  ifMatch?: string;
}

/**
 * Optional Parameters.
 */
export interface PipelinesGetOptionalParams extends msRest.RequestOptionsBase {
  /**
   * ETag of the pipeline entity. Should only be specified for get. If the ETag matches the
   * existing entity tag, or if * was provided, then no content will be returned.
   */
  ifNoneMatch?: string;
}

/**
 * Optional Parameters.
 */
export interface PipelinesCreateRunOptionalParams extends msRest.RequestOptionsBase {
  /**
   * The pipeline run identifier. If run ID is specified the parameters of the specified run will
   * be used to create a new run.
   */
  referencePipelineRunId?: string;
  /**
   * Recovery mode flag. If recovery mode is set to true, the specified referenced pipeline run and
   * the new run will be grouped under the same groupId.
   */
  isRecovery?: boolean;
  /**
   * In recovery mode, the rerun will start from this activity. If not specified, all activities
   * will run.
   */
  startActivityName?: string;
  /**
   * In recovery mode, if set to true, the rerun will start from failed activities. The property
   * will be used only if startActivityName is not specified.
   */
  startFromFailure?: boolean;
  /**
   * Parameters of the pipeline run. These parameters will be used only if the runId is not
   * specified.
   */
  parameters?: { [propertyName: string]: any };
}

/**
 * Optional Parameters.
 */
export interface PipelineRunsCancelOptionalParams extends msRest.RequestOptionsBase {
  /**
   * If true, cancel all the Child pipelines that are triggered by the current pipeline.
   */
  isRecursive?: boolean;
}

/**
 * Optional Parameters.
 */
export interface TriggersCreateOrUpdateOptionalParams extends msRest.RequestOptionsBase {
  /**
   * ETag of the trigger entity.  Should only be specified for update, for which it should match
   * existing entity or can be * for unconditional update.
   */
  ifMatch?: string;
}

/**
 * Optional Parameters.
 */
export interface TriggersGetOptionalParams extends msRest.RequestOptionsBase {
  /**
   * ETag of the trigger entity. Should only be specified for get. If the ETag matches the existing
   * entity tag, or if * was provided, then no content will be returned.
   */
  ifNoneMatch?: string;
}

/**
 * Optional Parameters.
 */
export interface DataFlowsCreateOrUpdateOptionalParams extends msRest.RequestOptionsBase {
  /**
   * ETag of the data flow entity. Should only be specified for update, for which it should match
   * existing entity or can be * for unconditional update.
   */
  ifMatch?: string;
}

/**
 * Optional Parameters.
 */
export interface DataFlowsGetOptionalParams extends msRest.RequestOptionsBase {
  /**
   * ETag of the data flow entity. Should only be specified for get. If the ETag matches the
   * existing entity tag, or if * was provided, then no content will be returned.
   */
  ifNoneMatch?: string;
}

/**
 * An interface representing DataFactoryManagementClientOptions.
 */
export interface DataFactoryManagementClientOptions extends AzureServiceClientOptions {
  baseUri?: string;
}

/**
 * Defines headers for Create operation.
 */
export interface DataFlowDebugSessionCreateHeaders {
  /**
   * URI to poll for asynchronous operation status.
   */
  location: string;
}

/**
 * Defines headers for ExecuteCommand operation.
 */
export interface DataFlowDebugSessionExecuteCommandHeaders {
  /**
   * URI to poll for asynchronous operation status.
   */
  location: string;
}

/**
 * @interface
 * A list of operations that can be performed by the Data Factory service.
 * @extends Array<Operation>
 */
export interface OperationListResponse extends Array<Operation> {
  /**
   * The link to the next page of results, if any remaining results exist.
   */
  nextLink?: string;
}

/**
 * @interface
 * A list of factory resources.
 * @extends Array<Factory>
 */
export interface FactoryListResponse extends Array<Factory> {
  /**
   * The link to the next page of results, if any remaining results exist.
   */
  nextLink?: string;
}

/**
 * @interface
 * A list of integration runtime resources.
 * @extends Array<IntegrationRuntimeResource>
 */
export interface IntegrationRuntimeListResponse extends Array<IntegrationRuntimeResource> {
  /**
   * The link to the next page of results, if any remaining results exist.
   */
  nextLink?: string;
}

/**
 * @interface
 * A list of linked service resources.
 * @extends Array<LinkedServiceResource>
 */
export interface LinkedServiceListResponse extends Array<LinkedServiceResource> {
  /**
   * The link to the next page of results, if any remaining results exist.
   */
  nextLink?: string;
}

/**
 * @interface
 * A list of dataset resources.
 * @extends Array<DatasetResource>
 */
export interface DatasetListResponse extends Array<DatasetResource> {
  /**
   * The link to the next page of results, if any remaining results exist.
   */
  nextLink?: string;
}

/**
 * @interface
 * A list of pipeline resources.
 * @extends Array<PipelineResource>
 */
export interface PipelineListResponse extends Array<PipelineResource> {
  /**
   * The link to the next page of results, if any remaining results exist.
   */
  nextLink?: string;
}

/**
 * @interface
 * A list of trigger resources.
 * @extends Array<TriggerResource>
 */
export interface TriggerListResponse extends Array<TriggerResource> {
  /**
   * The link to the next page of results, if any remaining results exist.
   */
  nextLink?: string;
}

/**
 * @interface
 * A list of data flow resources.
 * @extends Array<DataFlowResource>
 */
export interface DataFlowListResponse extends Array<DataFlowResource> {
  /**
   * The link to the next page of results, if any remaining results exist.
   */
  nextLink?: string;
}

/**
 * @interface
 * A list of active debug sessions.
 * @extends Array<DataFlowDebugSessionInfo>
 */
export interface QueryDataFlowDebugSessionsResponse extends Array<DataFlowDebugSessionInfo> {
  /**
   * The link to the next page of results, if any remaining results exist.
   */
  nextLink?: string;
}

/**
 * Defines values for GlobalParameterType.
 * Possible values include: 'Object', 'String', 'Int', 'Float', 'Bool', 'Array'
 * @readonly
 * @enum {string}
 */
export type GlobalParameterType = 'Object' | 'String' | 'Int' | 'Float' | 'Bool' | 'Array';

/**
 * Defines values for IntegrationRuntimeState.
 * Possible values include: 'Initial', 'Stopped', 'Started', 'Starting', 'Stopping',
 * 'NeedRegistration', 'Online', 'Limited', 'Offline', 'AccessDenied'
 * @readonly
 * @enum {string}
 */
export type IntegrationRuntimeState = 'Initial' | 'Stopped' | 'Started' | 'Starting' | 'Stopping' | 'NeedRegistration' | 'Online' | 'Limited' | 'Offline' | 'AccessDenied';

/**
 * Defines values for IntegrationRuntimeAutoUpdate.
 * Possible values include: 'On', 'Off'
 * @readonly
 * @enum {string}
 */
export type IntegrationRuntimeAutoUpdate = 'On' | 'Off';

/**
 * Defines values for ParameterType.
 * Possible values include: 'Object', 'String', 'Int', 'Float', 'Bool', 'Array', 'SecureString'
 * @readonly
 * @enum {string}
 */
export type ParameterType = 'Object' | 'String' | 'Int' | 'Float' | 'Bool' | 'Array' | 'SecureString';

/**
 * Defines values for DependencyCondition.
 * Possible values include: 'Succeeded', 'Failed', 'Skipped', 'Completed'
 * @readonly
 * @enum {string}
 */
export type DependencyCondition = 'Succeeded' | 'Failed' | 'Skipped' | 'Completed';

/**
 * Defines values for VariableType.
 * Possible values include: 'String', 'Bool', 'Array'
 * @readonly
 * @enum {string}
 */
export type VariableType = 'String' | 'Bool' | 'Array';

/**
 * Defines values for TriggerRuntimeState.
 * Possible values include: 'Started', 'Stopped', 'Disabled'
 * @readonly
 * @enum {string}
 */
export type TriggerRuntimeState = 'Started' | 'Stopped' | 'Disabled';

/**
 * Defines values for EventSubscriptionStatus.
 * Possible values include: 'Enabled', 'Provisioning', 'Deprovisioning', 'Disabled', 'Unknown'
 * @readonly
 * @enum {string}
 */
export type EventSubscriptionStatus = 'Enabled' | 'Provisioning' | 'Deprovisioning' | 'Disabled' | 'Unknown';

/**
 * Defines values for RunQueryFilterOperand.
 * Possible values include: 'PipelineName', 'Status', 'RunStart', 'RunEnd', 'ActivityName',
 * 'ActivityRunStart', 'ActivityRunEnd', 'ActivityType', 'TriggerName', 'TriggerRunTimestamp',
 * 'RunGroupId', 'LatestOnly'
 * @readonly
 * @enum {string}
 */
export type RunQueryFilterOperand = 'PipelineName' | 'Status' | 'RunStart' | 'RunEnd' | 'ActivityName' | 'ActivityRunStart' | 'ActivityRunEnd' | 'ActivityType' | 'TriggerName' | 'TriggerRunTimestamp' | 'RunGroupId' | 'LatestOnly';

/**
 * Defines values for RunQueryFilterOperator.
 * Possible values include: 'Equals', 'NotEquals', 'In', 'NotIn'
 * @readonly
 * @enum {string}
 */
export type RunQueryFilterOperator = 'Equals' | 'NotEquals' | 'In' | 'NotIn';

/**
 * Defines values for RunQueryOrderByField.
 * Possible values include: 'RunStart', 'RunEnd', 'PipelineName', 'Status', 'ActivityName',
 * 'ActivityRunStart', 'ActivityRunEnd', 'TriggerName', 'TriggerRunTimestamp'
 * @readonly
 * @enum {string}
 */
export type RunQueryOrderByField = 'RunStart' | 'RunEnd' | 'PipelineName' | 'Status' | 'ActivityName' | 'ActivityRunStart' | 'ActivityRunEnd' | 'TriggerName' | 'TriggerRunTimestamp';

/**
 * Defines values for RunQueryOrder.
 * Possible values include: 'ASC', 'DESC'
 * @readonly
 * @enum {string}
 */
export type RunQueryOrder = 'ASC' | 'DESC';

/**
 * Defines values for TriggerRunStatus.
 * Possible values include: 'Succeeded', 'Failed', 'Inprogress'
 * @readonly
 * @enum {string}
 */
export type TriggerRunStatus = 'Succeeded' | 'Failed' | 'Inprogress';

/**
 * Defines values for DataFlowDebugCommandType.
 * Possible values include: 'executePreviewQuery', 'executeStatisticsQuery',
 * 'executeExpressionQuery'
 * @readonly
 * @enum {string}
 */
export type DataFlowDebugCommandType = 'executePreviewQuery' | 'executeStatisticsQuery' | 'executeExpressionQuery';

/**
 * Defines values for GoogleAdWordsAuthenticationType.
 * Possible values include: 'ServiceAuthentication', 'UserAuthentication'
 * @readonly
 * @enum {string}
 */
export type GoogleAdWordsAuthenticationType = 'ServiceAuthentication' | 'UserAuthentication';

/**
 * Defines values for SparkServerType.
 * Possible values include: 'SharkServer', 'SharkServer2', 'SparkThriftServer'
 * @readonly
 * @enum {string}
 */
export type SparkServerType = 'SharkServer' | 'SharkServer2' | 'SparkThriftServer';

/**
 * Defines values for SparkThriftTransportProtocol.
 * Possible values include: 'Binary', 'SASL', 'HTTP '
 * @readonly
 * @enum {string}
 */
export type SparkThriftTransportProtocol = 'Binary' | 'SASL' | 'HTTP ';

/**
 * Defines values for SparkAuthenticationType.
 * Possible values include: 'Anonymous', 'Username', 'UsernameAndPassword',
 * 'WindowsAzureHDInsightService'
 * @readonly
 * @enum {string}
 */
export type SparkAuthenticationType = 'Anonymous' | 'Username' | 'UsernameAndPassword' | 'WindowsAzureHDInsightService';

/**
 * Defines values for ServiceNowAuthenticationType.
 * Possible values include: 'Basic', 'OAuth2'
 * @readonly
 * @enum {string}
 */
export type ServiceNowAuthenticationType = 'Basic' | 'OAuth2';

/**
 * Defines values for PrestoAuthenticationType.
 * Possible values include: 'Anonymous', 'LDAP'
 * @readonly
 * @enum {string}
 */
export type PrestoAuthenticationType = 'Anonymous' | 'LDAP';

/**
 * Defines values for PhoenixAuthenticationType.
 * Possible values include: 'Anonymous', 'UsernameAndPassword', 'WindowsAzureHDInsightService'
 * @readonly
 * @enum {string}
 */
export type PhoenixAuthenticationType = 'Anonymous' | 'UsernameAndPassword' | 'WindowsAzureHDInsightService';

/**
 * Defines values for ImpalaAuthenticationType.
 * Possible values include: 'Anonymous', 'SASLUsername', 'UsernameAndPassword'
 * @readonly
 * @enum {string}
 */
export type ImpalaAuthenticationType = 'Anonymous' | 'SASLUsername' | 'UsernameAndPassword';

/**
 * Defines values for HiveServerType.
 * Possible values include: 'HiveServer1', 'HiveServer2', 'HiveThriftServer'
 * @readonly
 * @enum {string}
 */
export type HiveServerType = 'HiveServer1' | 'HiveServer2' | 'HiveThriftServer';

/**
 * Defines values for HiveThriftTransportProtocol.
 * Possible values include: 'Binary', 'SASL', 'HTTP '
 * @readonly
 * @enum {string}
 */
export type HiveThriftTransportProtocol = 'Binary' | 'SASL' | 'HTTP ';

/**
 * Defines values for HiveAuthenticationType.
 * Possible values include: 'Anonymous', 'Username', 'UsernameAndPassword',
 * 'WindowsAzureHDInsightService'
 * @readonly
 * @enum {string}
 */
export type HiveAuthenticationType = 'Anonymous' | 'Username' | 'UsernameAndPassword' | 'WindowsAzureHDInsightService';

/**
 * Defines values for HBaseAuthenticationType.
 * Possible values include: 'Anonymous', 'Basic'
 * @readonly
 * @enum {string}
 */
export type HBaseAuthenticationType = 'Anonymous' | 'Basic';

/**
 * Defines values for GoogleBigQueryAuthenticationType.
 * Possible values include: 'ServiceAuthentication', 'UserAuthentication'
 * @readonly
 * @enum {string}
 */
export type GoogleBigQueryAuthenticationType = 'ServiceAuthentication' | 'UserAuthentication';

/**
 * Defines values for SapHanaAuthenticationType.
 * Possible values include: 'Basic', 'Windows'
 * @readonly
 * @enum {string}
 */
export type SapHanaAuthenticationType = 'Basic' | 'Windows';

/**
 * Defines values for SftpAuthenticationType.
 * Possible values include: 'Basic', 'SshPublicKey'
 * @readonly
 * @enum {string}
 */
export type SftpAuthenticationType = 'Basic' | 'SshPublicKey';

/**
 * Defines values for FtpAuthenticationType.
 * Possible values include: 'Basic', 'Anonymous'
 * @readonly
 * @enum {string}
 */
export type FtpAuthenticationType = 'Basic' | 'Anonymous';

/**
 * Defines values for HttpAuthenticationType.
 * Possible values include: 'Basic', 'Anonymous', 'Digest', 'Windows', 'ClientCertificate'
 * @readonly
 * @enum {string}
 */
export type HttpAuthenticationType = 'Basic' | 'Anonymous' | 'Digest' | 'Windows' | 'ClientCertificate';

/**
 * Defines values for RestServiceAuthenticationType.
 * Possible values include: 'Anonymous', 'Basic', 'AadServicePrincipal', 'ManagedServiceIdentity'
 * @readonly
 * @enum {string}
 */
export type RestServiceAuthenticationType = 'Anonymous' | 'Basic' | 'AadServicePrincipal' | 'ManagedServiceIdentity';

/**
 * Defines values for MongoDbAuthenticationType.
 * Possible values include: 'Basic', 'Anonymous'
 * @readonly
 * @enum {string}
 */
export type MongoDbAuthenticationType = 'Basic' | 'Anonymous';

/**
 * Defines values for ODataAuthenticationType.
 * Possible values include: 'Basic', 'Anonymous', 'Windows', 'AadServicePrincipal',
 * 'ManagedServiceIdentity'
 * @readonly
 * @enum {string}
 */
export type ODataAuthenticationType = 'Basic' | 'Anonymous' | 'Windows' | 'AadServicePrincipal' | 'ManagedServiceIdentity';

/**
 * Defines values for ODataAadServicePrincipalCredentialType.
 * Possible values include: 'ServicePrincipalKey', 'ServicePrincipalCert'
 * @readonly
 * @enum {string}
 */
export type ODataAadServicePrincipalCredentialType = 'ServicePrincipalKey' | 'ServicePrincipalCert';

/**
 * Defines values for TeradataAuthenticationType.
 * Possible values include: 'Basic', 'Windows'
 * @readonly
 * @enum {string}
 */
export type TeradataAuthenticationType = 'Basic' | 'Windows';

/**
 * Defines values for Db2AuthenticationType.
 * Possible values include: 'Basic'
 * @readonly
 * @enum {string}
 */
export type Db2AuthenticationType = 'Basic';

/**
 * Defines values for SybaseAuthenticationType.
 * Possible values include: 'Basic', 'Windows'
 * @readonly
 * @enum {string}
 */
export type SybaseAuthenticationType = 'Basic' | 'Windows';

/**
 * Defines values for DynamicsDeploymentType.
 * Possible values include: 'Online', 'OnPremisesWithIfd'
 * @readonly
 * @enum {string}
 */
export type DynamicsDeploymentType = 'Online' | 'OnPremisesWithIfd';

/**
 * Defines values for DynamicsAuthenticationType.
 * Possible values include: 'Office365', 'Ifd', 'AADServicePrincipal'
 * @readonly
 * @enum {string}
 */
export type DynamicsAuthenticationType = 'Office365' | 'Ifd' | 'AADServicePrincipal';

/**
 * Defines values for OrcCompressionCodec.
 * Possible values include: 'none', 'zlib', 'snappy'
 * @readonly
 * @enum {string}
 */
export type OrcCompressionCodec = 'none' | 'zlib' | 'snappy';

/**
 * Defines values for AvroCompressionCodec.
 * Possible values include: 'none', 'deflate', 'snappy', 'xz', 'bzip2'
 * @readonly
 * @enum {string}
 */
export type AvroCompressionCodec = 'none' | 'deflate' | 'snappy' | 'xz' | 'bzip2';

/**
 * Defines values for TumblingWindowFrequency.
 * Possible values include: 'Minute', 'Hour'
 * @readonly
 * @enum {string}
 */
export type TumblingWindowFrequency = 'Minute' | 'Hour';

/**
 * Defines values for BlobEventTypes.
 * Possible values include: 'Microsoft.Storage.BlobCreated', 'Microsoft.Storage.BlobDeleted'
 * @readonly
 * @enum {string}
 */
export type BlobEventTypes = 'Microsoft.Storage.BlobCreated' | 'Microsoft.Storage.BlobDeleted';

/**
 * Defines values for DayOfWeek.
 * Possible values include: 'Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday',
 * 'Saturday'
 * @readonly
 * @enum {string}
 */
export type DayOfWeek = 'Sunday' | 'Monday' | 'Tuesday' | 'Wednesday' | 'Thursday' | 'Friday' | 'Saturday';

/**
 * Defines values for DaysOfWeek.
 * Possible values include: 'Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday',
 * 'Saturday'
 * @readonly
 * @enum {string}
 */
export type DaysOfWeek = 'Sunday' | 'Monday' | 'Tuesday' | 'Wednesday' | 'Thursday' | 'Friday' | 'Saturday';

/**
 * Defines values for RecurrenceFrequency.
 * Possible values include: 'NotSpecified', 'Minute', 'Hour', 'Day', 'Week', 'Month', 'Year'
 * @readonly
 * @enum {string}
 */
export type RecurrenceFrequency = 'NotSpecified' | 'Minute' | 'Hour' | 'Day' | 'Week' | 'Month' | 'Year';

/**
 * Defines values for DataFlowComputeType.
 * Possible values include: 'General', 'MemoryOptimized', 'ComputeOptimized'
 * @readonly
 * @enum {string}
 */
export type DataFlowComputeType = 'General' | 'MemoryOptimized' | 'ComputeOptimized';

/**
 * Defines values for AzureFunctionActivityMethod.
 * Possible values include: 'GET', 'POST', 'PUT', 'DELETE', 'OPTIONS', 'HEAD', 'TRACE'
 * @readonly
 * @enum {string}
 */
export type AzureFunctionActivityMethod = 'GET' | 'POST' | 'PUT' | 'DELETE' | 'OPTIONS' | 'HEAD' | 'TRACE';

/**
 * Defines values for WebActivityMethod.
 * Possible values include: 'GET', 'POST', 'PUT', 'DELETE'
 * @readonly
 * @enum {string}
 */
export type WebActivityMethod = 'GET' | 'POST' | 'PUT' | 'DELETE';

/**
 * Defines values for OraclePartitionOption.
 * Possible values include: 'None', 'PhysicalPartitionsOfTable', 'DynamicRange'
 * @readonly
 * @enum {string}
 */
export type OraclePartitionOption = 'None' | 'PhysicalPartitionsOfTable' | 'DynamicRange';

/**
 * Defines values for SalesforceSourceReadBehavior.
 * Possible values include: 'Query', 'QueryAll'
 * @readonly
 * @enum {string}
 */
export type SalesforceSourceReadBehavior = 'Query' | 'QueryAll';

/**
 * Defines values for NetezzaPartitionOption.
 * Possible values include: 'None', 'DataSlice', 'DynamicRange'
 * @readonly
 * @enum {string}
 */
export type NetezzaPartitionOption = 'None' | 'DataSlice' | 'DynamicRange';

/**
 * Defines values for CassandraSourceReadConsistencyLevels.
 * Possible values include: 'ALL', 'EACH_QUORUM', 'QUORUM', 'LOCAL_QUORUM', 'ONE', 'TWO', 'THREE',
 * 'LOCAL_ONE', 'SERIAL', 'LOCAL_SERIAL'
 * @readonly
 * @enum {string}
 */
export type CassandraSourceReadConsistencyLevels = 'ALL' | 'EACH_QUORUM' | 'QUORUM' | 'LOCAL_QUORUM' | 'ONE' | 'TWO' | 'THREE' | 'LOCAL_ONE' | 'SERIAL' | 'LOCAL_SERIAL';

/**
 * Defines values for TeradataPartitionOption.
 * Possible values include: 'None', 'Hash', 'DynamicRange'
 * @readonly
 * @enum {string}
 */
export type TeradataPartitionOption = 'None' | 'Hash' | 'DynamicRange';

/**
 * Defines values for SqlPartitionOption.
 * Possible values include: 'None', 'PhysicalPartitionsOfTable', 'DynamicRange'
 * @readonly
 * @enum {string}
 */
export type SqlPartitionOption = 'None' | 'PhysicalPartitionsOfTable' | 'DynamicRange';

/**
 * Defines values for StoredProcedureParameterType.
 * Possible values include: 'String', 'Int', 'Int64', 'Decimal', 'Guid', 'Boolean', 'Date'
 * @readonly
 * @enum {string}
 */
export type StoredProcedureParameterType = 'String' | 'Int' | 'Int64' | 'Decimal' | 'Guid' | 'Boolean' | 'Date';

/**
 * Defines values for SapTablePartitionOption.
 * Possible values include: 'None', 'PartitionOnInt', 'PartitionOnCalendarYear',
 * 'PartitionOnCalendarMonth', 'PartitionOnCalendarDate', 'PartitionOnTime'
 * @readonly
 * @enum {string}
 */
export type SapTablePartitionOption = 'None' | 'PartitionOnInt' | 'PartitionOnCalendarYear' | 'PartitionOnCalendarMonth' | 'PartitionOnCalendarDate' | 'PartitionOnTime';

/**
 * Defines values for SapHanaPartitionOption.
 * Possible values include: 'None', 'PhysicalPartitionsOfTable', 'SapHanaDynamicRange'
 * @readonly
 * @enum {string}
 */
export type SapHanaPartitionOption = 'None' | 'PhysicalPartitionsOfTable' | 'SapHanaDynamicRange';

/**
 * Defines values for SsisPackageLocationType.
 * Possible values include: 'SSISDB', 'File', 'InlinePackage', 'PackageStore'
 * @readonly
 * @enum {string}
 */
export type SsisPackageLocationType = 'SSISDB' | 'File' | 'InlinePackage' | 'PackageStore';

/**
 * Defines values for HDInsightActivityDebugInfoOption.
 * Possible values include: 'None', 'Always', 'Failure'
 * @readonly
 * @enum {string}
 */
export type HDInsightActivityDebugInfoOption = 'None' | 'Always' | 'Failure';

/**
 * Defines values for SalesforceSinkWriteBehavior.
 * Possible values include: 'Insert', 'Upsert'
 * @readonly
 * @enum {string}
 */
export type SalesforceSinkWriteBehavior = 'Insert' | 'Upsert';

/**
 * Defines values for AzureSearchIndexWriteBehaviorType.
 * Possible values include: 'Merge', 'Upload'
 * @readonly
 * @enum {string}
 */
export type AzureSearchIndexWriteBehaviorType = 'Merge' | 'Upload';

/**
 * Defines values for PolybaseSettingsRejectType.
 * Possible values include: 'value', 'percentage'
 * @readonly
 * @enum {string}
 */
export type PolybaseSettingsRejectType = 'value' | 'percentage';

/**
 * Defines values for JsonWriteFilePattern.
 * Possible values include: 'setOfObjects', 'arrayOfObjects'
 * @readonly
 * @enum {string}
 */
export type JsonWriteFilePattern = 'setOfObjects' | 'arrayOfObjects';

/**
 * Defines values for SapCloudForCustomerSinkWriteBehavior.
 * Possible values include: 'Insert', 'Update'
 * @readonly
 * @enum {string}
 */
export type SapCloudForCustomerSinkWriteBehavior = 'Insert' | 'Update';

/**
 * Defines values for WebHookActivityMethod.
 * Possible values include: 'POST'
 * @readonly
 * @enum {string}
 */
export type WebHookActivityMethod = 'POST';

/**
 * Defines values for IntegrationRuntimeType.
 * Possible values include: 'Managed', 'SelfHosted'
 * @readonly
 * @enum {string}
 */
export type IntegrationRuntimeType = 'Managed' | 'SelfHosted';

/**
 * Defines values for SelfHostedIntegrationRuntimeNodeStatus.
 * Possible values include: 'NeedRegistration', 'Online', 'Limited', 'Offline', 'Upgrading',
 * 'Initializing', 'InitializeFailed'
 * @readonly
 * @enum {string}
 */
export type SelfHostedIntegrationRuntimeNodeStatus = 'NeedRegistration' | 'Online' | 'Limited' | 'Offline' | 'Upgrading' | 'Initializing' | 'InitializeFailed';

/**
 * Defines values for IntegrationRuntimeUpdateResult.
 * Possible values include: 'None', 'Succeed', 'Fail'
 * @readonly
 * @enum {string}
 */
export type IntegrationRuntimeUpdateResult = 'None' | 'Succeed' | 'Fail';

/**
 * Defines values for IntegrationRuntimeInternalChannelEncryptionMode.
 * Possible values include: 'NotSet', 'SslEncrypted', 'NotEncrypted'
 * @readonly
 * @enum {string}
 */
export type IntegrationRuntimeInternalChannelEncryptionMode = 'NotSet' | 'SslEncrypted' | 'NotEncrypted';

/**
 * Defines values for ManagedIntegrationRuntimeNodeStatus.
 * Possible values include: 'Starting', 'Available', 'Recycling', 'Unavailable'
 * @readonly
 * @enum {string}
 */
export type ManagedIntegrationRuntimeNodeStatus = 'Starting' | 'Available' | 'Recycling' | 'Unavailable';

/**
 * Defines values for IntegrationRuntimeEntityReferenceType.
 * Possible values include: 'IntegrationRuntimeReference', 'LinkedServiceReference'
 * @readonly
 * @enum {string}
 */
export type IntegrationRuntimeEntityReferenceType = 'IntegrationRuntimeReference' | 'LinkedServiceReference';

/**
 * Defines values for IntegrationRuntimeSsisCatalogPricingTier.
 * Possible values include: 'Basic', 'Standard', 'Premium', 'PremiumRS'
 * @readonly
 * @enum {string}
 */
export type IntegrationRuntimeSsisCatalogPricingTier = 'Basic' | 'Standard' | 'Premium' | 'PremiumRS';

/**
 * Defines values for IntegrationRuntimeLicenseType.
 * Possible values include: 'BasePrice', 'LicenseIncluded'
 * @readonly
 * @enum {string}
 */
export type IntegrationRuntimeLicenseType = 'BasePrice' | 'LicenseIncluded';

/**
 * Defines values for IntegrationRuntimeEdition.
 * Possible values include: 'Standard', 'Enterprise'
 * @readonly
 * @enum {string}
 */
export type IntegrationRuntimeEdition = 'Standard' | 'Enterprise';

/**
 * Defines values for SsisObjectMetadataType.
 * Possible values include: 'Folder', 'Project', 'Package', 'Environment'
 * @readonly
 * @enum {string}
 */
export type SsisObjectMetadataType = 'Folder' | 'Project' | 'Package' | 'Environment';

/**
 * Defines values for IntegrationRuntimeAuthKeyName.
 * Possible values include: 'authKey1', 'authKey2'
 * @readonly
 * @enum {string}
 */
export type IntegrationRuntimeAuthKeyName = 'authKey1' | 'authKey2';

/**
 * Contains response data for the list operation.
 */
export type OperationsListResponse = OperationListResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: OperationListResponse;
    };
};

/**
 * Contains response data for the listNext operation.
 */
export type OperationsListNextResponse = OperationListResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: OperationListResponse;
    };
};

/**
 * Contains response data for the list operation.
 */
export type FactoriesListResponse = FactoryListResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: FactoryListResponse;
    };
};

/**
 * Contains response data for the configureFactoryRepo operation.
 */
export type FactoriesConfigureFactoryRepoResponse = Factory & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: Factory;
    };
};

/**
 * Contains response data for the listByResourceGroup operation.
 */
export type FactoriesListByResourceGroupResponse = FactoryListResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: FactoryListResponse;
    };
};

/**
 * Contains response data for the createOrUpdate operation.
 */
export type FactoriesCreateOrUpdateResponse = Factory & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: Factory;
    };
};

/**
 * Contains response data for the update operation.
 */
export type FactoriesUpdateResponse = Factory & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: Factory;
    };
};

/**
 * Contains response data for the get operation.
 */
export type FactoriesGetResponse = Factory & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: Factory;
    };
};

/**
 * Contains response data for the getGitHubAccessToken operation.
 */
export type FactoriesGetGitHubAccessTokenResponse = GitHubAccessTokenResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: GitHubAccessTokenResponse;
    };
};

/**
 * Contains response data for the getDataPlaneAccess operation.
 */
export type FactoriesGetDataPlaneAccessResponse = AccessPolicyResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: AccessPolicyResponse;
    };
};

/**
 * Contains response data for the listNext operation.
 */
export type FactoriesListNextResponse = FactoryListResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: FactoryListResponse;
    };
};

/**
 * Contains response data for the listByResourceGroupNext operation.
 */
export type FactoriesListByResourceGroupNextResponse = FactoryListResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: FactoryListResponse;
    };
};

/**
 * Contains response data for the getFeatureValue operation.
 */
export type ExposureControlGetFeatureValueResponse = ExposureControlResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: ExposureControlResponse;
    };
};

/**
 * Contains response data for the getFeatureValueByFactory operation.
 */
export type ExposureControlGetFeatureValueByFactoryResponse = ExposureControlResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: ExposureControlResponse;
    };
};

/**
 * Contains response data for the listByFactory operation.
 */
export type IntegrationRuntimesListByFactoryResponse = IntegrationRuntimeListResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: IntegrationRuntimeListResponse;
    };
};

/**
 * Contains response data for the createOrUpdate operation.
 */
export type IntegrationRuntimesCreateOrUpdateResponse = IntegrationRuntimeResource & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: IntegrationRuntimeResource;
    };
};

/**
 * Contains response data for the get operation.
 */
export type IntegrationRuntimesGetResponse = IntegrationRuntimeResource & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: IntegrationRuntimeResource;
    };
};

/**
 * Contains response data for the update operation.
 */
export type IntegrationRuntimesUpdateResponse = IntegrationRuntimeResource & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: IntegrationRuntimeResource;
    };
};

/**
 * Contains response data for the getStatus operation.
 */
export type IntegrationRuntimesGetStatusResponse = IntegrationRuntimeStatusResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: IntegrationRuntimeStatusResponse;
    };
};

/**
 * Contains response data for the getConnectionInfo operation.
 */
export type IntegrationRuntimesGetConnectionInfoResponse = IntegrationRuntimeConnectionInfo & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: IntegrationRuntimeConnectionInfo;
    };
};

/**
 * Contains response data for the regenerateAuthKey operation.
 */
export type IntegrationRuntimesRegenerateAuthKeyResponse = IntegrationRuntimeAuthKeys & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: IntegrationRuntimeAuthKeys;
    };
};

/**
 * Contains response data for the listAuthKeys operation.
 */
export type IntegrationRuntimesListAuthKeysResponse = IntegrationRuntimeAuthKeys & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: IntegrationRuntimeAuthKeys;
    };
};

/**
 * Contains response data for the start operation.
 */
export type IntegrationRuntimesStartResponse = IntegrationRuntimeStatusResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: IntegrationRuntimeStatusResponse;
    };
};

/**
 * Contains response data for the getMonitoringData operation.
 */
export type IntegrationRuntimesGetMonitoringDataResponse = IntegrationRuntimeMonitoringData & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: IntegrationRuntimeMonitoringData;
    };
};

/**
 * Contains response data for the createLinkedIntegrationRuntime operation.
 */
export type IntegrationRuntimesCreateLinkedIntegrationRuntimeResponse = IntegrationRuntimeStatusResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: IntegrationRuntimeStatusResponse;
    };
};

/**
 * Contains response data for the beginStart operation.
 */
export type IntegrationRuntimesBeginStartResponse = IntegrationRuntimeStatusResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: IntegrationRuntimeStatusResponse;
    };
};

/**
 * Contains response data for the listByFactoryNext operation.
 */
export type IntegrationRuntimesListByFactoryNextResponse = IntegrationRuntimeListResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: IntegrationRuntimeListResponse;
    };
};

/**
 * Contains response data for the refresh operation.
 */
export type IntegrationRuntimeObjectMetadataRefreshResponse = SsisObjectMetadataStatusResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: SsisObjectMetadataStatusResponse;
    };
};

/**
 * Contains response data for the get operation.
 */
export type IntegrationRuntimeObjectMetadataGetResponse = SsisObjectMetadataListResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: SsisObjectMetadataListResponse;
    };
};

/**
 * Contains response data for the beginRefresh operation.
 */
export type IntegrationRuntimeObjectMetadataBeginRefreshResponse = SsisObjectMetadataStatusResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: SsisObjectMetadataStatusResponse;
    };
};

/**
 * Contains response data for the get operation.
 */
export type IntegrationRuntimeNodesGetResponse = SelfHostedIntegrationRuntimeNode & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: SelfHostedIntegrationRuntimeNode;
    };
};

/**
 * Contains response data for the update operation.
 */
export type IntegrationRuntimeNodesUpdateResponse = SelfHostedIntegrationRuntimeNode & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: SelfHostedIntegrationRuntimeNode;
    };
};

/**
 * Contains response data for the getIpAddress operation.
 */
export type IntegrationRuntimeNodesGetIpAddressResponse = IntegrationRuntimeNodeIpAddress & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: IntegrationRuntimeNodeIpAddress;
    };
};

/**
 * Contains response data for the listByFactory operation.
 */
export type LinkedServicesListByFactoryResponse = LinkedServiceListResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: LinkedServiceListResponse;
    };
};

/**
 * Contains response data for the createOrUpdate operation.
 */
export type LinkedServicesCreateOrUpdateResponse = LinkedServiceResource & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: LinkedServiceResource;
    };
};

/**
 * Contains response data for the get operation.
 */
export type LinkedServicesGetResponse = LinkedServiceResource & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: LinkedServiceResource;
    };
};

/**
 * Contains response data for the listByFactoryNext operation.
 */
export type LinkedServicesListByFactoryNextResponse = LinkedServiceListResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: LinkedServiceListResponse;
    };
};

/**
 * Contains response data for the listByFactory operation.
 */
export type DatasetsListByFactoryResponse = DatasetListResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: DatasetListResponse;
    };
};

/**
 * Contains response data for the createOrUpdate operation.
 */
export type DatasetsCreateOrUpdateResponse = DatasetResource & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: DatasetResource;
    };
};

/**
 * Contains response data for the get operation.
 */
export type DatasetsGetResponse = DatasetResource & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: DatasetResource;
    };
};

/**
 * Contains response data for the listByFactoryNext operation.
 */
export type DatasetsListByFactoryNextResponse = DatasetListResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: DatasetListResponse;
    };
};

/**
 * Contains response data for the listByFactory operation.
 */
export type PipelinesListByFactoryResponse = PipelineListResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: PipelineListResponse;
    };
};

/**
 * Contains response data for the createOrUpdate operation.
 */
export type PipelinesCreateOrUpdateResponse = PipelineResource & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: PipelineResource;
    };
};

/**
 * Contains response data for the get operation.
 */
export type PipelinesGetResponse = PipelineResource & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: PipelineResource;
    };
};

/**
 * Contains response data for the createRun operation.
 */
export type PipelinesCreateRunResponse = CreateRunResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: CreateRunResponse;
    };
};

/**
 * Contains response data for the listByFactoryNext operation.
 */
export type PipelinesListByFactoryNextResponse = PipelineListResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: PipelineListResponse;
    };
};

/**
 * Contains response data for the queryByFactory operation.
 */
export type PipelineRunsQueryByFactoryResponse = PipelineRunsQueryResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: PipelineRunsQueryResponse;
    };
};

/**
 * Contains response data for the get operation.
 */
export type PipelineRunsGetResponse = PipelineRun & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: PipelineRun;
    };
};

/**
 * Contains response data for the queryByPipelineRun operation.
 */
export type ActivityRunsQueryByPipelineRunResponse = ActivityRunsQueryResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: ActivityRunsQueryResponse;
    };
};

/**
 * Contains response data for the listByFactory operation.
 */
export type TriggersListByFactoryResponse = TriggerListResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: TriggerListResponse;
    };
};

/**
 * Contains response data for the queryByFactory operation.
 */
export type TriggersQueryByFactoryResponse = TriggerQueryResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: TriggerQueryResponse;
    };
};

/**
 * Contains response data for the createOrUpdate operation.
 */
export type TriggersCreateOrUpdateResponse = TriggerResource & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: TriggerResource;
    };
};

/**
 * Contains response data for the get operation.
 */
export type TriggersGetResponse = TriggerResource & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: TriggerResource;
    };
};

/**
 * Contains response data for the subscribeToEvents operation.
 */
export type TriggersSubscribeToEventsResponse = TriggerSubscriptionOperationStatus & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: TriggerSubscriptionOperationStatus;
    };
};

/**
 * Contains response data for the getEventSubscriptionStatus operation.
 */
export type TriggersGetEventSubscriptionStatusResponse = TriggerSubscriptionOperationStatus & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: TriggerSubscriptionOperationStatus;
    };
};

/**
 * Contains response data for the unsubscribeFromEvents operation.
 */
export type TriggersUnsubscribeFromEventsResponse = TriggerSubscriptionOperationStatus & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: TriggerSubscriptionOperationStatus;
    };
};

/**
 * Contains response data for the beginSubscribeToEvents operation.
 */
export type TriggersBeginSubscribeToEventsResponse = TriggerSubscriptionOperationStatus & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: TriggerSubscriptionOperationStatus;
    };
};

/**
 * Contains response data for the beginUnsubscribeFromEvents operation.
 */
export type TriggersBeginUnsubscribeFromEventsResponse = TriggerSubscriptionOperationStatus & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: TriggerSubscriptionOperationStatus;
    };
};

/**
 * Contains response data for the listByFactoryNext operation.
 */
export type TriggersListByFactoryNextResponse = TriggerListResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: TriggerListResponse;
    };
};

/**
 * Contains response data for the queryByFactory operation.
 */
export type TriggerRunsQueryByFactoryResponse = TriggerRunsQueryResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: TriggerRunsQueryResponse;
    };
};

/**
 * Contains response data for the createOrUpdate operation.
 */
export type DataFlowsCreateOrUpdateResponse = DataFlowResource & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: DataFlowResource;
    };
};

/**
 * Contains response data for the get operation.
 */
export type DataFlowsGetResponse = DataFlowResource & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: DataFlowResource;
    };
};

/**
 * Contains response data for the listByFactory operation.
 */
export type DataFlowsListByFactoryResponse = DataFlowListResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: DataFlowListResponse;
    };
};

/**
 * Contains response data for the listByFactoryNext operation.
 */
export type DataFlowsListByFactoryNextResponse = DataFlowListResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: DataFlowListResponse;
    };
};

/**
 * Contains response data for the create operation.
 */
export type DataFlowDebugSessionCreateResponse = CreateDataFlowDebugSessionResponse & DataFlowDebugSessionCreateHeaders & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The parsed HTTP response headers.
       */
      parsedHeaders: DataFlowDebugSessionCreateHeaders;

      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: CreateDataFlowDebugSessionResponse;
    };
};

/**
 * Contains response data for the queryByFactory operation.
 */
export type DataFlowDebugSessionQueryByFactoryResponse = QueryDataFlowDebugSessionsResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: QueryDataFlowDebugSessionsResponse;
    };
};

/**
 * Contains response data for the addDataFlow operation.
 */
export type DataFlowDebugSessionAddDataFlowResponse = AddDataFlowToDebugSessionResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: AddDataFlowToDebugSessionResponse;
    };
};

/**
 * Contains response data for the executeCommand operation.
 */
export type DataFlowDebugSessionExecuteCommandResponse = DataFlowDebugCommandResponse & DataFlowDebugSessionExecuteCommandHeaders & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The parsed HTTP response headers.
       */
      parsedHeaders: DataFlowDebugSessionExecuteCommandHeaders;

      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: DataFlowDebugCommandResponse;
    };
};

/**
 * Contains response data for the queryByFactoryNext operation.
 */
export type DataFlowDebugSessionQueryByFactoryNextResponse = QueryDataFlowDebugSessionsResponse & {
  /**
   * The underlying HTTP response.
   */
  _response: msRest.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: QueryDataFlowDebugSessionsResponse;
    };
};
