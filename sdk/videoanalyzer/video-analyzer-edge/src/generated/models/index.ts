/*
 * Copyright (c) Microsoft Corporation.
 * Licensed under the MIT License.
 *
 * Code generated by Microsoft (R) AutoRest Code Generator.
 * Changes may cause incorrect behavior and will be lost if the code is regenerated.
 */

import * as coreClient from "@azure/core-client";

export type SourceNodeBaseUnion =
  | SourceNodeBase
  | RtspSource
  | IotHubMessageSource;
export type ProcessorNodeBaseUnion =
  | ProcessorNodeBase
  | MotionDetectionProcessor
  | ObjectTrackingProcessor
  | LineCrossingProcessor
  | ExtensionProcessorBaseUnion
  | SignalGateProcessor
  | CognitiveServicesVisionProcessor;
export type SinkNodeBaseUnion =
  | SinkNodeBase
  | IotHubMessageSink
  | FileSink
  | VideoSink;
export type EndpointBaseUnion = EndpointBase | UnsecuredEndpoint | TlsEndpoint;
export type CredentialsBaseUnion =
  | CredentialsBase
  | UsernamePasswordCredentials
  | HttpHeaderCredentials
  | SymmetricKeyCredentials;
export type CertificateSourceUnion = CertificateSource | PemCertificateList;
export type NamedLineBaseUnion = NamedLineBase | NamedLineString;
export type ImageFormatPropertiesUnion =
  | ImageFormatProperties
  | ImageFormatRaw
  | ImageFormatJpeg
  | ImageFormatBmp
  | ImageFormatPng;
export type NamedPolygonBaseUnion = NamedPolygonBase | NamedPolygonString;
export type SpatialAnalysisOperationBaseUnion =
  | SpatialAnalysisOperationBase
  | SpatialAnalysisCustomOperation
  | SpatialAnalysisTypedOperationBaseUnion;
export type ExtensionProcessorBaseUnion =
  | ExtensionProcessorBase
  | GrpcExtension
  | HttpExtension;
export type SpatialAnalysisTypedOperationBaseUnion =
  | SpatialAnalysisTypedOperationBase
  | SpatialAnalysisPersonCountOperation
  | SpatialAnalysisPersonZoneCrossingOperation
  | SpatialAnalysisPersonDistanceOperation
  | SpatialAnalysisPersonLineCrossingOperation;

/** Live Pipeline represents an unique instance of a pipeline topology which is used for real-time content ingestion and analysis. */
export interface LivePipeline {
  /** Live pipeline unique identifier. */
  name: string;
  /** Read-only system metadata associated with this object. */
  systemData?: SystemData;
  /** Live pipeline properties. */
  properties?: LivePipelineProperties;
}

/** Read-only system metadata associated with a resource. */
export interface SystemData {
  /** Date and time when this resource was first created. Value is represented in UTC according to the ISO8601 date format. */
  createdAt?: Date;
  /** Date and time when this resource was last modified. Value is represented in UTC according to the ISO8601 date format. */
  lastModifiedAt?: Date;
}

/** Live pipeline properties. */
export interface LivePipelineProperties {
  /** An optional description of the live pipeline. */
  description?: string;
  /** The reference to an existing pipeline topology defined for real-time content processing. When activated, this live pipeline will process content according to the pipeline topology definition. */
  topologyName?: string;
  /** List of the instance level parameter values for the user-defined topology parameters. A pipeline can only define or override parameters values for parameters which have been declared in the referenced topology. Topology parameters without a default value must be defined. Topology parameters with a default value can be optionally be overridden. */
  parameters?: ParameterDefinition[];
  /** Current pipeline state (read-only). */
  state?: LivePipelineState;
}

/** Defines the parameter value of an specific pipeline topology parameter. See pipeline topology parameters for more information. */
export interface ParameterDefinition {
  /** Name of the parameter declared in the pipeline topology. */
  name: string;
  /** Parameter value to be applied on this specific live pipeline. */
  value?: string;
}

/** A collection of live pipelines. */
export interface LivePipelineCollection {
  /** List of live pipelines. */
  value?: LivePipeline[];
  /** A continuation token to be used in subsequent calls when enumerating through the collection. This is returned when the collection results won't fit in a single response. */
  continuationToken?: string;
}

/** A collection of pipeline topologies. */
export interface PipelineTopologyCollection {
  /** List of pipeline topologies. */
  value?: PipelineTopology[];
  /** A continuation token to be used in subsequent calls when enumerating through the collection. This is returned when the collection results won't fit in a single response. */
  continuationToken?: string;
}

/**
 * Pipeline topology describes the processing steps to be applied when processing media for a particular outcome. The topology should be defined according to the scenario to be achieved and can be reused across many pipeline instances which share the same processing characteristics. For instance, a pipeline topology which acquires data from a RTSP camera, process it with an specific AI model and stored the data on the cloud can be reused across many different cameras, as long as the same processing should be applied across all the cameras. Individual instance properties can be defined through the use of user-defined parameters, which allow for a topology to be parameterized, thus allowing individual pipelines to refer to different values, such as individual cameras RTSP endpoints and credentials. Overall a topology is composed of the following:
 *
 *   - Parameters: list of user defined parameters that can be references across the topology nodes.
 *   - Sources: list of one or more data sources nodes such as an RTSP source which allows for media to be ingested from cameras.
 *   - Processors: list of nodes which perform data analysis or transformations.
 *   -Sinks: list of one or more data sinks which allow for data to be stored or exported to other destinations.
 */
export interface PipelineTopology {
  /** Pipeline topology unique identifier. */
  name: string;
  /** Read-only system metadata associated with this object. */
  systemData?: SystemData;
  /** Pipeline topology properties. */
  properties?: PipelineTopologyProperties;
}

/** Pipeline topology properties. */
export interface PipelineTopologyProperties {
  /** An optional description of the pipeline topology. It is recommended that the expected use of the topology to be described here. */
  description?: string;
  /** List of the topology parameter declarations. Parameters declared here can be referenced throughout the topology nodes through the use of "${PARAMETER_NAME}" string pattern. Parameters can have optional default values and can later be defined in individual instances of the pipeline. */
  parameters?: ParameterDeclaration[];
  /** List of the topology source nodes. Source nodes enable external data to be ingested by the pipeline. */
  sources?: SourceNodeBaseUnion[];
  /** List of the topology processor nodes. Processor nodes enable pipeline data to be analyzed, processed or transformed. */
  processors?: ProcessorNodeBaseUnion[];
  /** List of the topology sink nodes. Sink nodes allow pipeline data to be stored or exported. */
  sinks?: SinkNodeBaseUnion[];
}

/** Single topology parameter declaration. Declared parameters can and must be referenced throughout the topology and can optionally have default values to be used when they are not defined in the pipeline instances. */
export interface ParameterDeclaration {
  /** Name of the parameter. */
  name: string;
  /** Type of the parameter. */
  type: ParameterType;
  /** Description of the parameter. */
  description?: string;
  /** The default value for the parameter to be used if the live pipeline does not specify a value. */
  default?: string;
}

/** Base class for topology source nodes. */
export interface SourceNodeBase {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type":
    | "#Microsoft.VideoAnalyzer.RtspSource"
    | "#Microsoft.VideoAnalyzer.IotHubMessageSource";
  /** Node name. Must be unique within the topology. */
  name: string;
}

/** Base class for topology processor nodes. */
export interface ProcessorNodeBase {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type":
    | "#Microsoft.VideoAnalyzer.MotionDetectionProcessor"
    | "#Microsoft.VideoAnalyzer.ObjectTrackingProcessor"
    | "#Microsoft.VideoAnalyzer.LineCrossingProcessor"
    | "#Microsoft.VideoAnalyzer.ExtensionProcessorBase"
    | "#Microsoft.VideoAnalyzer.GrpcExtension"
    | "#Microsoft.VideoAnalyzer.HttpExtension"
    | "#Microsoft.VideoAnalyzer.SignalGateProcessor"
    | "#Microsoft.VideoAnalyzer.CognitiveServicesVisionProcessor";
  /** Node name. Must be unique within the topology. */
  name: string;
  /** An array of upstream node references within the topology to be used as inputs for this node. */
  inputs: NodeInput[];
}

/** Describes an input signal to be used on a pipeline node. */
export interface NodeInput {
  /** The name of the upstream node in the pipeline which output is used as input of the current node. */
  nodeName: string;
  /** Allows for the selection of specific data streams (eg. video only) from another node. */
  outputSelectors?: OutputSelector[];
}

/** Allows for the selection of particular streams from another node. */
export interface OutputSelector {
  /** The property of the data stream to be used as the selection criteria. */
  property?: OutputSelectorProperty;
  /** The operator to compare properties by. */
  operator?: OutputSelectorOperator;
  /** Value to compare against. */
  value?: string;
}

/** Base class for topology sink nodes. */
export interface SinkNodeBase {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type":
    | "#Microsoft.VideoAnalyzer.IotHubMessageSink"
    | "#Microsoft.VideoAnalyzer.FileSink"
    | "#Microsoft.VideoAnalyzer.VideoSink";
  /** Node name. Must be unique within the topology. */
  name: string;
  /** An array of upstream node references within the topology to be used as inputs for this node. */
  inputs: NodeInput[];
}

/** Base class for endpoints. */
export interface EndpointBase {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type":
    | "#Microsoft.VideoAnalyzer.UnsecuredEndpoint"
    | "#Microsoft.VideoAnalyzer.TlsEndpoint";
  /** Credentials to be presented to the endpoint. */
  credentials?: CredentialsBaseUnion;
  /** The endpoint URL for Video Analyzer to connect to. */
  url: string;
}

/** Base class for credential objects. */
export interface CredentialsBase {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type":
    | "#Microsoft.VideoAnalyzer.UsernamePasswordCredentials"
    | "#Microsoft.VideoAnalyzer.HttpHeaderCredentials"
    | "#Microsoft.VideoAnalyzer.SymmetricKeyCredentials";
}

/** Base class for certificate sources. */
export interface CertificateSource {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.PemCertificateList";
}

/** Options for controlling the validation of TLS endpoints. */
export interface TlsValidationOptions {
  /** When set to 'true' causes the certificate subject name validation to be skipped. Default is 'false'. */
  ignoreHostname?: string;
  /** When set to 'true' causes the certificate chain trust validation to be skipped. Default is 'false'. */
  ignoreSignature?: string;
}

/** Options for changing video publishing behavior on the video sink and output video. */
export interface VideoPublishingOptions {
  /** When set to 'true' the video will publish preview images. Default is 'false'. */
  enableVideoPreviewImage?: string;
}

/** Optional video properties to be used in case a new video resource needs to be created on the service. These will not take effect if the video already exists. */
export interface VideoCreationProperties {
  /** Optional video title provided by the user. Value can be up to 256 characters long. */
  title?: string;
  /** Optional video description provided by the user. Value can be up to 2048 characters long. */
  description?: string;
  /** Video segment length indicates the length of individual video files (segments) which are persisted to storage. Smaller segments provide lower archive playback latency but generate larger volume of storage transactions. Larger segments reduce the amount of storage transactions while increasing the archive playback latency. Value must be specified in ISO8601 duration format (i.e. "PT30S" equals 30 seconds) and can vary between 30 seconds to 5 minutes, in 30 seconds increments. Changing this value after the video is initially created can lead to errors when uploading media to the archive. Default value is 30 seconds. */
  segmentLength?: string;
  /** Video retention period indicates how long the video is kept in storage, and must be a multiple of 1 day. For example, if this is set to 30 days, then content older than 30 days will be deleted. */
  retentionPeriod?: string;
}

/** Base class for named lines. */
export interface NamedLineBase {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.NamedLineString";
  /** Line name. Must be unique within the node. */
  name: string;
}

/** Image transformations and formatting options to be applied to the video frame(s). */
export interface ImageProperties {
  /** Image scaling mode. */
  scale?: ImageScale;
  /** Base class for image formatting properties. */
  format?: ImageFormatPropertiesUnion;
}

/** Image scaling mode. */
export interface ImageScale {
  /** Describes the image scaling mode to be applied. Default mode is 'pad'. */
  mode?: ImageScaleMode;
  /** The desired output image width. */
  width?: string;
  /** The desired output image height. */
  height?: string;
}

/** Base class for image formatting properties. */
export interface ImageFormatProperties {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type":
    | "#Microsoft.VideoAnalyzer.ImageFormatRaw"
    | "#Microsoft.VideoAnalyzer.ImageFormatJpeg"
    | "#Microsoft.VideoAnalyzer.ImageFormatBmp"
    | "#Microsoft.VideoAnalyzer.ImageFormatPng";
}

/** Defines how often media is submitted to the extension plugin. */
export interface SamplingOptions {
  /** When set to 'true', prevents frames without upstream inference data to be sent to the extension plugin. This is useful to limit the frames sent to the extension to pre-analyzed frames only. For example, when used downstream from a motion detector, this can enable for only frames in which motion has been detected to be further analyzed. */
  skipSamplesWithoutAnnotation?: string;
  /** Maximum rate of samples submitted to the extension. This prevents an extension plugin to be overloaded with data. */
  maximumSamplesPerSecond?: string;
}

/** Describes how media is transferred to the extension plugin. */
export interface GrpcExtensionDataTransfer {
  /** The share memory buffer for sample transfers, in mebibytes. It can only be used with the 'SharedMemory' transfer mode. */
  sharedMemorySizeMiB?: string;
  /** Data transfer mode: embedded or sharedMemory. */
  mode: GrpcExtensionDataTransferMode;
}

/** Describes the named polygon. */
export interface NamedPolygonBase {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.NamedPolygonString";
  /** Polygon name. Must be unique within the node. */
  name: string;
}

/** Base class for Azure Cognitive Services Spatial Analysis operations. */
export interface SpatialAnalysisOperationBase {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type":
    | "#Microsoft.VideoAnalyzer.SpatialAnalysisCustomOperation"
    | "SpatialAnalysisTypedOperationBase"
    | "#Microsoft.VideoAnalyzer.SpatialAnalysisPersonCountOperation"
    | "#Microsoft.VideoAnalyzer.SpatialAnalysisPersonZoneCrossingOperation"
    | "#Microsoft.VideoAnalyzer.SpatialAnalysisPersonDistanceOperation"
    | "#Microsoft.VideoAnalyzer.SpatialAnalysisPersonLineCrossingOperation";
}

/** Defines the Azure Cognitive Services Spatial Analysis operation eventing configuration. */
export interface SpatialAnalysisOperationEventBase {
  /** The event threshold. */
  threshold?: string;
  /** The operation focus type. */
  focus?: SpatialAnalysisOperationFocus;
}

export interface SpatialAnalysisPersonCountZoneEvents {
  /** The named zone. */
  zone: NamedPolygonBaseUnion;
  /** The event configuration. */
  events?: SpatialAnalysisPersonCountEvent[];
}

export interface SpatialAnalysisPersonZoneCrossingZoneEvents {
  /** The named zone. */
  zone: NamedPolygonBaseUnion;
  /** The event configuration. */
  events?: SpatialAnalysisPersonZoneCrossingEvent[];
}

export interface SpatialAnalysisPersonDistanceZoneEvents {
  /** The named zone. */
  zone: NamedPolygonBaseUnion;
  /** The event configuration. */
  events?: SpatialAnalysisPersonDistanceEvent[];
}

export interface SpatialAnalysisPersonLineCrossingLineEvents {
  /** The named line. */
  line: NamedLineBaseUnion;
  /** The event configuration. */
  events?: SpatialAnalysisPersonLineCrossingEvent[];
}

/** The Video Analyzer edge module can act as a transparent gateway for video, enabling IoT devices to send video to the cloud from behind a firewall. A remote device adapter should be created for each such IoT device. Communication between the cloud and IoT device would then flow via the Video Analyzer edge module. */
export interface RemoteDeviceAdapter {
  /** The unique identifier for the remote device adapter. */
  name: string;
  /** Read-only system metadata associated with this object. */
  systemData?: SystemData;
  /** Properties of the remote device adapter. */
  properties?: RemoteDeviceAdapterProperties;
}

/** Remote device adapter properties. */
export interface RemoteDeviceAdapterProperties {
  /** An optional description for the remote device adapter. */
  description?: string;
  /** The IoT device to which this remote device will connect. */
  target: RemoteDeviceAdapterTarget;
  /** Information that enables communication between the IoT Hub and the IoT device - allowing this edge module to act as a transparent gateway between the two. */
  iotHubDeviceConnection: IotHubDeviceConnection;
}

/** Properties of the remote device adapter target. */
export interface RemoteDeviceAdapterTarget {
  /** Hostname or IP address of the remote device. */
  host: string;
}

/** Information that enables communication between the IoT Hub and the IoT device - allowing this edge module to act as a transparent gateway between the two. */
export interface IotHubDeviceConnection {
  /** The name of the IoT device configured and managed in IoT Hub. (case-sensitive) */
  deviceId: string;
  /** IoT device connection credentials. Currently IoT device symmetric key credentials are supported. */
  credentials?: CredentialsBaseUnion;
}

/** A list of remote device adapters. */
export interface RemoteDeviceAdapterCollection {
  /** An array of remote device adapters. */
  value?: RemoteDeviceAdapter[];
  /** A continuation token to use in subsequent calls to enumerate through the remote device adapter collection. This is used when the collection contains too many results to return in one response. */
  continuationToken?: string;
}

/** A list of ONVIF devices that were discovered in the same subnet as the IoT Edge device. */
export interface DiscoveredOnvifDeviceCollection {
  /** An array of ONVIF devices that have been discovered in the same subnet as the IoT Edge device. */
  value?: DiscoveredOnvifDevice[];
}

/** The discovered properties of the ONVIF device that are returned during the discovery. */
export interface DiscoveredOnvifDevice {
  /** The unique identifier of the ONVIF device that was discovered in the same subnet as the IoT Edge device. */
  serviceIdentifier?: string;
  /** The IP address of the ONVIF device that was discovered in the same subnet as the IoT Edge device. */
  remoteIPAddress?: string;
  /** An array of hostnames for the ONVIF discovered devices that are in the same subnet as the IoT Edge device. */
  scopes?: string[];
  /** An array of media profile endpoints that the ONVIF discovered device supports. */
  endpoints?: string[];
}

/** The ONVIF device properties. */
export interface OnvifDevice {
  /** The hostname of the ONVIF device. */
  hostname?: OnvifHostName;
  /** The system date and time of the ONVIF device. */
  systemDateTime?: OnvifSystemDateTime;
  /** The ONVIF device DNS properties. */
  dns?: OnvifDns;
  /** An array of of ONVIF media profiles supported by the ONVIF device. */
  mediaProfiles?: MediaProfile[];
}

/** The ONVIF device DNS properties. */
export interface OnvifHostName {
  /** Result value showing if the ONVIF device is configured to use DHCP. */
  fromDhcp?: boolean;
  /** The hostname of the ONVIF device. */
  hostname?: string;
}

/** The ONVIF device DNS properties. */
export interface OnvifSystemDateTime {
  /** An enum value determining whether the date time was configured using NTP or manual. */
  type?: OnvifSystemDateTimeType;
  /** The device datetime returned when calling the request. */
  time?: string;
  /** The timezone of the ONVIF device datetime. */
  timeZone?: string;
}

/** The ONVIF device DNS properties. */
export interface OnvifDns {
  /** Result value showing if the ONVIF device is configured to use DHCP. */
  fromDhcp?: boolean;
  /** An array of IPv4 address for the discovered ONVIF device. */
  ipv4Address?: string[];
  /** An array of IPv6 address for the discovered ONVIF device. */
  ipv6Address?: string[];
}

/** Class representing the ONVIF MediaProfiles. */
export interface MediaProfile {
  /** The name of the Media Profile. */
  name?: string;
  /** Object representing the URI that will be used to request for media streaming. */
  mediaUri?: Record<string, unknown>;
  /** The Video encoder configuration. */
  videoEncoderConfiguration?: VideoEncoderConfiguration;
}

/** Class representing the MPEG4 Configuration. */
export interface VideoEncoderConfiguration {
  /** The video codec used by the Media Profile. */
  encoding?: VideoEncoding;
  /** Relative value representing the quality of the video. */
  quality?: number;
  /** The Video Resolution. */
  resolution?: VideoResolution;
  /** The Video's rate control. */
  rateControl?: RateControl;
  /** The H264 Configuration. */
  h264?: H264Configuration;
  /** The H264 Configuration. */
  mpeg4?: Mpeg4Configuration;
}

/** The Video resolution. */
export interface VideoResolution {
  /** The number of columns of the Video image. */
  width?: number;
  /** The number of lines of the Video image. */
  height?: number;
}

/** Class  representing the video's rate control. */
export interface RateControl {
  /** the maximum output bitrate in kbps. */
  bitRateLimit?: number;
  /** Interval at which images are encoded and transmitted. */
  encodingInterval?: number;
  /** Maximum output framerate in fps. */
  frameRateLimit?: number;
  /** A value of true indicates that frame rate is a fixed value rather than an upper limit, and that the video encoder shall prioritize frame rate over all other adaptable configuration values such as bitrate. */
  guaranteedFrameRate?: boolean;
}

/** Class representing the H264 Configuration. */
export interface H264Configuration {
  /** Group of Video frames length. */
  govLength?: number;
  /** The H264 Profile */
  profile?: H264Profile;
}

/** Class representing the MPEG4 Configuration. */
export interface Mpeg4Configuration {
  /** Group of Video frames length. */
  govLength?: number;
  /** The MPEG4 Profile */
  profile?: Mpeg4Profile;
}

/** Object representing the URI that will be used to request for media streaming. */
export interface MediaUri {
  /** URI that can be used for media streaming. */
  uri?: string;
}

/** Base class for direct method calls. */
export interface MethodRequest {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  methodName: "undefined";
  /** Video Analyzer API version. */
  apiVersion?: "1.1";
}

/** RTSP source allows for media from an RTSP camera or generic RTSP server to be ingested into a live pipeline. */
export type RtspSource = SourceNodeBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.RtspSource";
  /** Network transport utilized by the RTSP and RTP exchange: TCP or HTTP. When using TCP, the RTP packets are interleaved on the TCP RTSP connection. When using HTTP, the RTSP messages are exchanged through long lived HTTP connections, and the RTP packages are interleaved in the HTTP connections alongside the RTSP messages. */
  transport?: RtspTransport;
  /** RTSP endpoint information for Video Analyzer to connect to. This contains the required information for Video Analyzer to connect to RTSP cameras and/or generic RTSP servers. */
  endpoint: EndpointBaseUnion;
};

/** IoT Hub Message source allows for the pipeline to consume messages from the IoT Edge Hub. Messages can be routed from other IoT modules via routes declared in the IoT Edge deployment manifest. */
export type IotHubMessageSource = SourceNodeBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.IotHubMessageSource";
  /** Name of the IoT Edge Hub input from which messages will be consumed. */
  hubInputName?: string;
};

/** Motion detection processor allows for motion detection on the video stream. It generates motion events whenever motion is present on the video. */
export type MotionDetectionProcessor = ProcessorNodeBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.MotionDetectionProcessor";
  /** Motion detection sensitivity: low, medium, high. */
  sensitivity?: MotionDetectionSensitivity;
  /** Indicates whether the processor should detect and output the regions within the video frame where motion was detected. Default is true. */
  outputMotionRegion?: boolean;
  /** Time window duration on which events are aggregated before being emitted. Value must be specified in ISO8601 duration format (i.e. "PT2S" equals 2 seconds). Use 0 seconds for no aggregation. Default is 1 second. */
  eventAggregationWindow?: string;
};

/** Object tracker processor allows for continuous tracking of one of more objects over a finite sequence of video frames. It must be used downstream of an object detector extension node, thus allowing for the extension to be configured to to perform inferences on sparse frames through the use of the 'maximumSamplesPerSecond' sampling property. The object tracker node will then track the detected objects over the frames in which the detector is not invoked resulting on a smother tracking of detected objects across the continuum of video frames. The tracker will stop tracking objects which are not subsequently detected by the upstream detector on the subsequent detections. */
export type ObjectTrackingProcessor = ProcessorNodeBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.ObjectTrackingProcessor";
  /** Object tracker accuracy: low, medium, high. Higher accuracy leads to higher CPU consumption in average. */
  accuracy?: ObjectTrackingAccuracy;
};

/** Line crossing processor allows for the detection of tracked objects moving across one or more predefined lines. It must be downstream of an object tracker of downstream on an AI extension node that generates sequenceId for objects which are tracked across different frames of the video. Inference events are generated every time objects crosses from one side of the line to another. */
export type LineCrossingProcessor = ProcessorNodeBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.LineCrossingProcessor";
  /** An array of lines used to compute line crossing events. */
  lines: NamedLineBaseUnion[];
};

/** Base class for pipeline extension processors. Pipeline extensions allow for custom media analysis and processing to be plugged into the Video Analyzer pipeline. */
export type ExtensionProcessorBase = ProcessorNodeBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type":
    | "#Microsoft.VideoAnalyzer.ExtensionProcessorBase"
    | "#Microsoft.VideoAnalyzer.GrpcExtension"
    | "#Microsoft.VideoAnalyzer.HttpExtension";
  /** Endpoint details of the pipeline extension plugin. */
  endpoint: EndpointBaseUnion;
  /** Image transformations and formatting options to be applied to the video frame(s) prior submission to the pipeline extension plugin. */
  image: ImageProperties;
  /** Media sampling parameters that define how often media is submitted to the extension plugin. */
  samplingOptions?: SamplingOptions;
};

/** A signal gate determines when to block (gate) incoming media, and when to allow it through. It gathers input events over the activationEvaluationWindow, and determines whether to open or close the gate. See https://aka.ms/ava-signalgate for more information. */
export type SignalGateProcessor = ProcessorNodeBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.SignalGateProcessor";
  /** The period of time over which the gate gathers input events before evaluating them. */
  activationEvaluationWindow?: string;
  /** Signal offset once the gate is activated (can be negative). It determines the how much farther behind of after the signal will be let through based on the activation time. A negative offset indicates that data prior the activation time must be included on the signal that is let through, once the gate is activated. When used upstream of a file or video sink, this allows for scenarios such as recording buffered media prior an event, such as: record video 5 seconds prior motions is detected. */
  activationSignalOffset?: string;
  /** The minimum period for which the gate remains open in the absence of subsequent triggers (events). When used upstream of a file or video sink, it determines the minimum length of the recorded video clip. */
  minimumActivationTime?: string;
  /** The maximum period for which the gate remains open in the presence of subsequent triggers (events). When used upstream of a file or video sink, it determines the maximum length of the recorded video clip. */
  maximumActivationTime?: string;
};

/** A processor that allows the pipeline topology to send video frames to a Cognitive Services Vision extension. Inference results are relayed to downstream nodes. */
export type CognitiveServicesVisionProcessor = ProcessorNodeBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.CognitiveServicesVisionProcessor";
  /** Endpoint to which this processor should connect. */
  endpoint: EndpointBaseUnion;
  /** Describes the parameters of the image that is sent as input to the endpoint. */
  image?: ImageProperties;
  /** Describes the sampling options to be applied when forwarding samples to the extension. */
  samplingOptions?: SamplingOptions;
  /** Describes the Spatial Analysis operation to be used in the Cognitive Services Vision processor. */
  operation: SpatialAnalysisOperationBaseUnion;
};

/** IoT Hub Message sink allows for pipeline messages to published into the IoT Edge Hub. Published messages can then be delivered to the cloud and other modules via routes declared in the IoT Edge deployment manifest. */
export type IotHubMessageSink = SinkNodeBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.IotHubMessageSink";
  /** Name of the Iot Edge Hub output to which the messages will be published. */
  hubOutputName: string;
};

/** File sink allows for video and audio content to be recorded on the file system on the edge device. */
export type FileSink = SinkNodeBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.FileSink";
  /** Absolute directory path where media files will be stored. */
  baseDirectoryPath: string;
  /** File name pattern for creating new files when performing event based recording. The pattern must include at least one system variable. */
  fileNamePattern: string;
  /** Maximum amount of disk space that can be used for storing files from this sink. Once this limit is reached, the oldest files from this sink will be automatically deleted. */
  maximumSizeMiB: string;
};

/** Video sink allows for video and audio to be recorded to the Video Analyzer service. The recorded video can be played from anywhere and further managed from the cloud. Due to security reasons, a given Video Analyzer edge module instance can only record content to new video entries, or existing video entries previously recorded by the same module. Any attempt to record content to an existing video which has not been created by the same module instance will result in failure to record. */
export type VideoSink = SinkNodeBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.VideoSink";
  /** Name of a new or existing Video Analyzer video resource used for the media recording. */
  videoName: string;
  /** Optional video properties to be used in case a new video resource needs to be created on the service. */
  videoCreationProperties?: VideoCreationProperties;
  /** Optional video publishing options to be used for changing publishing behavior of the output video. */
  videoPublishingOptions?: VideoPublishingOptions;
  /** Path to a local file system directory for caching of temporary media files. This will also be used to store content which cannot be immediately uploaded to Azure due to Internet connectivity issues. */
  localMediaCachePath: string;
  /** Maximum amount of disk space that can be used for caching of temporary media files. Once this limit is reached, the oldest segments of the media archive will be continuously deleted in order to make space for new media, thus leading to gaps in the cloud recorded content. */
  localMediaCacheMaximumSizeMiB: string;
};

/** Unsecured endpoint describes an endpoint that the pipeline can connect to over clear transport (no encryption in transit). */
export type UnsecuredEndpoint = EndpointBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.UnsecuredEndpoint";
};

/** TLS endpoint describes an endpoint that the pipeline can connect to over TLS transport (data is encrypted in transit). */
export type TlsEndpoint = EndpointBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.TlsEndpoint";
  /** List of trusted certificate authorities when authenticating a TLS connection. A null list designates that Azure Video Analyzer's list of trusted authorities should be used. */
  trustedCertificates?: CertificateSourceUnion;
  /** Validation options to use when authenticating a TLS connection. By default, strict validation is used. */
  validationOptions?: TlsValidationOptions;
};

/** Username and password credentials. */
export type UsernamePasswordCredentials = CredentialsBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.UsernamePasswordCredentials";
  /** Username to be presented as part of the credentials. */
  username: string;
  /** Password to be presented as part of the credentials. It is recommended that this value is parameterized as a secret string in order to prevent this value to be returned as part of the resource on API requests. */
  password: string;
};

/** HTTP header credentials. */
export type HttpHeaderCredentials = CredentialsBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.HttpHeaderCredentials";
  /** HTTP header name. */
  headerName: string;
  /** HTTP header value. It is recommended that this value is parameterized as a secret string in order to prevent this value to be returned as part of the resource on API requests. */
  headerValue: string;
};

/** Symmetric key credential. */
export type SymmetricKeyCredentials = CredentialsBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.SymmetricKeyCredentials";
  /** Symmetric key credential. */
  key: string;
};

/** A list of PEM formatted certificates. */
export type PemCertificateList = CertificateSource & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.PemCertificateList";
  /** PEM formatted public certificates. One certificate per entry. */
  certificates: string[];
};

/** Describes a line configuration. */
export type NamedLineString = NamedLineBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.NamedLineString";
  /** Point coordinates for the line start and end, respectively. Example: '[[0.3, 0.2],[0.9, 0.8]]'. Each point is expressed as [LEFT, TOP] coordinate ratios ranging from 0.0 to 1.0, where [0,0] is the upper-left frame corner and [1, 1] is the bottom-right frame corner. */
  line: string;
};

/** Raw image formatting. */
export type ImageFormatRaw = ImageFormatProperties & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.ImageFormatRaw";
  /** Pixel format to be applied to the raw image. */
  pixelFormat: ImageFormatRawPixelFormat;
};

/** JPEG image encoding. */
export type ImageFormatJpeg = ImageFormatProperties & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.ImageFormatJpeg";
  /** Image quality value between 0 to 100 (best quality). */
  quality?: string;
};

/** BMP image encoding. */
export type ImageFormatBmp = ImageFormatProperties & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.ImageFormatBmp";
};

/** PNG image encoding. */
export type ImageFormatPng = ImageFormatProperties & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.ImageFormatPng";
};

/** Describes a closed polygon configuration. */
export type NamedPolygonString = NamedPolygonBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.NamedPolygonString";
  /** Point coordinates for the polygon. Example: '[[0.3, 0.2],[0.9, 0.8],[0.7, 0.6]]'. Each point is expressed as [LEFT, TOP] coordinate ratios ranging from 0.0 to 1.0, where [0,0] is the upper-left frame corner and [1, 1] is the bottom-right frame corner. */
  polygon: string;
};

/** Defines a Spatial Analysis custom operation. This requires the Azure Cognitive Services Spatial analysis module to be deployed alongside the Video Analyzer module, please see https://aka.ms/ava-spatial-analysis for more information. */
export type SpatialAnalysisCustomOperation = SpatialAnalysisOperationBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.SpatialAnalysisCustomOperation";
  /** Custom configuration to pass to the Azure Cognitive Services Spatial Analysis module. */
  extensionConfiguration: string;
};

/** Base class for Azure Cognitive Services Spatial Analysis typed operations. */
export type SpatialAnalysisTypedOperationBase = SpatialAnalysisOperationBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type":
    | "SpatialAnalysisTypedOperationBase"
    | "#Microsoft.VideoAnalyzer.SpatialAnalysisPersonCountOperation"
    | "#Microsoft.VideoAnalyzer.SpatialAnalysisPersonZoneCrossingOperation"
    | "#Microsoft.VideoAnalyzer.SpatialAnalysisPersonDistanceOperation"
    | "#Microsoft.VideoAnalyzer.SpatialAnalysisPersonLineCrossingOperation";
  /** If set to 'true', enables debugging mode for this operation. */
  debug?: string;
  /** Advanced calibration configuration. */
  calibrationConfiguration?: string;
  /** Advanced camera configuration. */
  cameraConfiguration?: string;
  /** Advanced camera calibrator configuration. */
  cameraCalibratorNodeConfiguration?: string;
  /** Advanced detector node configuration. */
  detectorNodeConfiguration?: string;
  /** Advanced tracker node configuration. */
  trackerNodeConfiguration?: string;
  /** If set to 'true', enables face mask detection for this operation. */
  enableFaceMaskClassifier?: string;
};

/** Defines a Spatial Analysis person count operation eventing configuration. */
export type SpatialAnalysisPersonCountEvent = SpatialAnalysisOperationEventBase & {
  /** The event trigger type. */
  trigger?: SpatialAnalysisPersonCountEventTrigger;
  /** The event or interval output frequency. */
  outputFrequency?: string;
};

/** Defines a Spatial Analysis person crossing zone operation eventing configuration. */
export type SpatialAnalysisPersonZoneCrossingEvent = SpatialAnalysisOperationEventBase & {
  /** The event type. */
  eventType?: SpatialAnalysisPersonZoneCrossingEventType;
};

/** Defines a Spatial Analysis person distance operation eventing configuration. */
export type SpatialAnalysisPersonDistanceEvent = SpatialAnalysisOperationEventBase & {
  /** The event trigger type. */
  trigger?: SpatialAnalysisPersonDistanceEventTrigger;
  /** The event or interval output frequency. */
  outputFrequency?: string;
  /** The minimum distance threshold */
  minimumDistanceThreshold?: string;
  /** The maximum distance threshold */
  maximumDistanceThreshold?: string;
};

/** Defines a Spatial Analysis person line crossing operation eventing configuration. */
export type SpatialAnalysisPersonLineCrossingEvent = SpatialAnalysisOperationEventBase & {};

/** GRPC extension processor allows pipeline extension plugins to be connected to the pipeline through over a gRPC channel. Extension plugins must act as an gRPC server. Please see https://aka.ms/ava-extension-grpc for details. */
export type GrpcExtension = ExtensionProcessorBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.GrpcExtension";
  /** Specifies how media is transferred to the extension plugin. */
  dataTransfer: GrpcExtensionDataTransfer;
  /** An optional configuration string that is sent to the extension plugin. The configuration string is specific to each custom extension and it not understood neither validated by Video Analyzer. Please see https://aka.ms/ava-extension-grpc for details. */
  extensionConfiguration?: string;
};

/** HTTP extension processor allows pipeline extension plugins to be connected to the pipeline through over the HTTP protocol. Extension plugins must act as an HTTP server. Please see https://aka.ms/ava-extension-http for details. */
export type HttpExtension = ExtensionProcessorBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.HttpExtension";
};

/** Defines a Spatial Analysis person count operation. This requires the Azure Cognitive Services Spatial analysis module to be deployed alongside the Video Analyzer module, please see https://aka.ms/ava-spatial-analysis for more information. */
export type SpatialAnalysisPersonCountOperation = SpatialAnalysisTypedOperationBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.SpatialAnalysisPersonCountOperation";
  /** The list of zones and optional events. */
  zones: SpatialAnalysisPersonCountZoneEvents[];
};

/** Defines a Spatial Analysis person zone crossing operation. This requires the Azure Cognitive Services Spatial analysis module to be deployed alongside the Video Analyzer module, please see https://aka.ms/ava-spatial-analysis for more information. */
export type SpatialAnalysisPersonZoneCrossingOperation = SpatialAnalysisTypedOperationBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.SpatialAnalysisPersonZoneCrossingOperation";
  /** The list of zones with optional events. */
  zones: SpatialAnalysisPersonZoneCrossingZoneEvents[];
};

/** Defines a Spatial Analysis person distance operation. This requires the Azure Cognitive Services Spatial analysis module to be deployed alongside the Video Analyzer module, please see https://aka.ms/ava-spatial-analysis for more information. */
export type SpatialAnalysisPersonDistanceOperation = SpatialAnalysisTypedOperationBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.SpatialAnalysisPersonDistanceOperation";
  /** The list of zones with optional events. */
  zones: SpatialAnalysisPersonDistanceZoneEvents[];
};

/** Defines a Spatial Analysis person line crossing operation. This requires the Azure Cognitive Services Spatial analysis module to be deployed alongside the Video Analyzer module, please see https://aka.ms/ava-spatial-analysis for more information. */
export type SpatialAnalysisPersonLineCrossingOperation = SpatialAnalysisTypedOperationBase & {
  /** Polymorphic discriminator, which specifies the different types this object can be */
  "@type": "#Microsoft.VideoAnalyzer.SpatialAnalysisPersonLineCrossingOperation";
  /** The list of lines with optional events. */
  lines: SpatialAnalysisPersonLineCrossingLineEvents[];
};

/** Known values of {@link LivePipelineState} that the service accepts. */
export enum KnownLivePipelineState {
  /** The live pipeline is idle and not processing media. */
  Inactive = "inactive",
  /** The live pipeline is transitioning into the active state. */
  Activating = "activating",
  /** The live pipeline is active and able to process media. If your data source is not available, for instance, if your RTSP camera is powered off or unreachable, the pipeline will still be active and periodically retrying the connection. Your Azure subscription will be billed for the duration in which the live pipeline is in the active state. */
  Active = "active",
  /** The live pipeline is transitioning into the inactive state. */
  Deactivating = "deactivating"
}

/**
 * Defines values for LivePipelineState. \
 * {@link KnownLivePipelineState} can be used interchangeably with LivePipelineState,
 *  this enum contains the known values that the service supports.
 * ### Known values supported by the service
 * **inactive**: The live pipeline is idle and not processing media. \
 * **activating**: The live pipeline is transitioning into the active state. \
 * **active**: The live pipeline is active and able to process media. If your data source is not available, for instance, if your RTSP camera is powered off or unreachable, the pipeline will still be active and periodically retrying the connection. Your Azure subscription will be billed for the duration in which the live pipeline is in the active state. \
 * **deactivating**: The live pipeline is transitioning into the inactive state.
 */
export type LivePipelineState = string;

/** Known values of {@link ParameterType} that the service accepts. */
export enum KnownParameterType {
  /** The parameter's value is a string. */
  String = "string",
  /** The parameter's value is a string that holds sensitive information. */
  SecretString = "secretString",
  /** The parameter's value is a 32-bit signed integer. */
  Int = "int",
  /** The parameter's value is a 64-bit double-precision floating point. */
  Double = "double",
  /** The parameter's value is a boolean value that is either true or false. */
  Bool = "bool"
}

/**
 * Defines values for ParameterType. \
 * {@link KnownParameterType} can be used interchangeably with ParameterType,
 *  this enum contains the known values that the service supports.
 * ### Known values supported by the service
 * **string**: The parameter's value is a string. \
 * **secretString**: The parameter's value is a string that holds sensitive information. \
 * **int**: The parameter's value is a 32-bit signed integer. \
 * **double**: The parameter's value is a 64-bit double-precision floating point. \
 * **bool**: The parameter's value is a boolean value that is either true or false.
 */
export type ParameterType = string;

/** Known values of {@link OutputSelectorProperty} that the service accepts. */
export enum KnownOutputSelectorProperty {
  /** The stream's MIME type or subtype: audio, video or application */
  MediaType = "mediaType"
}

/**
 * Defines values for OutputSelectorProperty. \
 * {@link KnownOutputSelectorProperty} can be used interchangeably with OutputSelectorProperty,
 *  this enum contains the known values that the service supports.
 * ### Known values supported by the service
 * **mediaType**: The stream's MIME type or subtype: audio, video or application
 */
export type OutputSelectorProperty = string;

/** Known values of {@link OutputSelectorOperator} that the service accepts. */
export enum KnownOutputSelectorOperator {
  /** The property is of the type defined by value. */
  Is = "is",
  /** The property is not of the type defined by value. */
  IsNot = "isNot"
}

/**
 * Defines values for OutputSelectorOperator. \
 * {@link KnownOutputSelectorOperator} can be used interchangeably with OutputSelectorOperator,
 *  this enum contains the known values that the service supports.
 * ### Known values supported by the service
 * **is**: The property is of the type defined by value. \
 * **isNot**: The property is not of the type defined by value.
 */
export type OutputSelectorOperator = string;

/** Known values of {@link RtspTransport} that the service accepts. */
export enum KnownRtspTransport {
  /** HTTP transport. RTSP messages are exchanged over long running HTTP requests and RTP packets are interleaved within the HTTP channel. */
  Http = "http",
  /** TCP transport. RTSP is used directly over TCP and RTP packets are interleaved within the TCP channel. */
  Tcp = "tcp"
}

/**
 * Defines values for RtspTransport. \
 * {@link KnownRtspTransport} can be used interchangeably with RtspTransport,
 *  this enum contains the known values that the service supports.
 * ### Known values supported by the service
 * **http**: HTTP transport. RTSP messages are exchanged over long running HTTP requests and RTP packets are interleaved within the HTTP channel. \
 * **tcp**: TCP transport. RTSP is used directly over TCP and RTP packets are interleaved within the TCP channel.
 */
export type RtspTransport = string;

/** Known values of {@link MotionDetectionSensitivity} that the service accepts. */
export enum KnownMotionDetectionSensitivity {
  /** Low sensitivity. */
  Low = "low",
  /** Medium sensitivity. */
  Medium = "medium",
  /** High sensitivity. */
  High = "high"
}

/**
 * Defines values for MotionDetectionSensitivity. \
 * {@link KnownMotionDetectionSensitivity} can be used interchangeably with MotionDetectionSensitivity,
 *  this enum contains the known values that the service supports.
 * ### Known values supported by the service
 * **low**: Low sensitivity. \
 * **medium**: Medium sensitivity. \
 * **high**: High sensitivity.
 */
export type MotionDetectionSensitivity = string;

/** Known values of {@link ObjectTrackingAccuracy} that the service accepts. */
export enum KnownObjectTrackingAccuracy {
  /** Low accuracy. */
  Low = "low",
  /** Medium accuracy. */
  Medium = "medium",
  /** High accuracy. */
  High = "high"
}

/**
 * Defines values for ObjectTrackingAccuracy. \
 * {@link KnownObjectTrackingAccuracy} can be used interchangeably with ObjectTrackingAccuracy,
 *  this enum contains the known values that the service supports.
 * ### Known values supported by the service
 * **low**: Low accuracy. \
 * **medium**: Medium accuracy. \
 * **high**: High accuracy.
 */
export type ObjectTrackingAccuracy = string;

/** Known values of {@link ImageScaleMode} that the service accepts. */
export enum KnownImageScaleMode {
  /** Preserves the same aspect ratio as the input image. If only one image dimension is provided, the second dimension is calculated based on the input image aspect ratio. When 2 dimensions are provided, the image is resized to fit the most constraining dimension, considering the input image size and aspect ratio. */
  PreserveAspectRatio = "preserveAspectRatio",
  /** Pads the image with black horizontal stripes (letterbox) or black vertical stripes (pillar-box) so the image is resized to the specified dimensions while not altering the content aspect ratio. */
  Pad = "pad",
  /** Stretches the original image so it resized to the specified dimensions. */
  Stretch = "stretch"
}

/**
 * Defines values for ImageScaleMode. \
 * {@link KnownImageScaleMode} can be used interchangeably with ImageScaleMode,
 *  this enum contains the known values that the service supports.
 * ### Known values supported by the service
 * **preserveAspectRatio**: Preserves the same aspect ratio as the input image. If only one image dimension is provided, the second dimension is calculated based on the input image aspect ratio. When 2 dimensions are provided, the image is resized to fit the most constraining dimension, considering the input image size and aspect ratio. \
 * **pad**: Pads the image with black horizontal stripes (letterbox) or black vertical stripes (pillar-box) so the image is resized to the specified dimensions while not altering the content aspect ratio. \
 * **stretch**: Stretches the original image so it resized to the specified dimensions.
 */
export type ImageScaleMode = string;

/** Known values of {@link GrpcExtensionDataTransferMode} that the service accepts. */
export enum KnownGrpcExtensionDataTransferMode {
  /** Media samples are embedded into the gRPC messages. This mode is less efficient but it requires a simpler implementations and can be used with plugins which are not on the same node as the Video Analyzer module. */
  Embedded = "embedded",
  /** Media samples are made available through shared memory. This mode enables efficient data transfers but it requires that the extension plugin to be co-located on the same node and sharing the same shared memory space. */
  SharedMemory = "sharedMemory"
}

/**
 * Defines values for GrpcExtensionDataTransferMode. \
 * {@link KnownGrpcExtensionDataTransferMode} can be used interchangeably with GrpcExtensionDataTransferMode,
 *  this enum contains the known values that the service supports.
 * ### Known values supported by the service
 * **embedded**: Media samples are embedded into the gRPC messages. This mode is less efficient but it requires a simpler implementations and can be used with plugins which are not on the same node as the Video Analyzer module. \
 * **sharedMemory**: Media samples are made available through shared memory. This mode enables efficient data transfers but it requires that the extension plugin to be co-located on the same node and sharing the same shared memory space.
 */
export type GrpcExtensionDataTransferMode = string;

/** Known values of {@link ImageFormatRawPixelFormat} that the service accepts. */
export enum KnownImageFormatRawPixelFormat {
  /** Planar YUV 4:2:0, 12bpp, (1 Cr and Cb sample per 2x2 Y samples). */
  Yuv420P = "yuv420p",
  /** Packed RGB 5:6:5, 16bpp, (msb)   5R 6G 5B(lsb), big-endian. */
  Rgb565Be = "rgb565be",
  /** Packed RGB 5:6:5, 16bpp, (msb)   5R 6G 5B(lsb), little-endian. */
  Rgb565Le = "rgb565le",
  /** Packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), big-endian , X=unused/undefined. */
  Rgb555Be = "rgb555be",
  /** Packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), little-endian, X=unused/undefined. */
  Rgb555Le = "rgb555le",
  /** Packed RGB 8:8:8, 24bpp, RGBRGB. */
  Rgb24 = "rgb24",
  /** Packed RGB 8:8:8, 24bpp, BGRBGR. */
  Bgr24 = "bgr24",
  /** Packed ARGB 8:8:8:8, 32bpp, ARGBARGB. */
  Argb = "argb",
  /** Packed RGBA 8:8:8:8, 32bpp, RGBARGBA. */
  Rgba = "rgba",
  /** Packed ABGR 8:8:8:8, 32bpp, ABGRABGR. */
  Abgr = "abgr",
  /** Packed BGRA 8:8:8:8, 32bpp, BGRABGRA. */
  Bgra = "bgra"
}

/**
 * Defines values for ImageFormatRawPixelFormat. \
 * {@link KnownImageFormatRawPixelFormat} can be used interchangeably with ImageFormatRawPixelFormat,
 *  this enum contains the known values that the service supports.
 * ### Known values supported by the service
 * **yuv420p**: Planar YUV 4:2:0, 12bpp, (1 Cr and Cb sample per 2x2 Y samples). \
 * **rgb565be**: Packed RGB 5:6:5, 16bpp, (msb)   5R 6G 5B(lsb), big-endian. \
 * **rgb565le**: Packed RGB 5:6:5, 16bpp, (msb)   5R 6G 5B(lsb), little-endian. \
 * **rgb555be**: Packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), big-endian , X=unused\/undefined. \
 * **rgb555le**: Packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), little-endian, X=unused\/undefined. \
 * **rgb24**: Packed RGB 8:8:8, 24bpp, RGBRGB. \
 * **bgr24**: Packed RGB 8:8:8, 24bpp, BGRBGR. \
 * **argb**: Packed ARGB 8:8:8:8, 32bpp, ARGBARGB. \
 * **rgba**: Packed RGBA 8:8:8:8, 32bpp, RGBARGBA. \
 * **abgr**: Packed ABGR 8:8:8:8, 32bpp, ABGRABGR. \
 * **bgra**: Packed BGRA 8:8:8:8, 32bpp, BGRABGRA.
 */
export type ImageFormatRawPixelFormat = string;

/** Known values of {@link SpatialAnalysisOperationFocus} that the service accepts. */
export enum KnownSpatialAnalysisOperationFocus {
  /** The center of the object. */
  Center = "center",
  /** The bottom center of the object. */
  BottomCenter = "bottomCenter",
  /** The footprint. */
  Footprint = "footprint"
}

/**
 * Defines values for SpatialAnalysisOperationFocus. \
 * {@link KnownSpatialAnalysisOperationFocus} can be used interchangeably with SpatialAnalysisOperationFocus,
 *  this enum contains the known values that the service supports.
 * ### Known values supported by the service
 * **center**: The center of the object. \
 * **bottomCenter**: The bottom center of the object. \
 * **footprint**: The footprint.
 */
export type SpatialAnalysisOperationFocus = string;

/** Known values of {@link SpatialAnalysisPersonCountEventTrigger} that the service accepts. */
export enum KnownSpatialAnalysisPersonCountEventTrigger {
  /** Event trigger. */
  Event = "event",
  /** Interval trigger. */
  Interval = "interval"
}

/**
 * Defines values for SpatialAnalysisPersonCountEventTrigger. \
 * {@link KnownSpatialAnalysisPersonCountEventTrigger} can be used interchangeably with SpatialAnalysisPersonCountEventTrigger,
 *  this enum contains the known values that the service supports.
 * ### Known values supported by the service
 * **event**: Event trigger. \
 * **interval**: Interval trigger.
 */
export type SpatialAnalysisPersonCountEventTrigger = string;

/** Known values of {@link SpatialAnalysisPersonZoneCrossingEventType} that the service accepts. */
export enum KnownSpatialAnalysisPersonZoneCrossingEventType {
  /** Zone crossing event type. */
  ZoneCrossing = "zoneCrossing",
  /** Zone dwell time event type. */
  ZoneDwellTime = "zoneDwellTime"
}

/**
 * Defines values for SpatialAnalysisPersonZoneCrossingEventType. \
 * {@link KnownSpatialAnalysisPersonZoneCrossingEventType} can be used interchangeably with SpatialAnalysisPersonZoneCrossingEventType,
 *  this enum contains the known values that the service supports.
 * ### Known values supported by the service
 * **zoneCrossing**: Zone crossing event type. \
 * **zoneDwellTime**: Zone dwell time event type.
 */
export type SpatialAnalysisPersonZoneCrossingEventType = string;

/** Known values of {@link SpatialAnalysisPersonDistanceEventTrigger} that the service accepts. */
export enum KnownSpatialAnalysisPersonDistanceEventTrigger {
  /** Event trigger. */
  Event = "event",
  /** Interval trigger. */
  Interval = "interval"
}

/**
 * Defines values for SpatialAnalysisPersonDistanceEventTrigger. \
 * {@link KnownSpatialAnalysisPersonDistanceEventTrigger} can be used interchangeably with SpatialAnalysisPersonDistanceEventTrigger,
 *  this enum contains the known values that the service supports.
 * ### Known values supported by the service
 * **event**: Event trigger. \
 * **interval**: Interval trigger.
 */
export type SpatialAnalysisPersonDistanceEventTrigger = string;

/** Known values of {@link OnvifSystemDateTimeType} that the service accepts. */
export enum KnownOnvifSystemDateTimeType {
  Ntp = "Ntp",
  Manual = "Manual"
}

/**
 * Defines values for OnvifSystemDateTimeType. \
 * {@link KnownOnvifSystemDateTimeType} can be used interchangeably with OnvifSystemDateTimeType,
 *  this enum contains the known values that the service supports.
 * ### Known values supported by the service
 * **Ntp** \
 * **Manual**
 */
export type OnvifSystemDateTimeType = string;

/** Known values of {@link VideoEncoding} that the service accepts. */
export enum KnownVideoEncoding {
  /** The Media Profile uses JPEG encoding. */
  Jpeg = "JPEG",
  /** The Media Profile uses H264 encoding. */
  H264 = "H264",
  /** The Media Profile uses MPEG4 encoding. */
  Mpeg4 = "MPEG4"
}

/**
 * Defines values for VideoEncoding. \
 * {@link KnownVideoEncoding} can be used interchangeably with VideoEncoding,
 *  this enum contains the known values that the service supports.
 * ### Known values supported by the service
 * **JPEG**: The Media Profile uses JPEG encoding. \
 * **H264**: The Media Profile uses H264 encoding. \
 * **MPEG4**: The Media Profile uses MPEG4 encoding.
 */
export type VideoEncoding = string;

/** Known values of {@link H264Profile} that the service accepts. */
export enum KnownH264Profile {
  Baseline = "Baseline",
  Main = "Main",
  Extended = "Extended",
  High = "High"
}

/**
 * Defines values for H264Profile. \
 * {@link KnownH264Profile} can be used interchangeably with H264Profile,
 *  this enum contains the known values that the service supports.
 * ### Known values supported by the service
 * **Baseline** \
 * **Main** \
 * **Extended** \
 * **High**
 */
export type H264Profile = string;

/** Known values of {@link Mpeg4Profile} that the service accepts. */
export enum KnownMpeg4Profile {
  /** Simple Profile. */
  SP = "SP",
  /** Advanced Simple Profile. */
  ASP = "ASP"
}

/**
 * Defines values for Mpeg4Profile. \
 * {@link KnownMpeg4Profile} can be used interchangeably with Mpeg4Profile,
 *  this enum contains the known values that the service supports.
 * ### Known values supported by the service
 * **SP**: Simple Profile. \
 * **ASP**: Advanced Simple Profile.
 */
export type Mpeg4Profile = string;

/** Optional parameters. */
export interface GeneratedClientOptionalParams
  extends coreClient.ServiceClientOptions {
  /** Overrides client endpoint. */
  endpoint?: string;
}
