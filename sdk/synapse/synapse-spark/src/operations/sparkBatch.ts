/*
 * Copyright (c) Microsoft Corporation.
 * Licensed under the MIT License.
 *
 * Code generated by Microsoft (R) AutoRest Code Generator.
 * Changes may cause incorrect behavior and will be lost if the code is regenerated.
 */

import { tracingClient } from "../tracing";
import { SparkBatch } from "../operationsInterfaces";
import * as coreClient from "@azure/core-client";
import * as Mappers from "../models/mappers";
import * as Parameters from "../models/parameters";
import { SparkClient } from "../sparkClient";
import {
  SparkBatchGetSparkBatchJobsOptionalParams,
  SparkBatchGetSparkBatchJobsResponse,
  SparkBatchJobOptions,
  SparkBatchCreateSparkBatchJobOptionalParams,
  SparkBatchCreateSparkBatchJobResponse,
  SparkBatchGetSparkBatchJobOptionalParams,
  SparkBatchGetSparkBatchJobResponse,
  SparkBatchCancelSparkBatchJobOptionalParams
} from "../models";

/** Class containing SparkBatch operations. */
export class SparkBatchImpl implements SparkBatch {
  private readonly client: SparkClient;

  /**
   * Initialize a new instance of the class SparkBatch class.
   * @param client Reference to the service client
   */
  constructor(client: SparkClient) {
    this.client = client;
  }

  /**
   * List all spark batch jobs which are running under a particular spark pool.
   * @param options The options parameters.
   */
  async getSparkBatchJobs(
    options?: SparkBatchGetSparkBatchJobsOptionalParams
  ): Promise<SparkBatchGetSparkBatchJobsResponse> {
    return tracingClient.withSpan(
      "SparkClient.getSparkBatchJobs",
      options ?? {},
      async (options) => {
        return this.client.sendOperationRequest(
          { options },
          getSparkBatchJobsOperationSpec
        ) as Promise<SparkBatchGetSparkBatchJobsResponse>;
      }
    );
  }

  /**
   * Create new spark batch job.
   * @param sparkBatchJobOptions Livy compatible batch job request payload.
   * @param options The options parameters.
   */
  async createSparkBatchJob(
    sparkBatchJobOptions: SparkBatchJobOptions,
    options?: SparkBatchCreateSparkBatchJobOptionalParams
  ): Promise<SparkBatchCreateSparkBatchJobResponse> {
    return tracingClient.withSpan(
      "SparkClient.createSparkBatchJob",
      options ?? {},
      async (options) => {
        return this.client.sendOperationRequest(
          { sparkBatchJobOptions, options },
          createSparkBatchJobOperationSpec
        ) as Promise<SparkBatchCreateSparkBatchJobResponse>;
      }
    );
  }

  /**
   * Gets a single spark batch job.
   * @param batchId Identifier for the batch job.
   * @param options The options parameters.
   */
  async getSparkBatchJob(
    batchId: number,
    options?: SparkBatchGetSparkBatchJobOptionalParams
  ): Promise<SparkBatchGetSparkBatchJobResponse> {
    return tracingClient.withSpan(
      "SparkClient.getSparkBatchJob",
      options ?? {},
      async (options) => {
        return this.client.sendOperationRequest(
          { batchId, options },
          getSparkBatchJobOperationSpec
        ) as Promise<SparkBatchGetSparkBatchJobResponse>;
      }
    );
  }

  /**
   * Cancels a running spark batch job.
   * @param batchId Identifier for the batch job.
   * @param options The options parameters.
   */
  async cancelSparkBatchJob(
    batchId: number,
    options?: SparkBatchCancelSparkBatchJobOptionalParams
  ): Promise<void> {
    return tracingClient.withSpan(
      "SparkClient.cancelSparkBatchJob",
      options ?? {},
      async (options) => {
        return this.client.sendOperationRequest(
          { batchId, options },
          cancelSparkBatchJobOperationSpec
        ) as Promise<void>;
      }
    );
  }
}
// Operation Specifications
const serializer = coreClient.createSerializer(Mappers, /* isXml */ false);

const getSparkBatchJobsOperationSpec: coreClient.OperationSpec = {
  path: "/livyApi/versions/{livyApiVersion}/sparkPools/{sparkPoolName}/batches",
  httpMethod: "GET",
  responses: {
    200: {
      bodyMapper: Mappers.SparkBatchJobCollection
    }
  },
  queryParameters: [Parameters.fromParam, Parameters.size, Parameters.detailed],
  urlParameters: [
    Parameters.endpoint,
    Parameters.livyApiVersion,
    Parameters.sparkPoolName
  ],
  headerParameters: [Parameters.accept],
  serializer
};
const createSparkBatchJobOperationSpec: coreClient.OperationSpec = {
  path: "/livyApi/versions/{livyApiVersion}/sparkPools/{sparkPoolName}/batches",
  httpMethod: "POST",
  responses: {
    200: {
      bodyMapper: Mappers.SparkBatchJob
    }
  },
  requestBody: Parameters.sparkBatchJobOptions,
  queryParameters: [Parameters.detailed],
  urlParameters: [
    Parameters.endpoint,
    Parameters.livyApiVersion,
    Parameters.sparkPoolName
  ],
  headerParameters: [Parameters.accept, Parameters.contentType],
  mediaType: "json",
  serializer
};
const getSparkBatchJobOperationSpec: coreClient.OperationSpec = {
  path:
    "/livyApi/versions/{livyApiVersion}/sparkPools/{sparkPoolName}/batches/{batchId}",
  httpMethod: "GET",
  responses: {
    200: {
      bodyMapper: Mappers.SparkBatchJob
    }
  },
  queryParameters: [Parameters.detailed],
  urlParameters: [
    Parameters.endpoint,
    Parameters.livyApiVersion,
    Parameters.sparkPoolName,
    Parameters.batchId
  ],
  headerParameters: [Parameters.accept],
  serializer
};
const cancelSparkBatchJobOperationSpec: coreClient.OperationSpec = {
  path:
    "/livyApi/versions/{livyApiVersion}/sparkPools/{sparkPoolName}/batches/{batchId}",
  httpMethod: "DELETE",
  responses: { 200: {} },
  urlParameters: [
    Parameters.endpoint,
    Parameters.livyApiVersion,
    Parameters.sparkPoolName,
    Parameters.batchId
  ],
  serializer
};
