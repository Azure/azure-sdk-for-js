"use strict";
/*
 * Copyright (c) Microsoft Corporation.
 * Licensed under the MIT License.
 *
 * Code generated by Microsoft (R) AutoRest Code Generator.
 * Changes may cause incorrect behavior and will be lost if the code is regenerated.
 */
Object.defineProperty(exports, "__esModule", { value: true });
exports.SparkBatchImpl = void 0;
const tslib_1 = require("tslib");
const tracing_js_1 = require("../tracing.js");
const coreClient = tslib_1.__importStar(require("@azure/core-client"));
const Mappers = tslib_1.__importStar(require("../models/mappers.js"));
const Parameters = tslib_1.__importStar(require("../models/parameters.js"));
// Operation Specifications
const serializer = coreClient.createSerializer(Mappers, /* isXml */ false);
const getSparkBatchJobsOperationSpec = {
    path: "/livyApi/versions/{livyApiVersion}/sparkPools/{sparkPoolName}/batches",
    httpMethod: "GET",
    responses: {
        200: {
            bodyMapper: Mappers.SparkBatchJobCollection,
        },
    },
    queryParameters: [Parameters.fromParam, Parameters.size, Parameters.detailed],
    urlParameters: [Parameters.endpoint, Parameters.livyApiVersion, Parameters.sparkPoolName],
    headerParameters: [Parameters.accept],
    serializer,
};
const createSparkBatchJobOperationSpec = {
    path: "/livyApi/versions/{livyApiVersion}/sparkPools/{sparkPoolName}/batches",
    httpMethod: "POST",
    responses: {
        200: {
            bodyMapper: Mappers.SparkBatchJob,
        },
    },
    requestBody: Parameters.sparkBatchJobOptions,
    queryParameters: [Parameters.detailed],
    urlParameters: [Parameters.endpoint, Parameters.livyApiVersion, Parameters.sparkPoolName],
    headerParameters: [Parameters.accept, Parameters.contentType],
    mediaType: "json",
    serializer,
};
const getSparkBatchJobOperationSpec = {
    path: "/livyApi/versions/{livyApiVersion}/sparkPools/{sparkPoolName}/batches/{batchId}",
    httpMethod: "GET",
    responses: {
        200: {
            bodyMapper: Mappers.SparkBatchJob,
        },
    },
    queryParameters: [Parameters.detailed],
    urlParameters: [
        Parameters.endpoint,
        Parameters.livyApiVersion,
        Parameters.sparkPoolName,
        Parameters.batchId,
    ],
    headerParameters: [Parameters.accept],
    serializer,
};
const cancelSparkBatchJobOperationSpec = {
    path: "/livyApi/versions/{livyApiVersion}/sparkPools/{sparkPoolName}/batches/{batchId}",
    httpMethod: "DELETE",
    responses: { 200: {} },
    urlParameters: [
        Parameters.endpoint,
        Parameters.livyApiVersion,
        Parameters.sparkPoolName,
        Parameters.batchId,
    ],
    serializer,
};
/** Class containing SparkBatch operations. */
class SparkBatchImpl {
    /**
     * Initialize a new instance of the class SparkBatch class.
     * @param client - Reference to the service client
     */
    constructor(client) {
        this.client = client;
    }
    /**
     * List all spark batch jobs which are running under a particular spark pool.
     * @param options - The options parameters.
     */
    async getSparkBatchJobs(options) {
        return tracing_js_1.tracingClient.withSpan("SparkClient.getSparkBatchJobs", options !== null && options !== void 0 ? options : {}, async (updatedOptions) => {
            return this.client.sendOperationRequest({ updatedOptions }, getSparkBatchJobsOperationSpec);
        });
    }
    /**
     * Create new spark batch job.
     * @param sparkBatchJobOptions - Livy compatible batch job request payload.
     * @param options - The options parameters.
     */
    async createSparkBatchJob(sparkBatchJobOptions, options) {
        return tracing_js_1.tracingClient.withSpan("SparkClient.createSparkBatchJob", options !== null && options !== void 0 ? options : {}, async (updatedOptions) => {
            return this.client.sendOperationRequest({ sparkBatchJobOptions, updatedOptions }, createSparkBatchJobOperationSpec);
        });
    }
    /**
     * Gets a single spark batch job.
     * @param batchId - Identifier for the batch job.
     * @param options - The options parameters.
     */
    async getSparkBatchJob(batchId, options) {
        return tracing_js_1.tracingClient.withSpan("SparkClient.getSparkBatchJob", options !== null && options !== void 0 ? options : {}, async (updatedOptions) => {
            return this.client.sendOperationRequest({ batchId, updatedOptions }, getSparkBatchJobOperationSpec);
        });
    }
    /**
     * Cancels a running spark batch job.
     * @param batchId - Identifier for the batch job.
     * @param options - The options parameters.
     */
    async cancelSparkBatchJob(batchId, options) {
        return tracing_js_1.tracingClient.withSpan("SparkClient.cancelSparkBatchJob", options !== null && options !== void 0 ? options : {}, async (updatedOptions) => {
            return this.client.sendOperationRequest({ batchId, updatedOptions }, cancelSparkBatchJobOperationSpec);
        });
    }
}
exports.SparkBatchImpl = SparkBatchImpl;
//# sourceMappingURL=sparkBatch.js.map