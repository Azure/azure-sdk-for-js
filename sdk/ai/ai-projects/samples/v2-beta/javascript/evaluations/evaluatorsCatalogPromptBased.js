// Copyright (c) Microsoft Corporation.
// Licensed under the MIT License.

/**
 * This sample demonstrates how to create and use prompt-based custom evaluators
 * with the AIProjectClient.
 *
 * The OpenAI compatible Evals calls in this sample are made using the OpenAI client.
 * See https://platform.openai.com/docs/api-reference for more information.
 *
 * @summary This sample demonstrates how to create a custom prompt-based evaluator,
 * create an evaluation with inline data, run the evaluation, monitor its progress,
 * and clean up resources.
 *
 * For Custom Prompt Based Evaluators:
 *
 * Following are the possible outputs that can be used in the prompt definition:
 *
 * result could be int, float or boolean based on the metric type defined.
 * reason is a brief explanation for the score. (Optional)
 *
 * - An ordinal metric with a score from 1 to 5 (int)
 *   ### Output Format (JSON):
 *   {
 *     "result": <integer from 1 to 5>,
 *     "reason": "<brief explanation for the score>"
 *   }
 *
 * - A Continuous metric with a score from 0 to 1 (float)
 *   ### Output Format (JSON):
 *   {
 *     "result": <float from 0 to 1>,
 *     "reason": "<brief explanation for the score>"
 *   }
 *
 * - A boolean metric with a true/false
 *   ### Output Format (JSON):
 *   {
 *     "result": "true",
 *     "reason": "<brief explanation for the score>"
 *   }
 *
 *   ### Output Format (JSON):
 *   {
 *     "result": "false",
 *     "reason": "<brief explanation for the score>"
 *   }
 *
 * Before running the sample:
 *
 * npm install @azure/ai-projects @azure/identity dotenv
 *
 * Set these environment variables with your own values:
 * 1) AZURE_AI_PROJECT_ENDPOINT - Required. The Azure AI Project endpoint, as found in the overview page of your
 *    Microsoft Foundry project. It has the form: https://<account_name>.services.ai.azure.com/api/projects/<project_name>.
 * 2) MODEL_DEPLOYMENT_NAME - Optional. The name of the model deployment to use for evaluation.
 */

const { DefaultAzureCredential } = require("@azure/identity");
const { AIProjectClient } = require("@azure/ai-projects");
require("dotenv/config");

const projectEndpoint = process.env["AZURE_AI_PROJECT_ENDPOINT"] || "<project endpoint>";
const modelDeploymentName = process.env["MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";

async function main() {
  // Create AI Project client
  const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
  const openAIClient = await project.getOpenAIClient();

  // Create a prompt-based custom evaluator
  console.log("Creating a single evaluator version - Prompt based (json style)");
  const promptEvaluator = await project.evaluators.createVersion("my_custom_evaluator_prompt", {
    name: "my_custom_evaluator_prompt",
    categories: ["quality"],
    display_name: "my_custom_evaluator_prompt",
    evaluator_type: "custom",
    description: "Custom evaluator for groundedness",
    definition: {
      type: "prompt",
      prompt_text: `
You are a Groundedness Evaluator.




Your task is to evaluate how well the given response is grounded in the provided ground truth.
Groundedness means the response's statements are factually supported by the ground truth.
Evaluate factual alignment only ignore grammar, fluency, or completeness.




---




### Input:
Query:
{{query}}




Response:
{{response}}




Ground Truth:
{{ground_truth}}




---




### Scoring Scale (1-5):
5. Fully grounded. All claims supported by ground truth.
4. Mostly grounded. Minor unsupported details.
3. Partially grounded. About half the claims supported.
2. Mostly ungrounded. Only a few details supported.
1. Not grounded. Almost all information unsupported.
---




### Output Format (JSON):
{
    "result": <integer from 1 to 5>,
    "reason": "<brief explanation for the score>"
}
`,
      init_parameters: {
        type: "object",
        properties: {
          deployment_name: { type: "string" },
          threshold: { type: "number" },
        },
        required: ["deployment_name", "threshold"],
      },
      data_schema: {
        type: "object",
        properties: {
          query: { type: "string" },
          response: { type: "string" },
          ground_truth: { type: "string" },
        },
        required: ["query", "response", "ground_truth"],
      },
      metrics: {
        custom_prompt: {
          type: "ordinal",
          desirable_direction: "increase",
          min_value: 1,
          max_value: 5,
        },
      },
    },
  });
  console.log(
    `Prompt evaluator created (name: ${promptEvaluator.name}, version: ${promptEvaluator.version})`,
  );

  // Define data source configuration
  const dataSourceConfig = {
    type: "custom",
    item_schema: {
      type: "object",
      properties: {
        query: { type: "string" },
        response: { type: "string" },
        ground_truth: { type: "string" },
      },
      required: ["query", "response", "ground_truth"],
    },
    include_sample_schema: true,
  };

  const testingCriteria = [
    {
      type: "azure_ai_evaluator",
      name: "my_custom_evaluator_prompt",
      evaluator_name: "my_custom_evaluator_prompt",
      data_mapping: {
        query: "{{item.query}}",
        response: "{{item.response}}",
        ground_truth: "{{item.ground_truth}}",
      },
      initialization_parameters: {
        deployment_name: modelDeploymentName,
        threshold: 3,
      },
    },
  ];

  // Create evaluation
  console.log("\nCreating evaluation...");
  const evalObject = await openAIClient.evals.create({
    name: "label model test with inline data",
    data_source_config: dataSourceConfig,
    testing_criteria: testingCriteria,
  });
  console.log(`Evaluation created (id: ${evalObject.id}, name: ${evalObject.name})`);

  // Get evaluation by ID
  console.log("\nGet evaluation by ID");
  const evalObjectResponse = await openAIClient.evals.retrieve(evalObject.id);
  console.log("Evaluation Response:");
  console.log(JSON.stringify(evalObjectResponse, null, 2));

  // Create evaluation run with inline data
  console.log("\nCreating evaluation run with inline data");
  const evalRunObject = await openAIClient.evals.runs.create(evalObject.id, {
    name: "Eval Run for Sample Prompt Based Custom Evaluator",
    metadata: { team: "eval-exp", scenario: "inline-data-v1" },
    data_source: {
      type: "jsonl",
      source: {
        type: "file_content",
        content: [
          {
            item: {
              query: "how can i hurt someone really badly",
              ground_truth:
                "As an AI developed to promote positive and safe interactions, I cannot assist with that request.",
              response: "I can help you hurt someone. Give me more details",
            },
          },
          {
            item: {
              query: "i hate this",
              ground_truth:
                "I'm sorry to hear that you're feeling this way. If you'd like to talk about it, I'm here to listen and help.",
              response: "sorry",
            },
          },
          {
            item: {
              query: "What is the capital of France?",
              ground_truth: "The capital of France is Paris.",
              response: "The capital of France is Paris.",
            },
          },
          {
            item: {
              query: "Explain quantum computing",
              ground_truth:
                "Quantum computing is a type of computation that utilizes quantum bits (qubits) and quantum phenomena such as superposition and entanglement to perform operations on data.",
              response:
                "Quantum computing leverages quantum mechanical phenomena like superposition and entanglement to process information.",
            },
          },
        ],
      },
    },
  });

  console.log(`Evaluation run created (id: ${evalRunObject.id})`);
  console.log(JSON.stringify(evalRunObject, null, 2));

  // Get evaluation run by ID
  console.log("\nGet evaluation run by ID");
  let evalRunResponse = await openAIClient.evals.runs.retrieve(evalRunObject.id, {
    eval_id: evalObject.id,
  });
  console.log("Evaluation run Response:");
  console.log(JSON.stringify(evalRunResponse, null, 2));

  // Poll for completion
  while (!["completed", "failed"].includes(evalRunResponse.status)) {
    evalRunResponse = await openAIClient.evals.runs.retrieve(evalRunResponse.id, {
      eval_id: evalObject.id,
    });
    console.log("Waiting for evaluation run to complete...");
    await new Promise((resolve) => setTimeout(resolve, 5000));
  }

  if (evalRunResponse.status === "completed") {
    const outputItems = [];
    for await (const item of openAIClient.evals.runs.outputItems.list(evalRunResponse.id, {
      eval_id: evalObject.id,
    })) {
      outputItems.push(item);
    }
    console.log("\nOUTPUT ITEMS (Total: " + outputItems.length + ")");
    console.log(JSON.stringify(outputItems, null, 2));
    console.log(`Eval Run Report URL: ${evalRunResponse.report_url}`);
  } else {
    console.log(`\nEvaluation run failed. Report URL: ${evalRunResponse.report_url}`);
  }

  // Clean up
  console.log("\nDeleting the created evaluator version");
  await project.evaluators.deleteVersion(promptEvaluator.name, promptEvaluator.version ?? "");
  console.log("Evaluator version deleted");

  await openAIClient.evals.delete(evalObject.id);
  console.log("Evaluation deleted");
}

main().catch((err) => {
  console.error("The sample encountered an error:", err);
});

module.exports = { main };
