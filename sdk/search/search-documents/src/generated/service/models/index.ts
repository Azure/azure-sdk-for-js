/*
 * Copyright (c) Microsoft Corporation. All rights reserved.
 * Licensed under the MIT License. See License.txt in the project root for license information.
 *
 * Code generated by Microsoft (R) AutoRest Code Generator.
 * Changes may cause incorrect behavior and will be lost if the code is regenerated.
 */


import * as coreHttp from "@azure/core-http";

/**
 * Specifies some text and analysis components used to break that text into tokens.
 */
export interface AnalyzeRequest {
  /**
   * The text to break into tokens.
   */
  text: string;
  /**
   * The name of the analyzer to use to break the given text. If this parameter is not specified,
   * you must specify a tokenizer instead. The tokenizer and analyzer parameters are mutually
   * exclusive. KnownAnalyzerNames is an enum containing known values.
   */
  analyzer?: string;
  /**
   * The name of the tokenizer to use to break the given text. If this parameter is not specified,
   * you must specify an analyzer instead. The tokenizer and analyzer parameters are mutually
   * exclusive. KnownTokenizerNames is an enum containing known values.
   */
  tokenizer?: string;
  /**
   * An optional list of token filters to use when breaking the given text. This parameter can only
   * be set when using the tokenizer parameter.
   */
  tokenFilters?: string[];
  /**
   * An optional list of character filters to use when breaking the given text. This parameter can
   * only be set when using the tokenizer parameter.
   */
  charFilters?: string[];
}

/**
 * Information about a token returned by an analyzer.
 */
export interface TokenInfo {
  /**
   * The token returned by the analyzer.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly token: string;
  /**
   * The index of the first character of the token in the input text.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly startOffset: number;
  /**
   * The index of the last character of the token in the input text.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly endOffset: number;
  /**
   * The position of the token in the input text relative to other tokens. The first token in the
   * input text has position 0, the next has position 1, and so on. Depending on the analyzer used,
   * some tokens might have the same position, for example if they are synonyms of each other.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly position: number;
}

/**
 * The result of testing an analyzer on text.
 */
export interface AnalyzeResult {
  /**
   * The list of tokens returned by the analyzer specified in the request.
   */
  tokens: TokenInfo[];
}

/**
 * Contains the possible cases for Analyzer.
 */
export type AnalyzerUnion = Analyzer | CustomAnalyzer | PatternAnalyzer | StandardAnalyzer | StopAnalyzer;

/**
 * Abstract base class for analyzers.
 */
export interface Analyzer {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "Analyzer";
  /**
   * The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores,
   * can only start and end with alphanumeric characters, and is limited to 128 characters.
   */
  name: string;
}

/**
 * Allows you to take control over the process of converting text into indexable/searchable tokens.
 * It's a user-defined configuration consisting of a single predefined tokenizer and one or more
 * filters. The tokenizer is responsible for breaking text into tokens, and the filters for
 * modifying tokens emitted by the tokenizer.
 */
export interface CustomAnalyzer {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.CustomAnalyzer";
  /**
   * The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores,
   * can only start and end with alphanumeric characters, and is limited to 128 characters.
   */
  name: string;
  /**
   * The name of the tokenizer to use to divide continuous text into a sequence of tokens, such as
   * breaking a sentence into words. KnownTokenizerNames is an enum containing known values.
   */
  tokenizer: string;
  /**
   * A list of token filters used to filter out or modify the tokens generated by a tokenizer. For
   * example, you can specify a lowercase filter that converts all characters to lowercase. The
   * filters are run in the order in which they are listed.
   */
  tokenFilters?: string[];
  /**
   * A list of character filters used to prepare input text before it is processed by the
   * tokenizer. For instance, they can replace certain characters or symbols. The filters are run
   * in the order in which they are listed.
   */
  charFilters?: string[];
}

/**
 * Flexibly separates text into terms via a regular expression pattern. This analyzer is
 * implemented using Apache Lucene.
 */
export interface PatternAnalyzer {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.PatternAnalyzer";
  /**
   * The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores,
   * can only start and end with alphanumeric characters, and is limited to 128 characters.
   */
  name: string;
  /**
   * A value indicating whether terms should be lower-cased. Default is true. Default value: true.
   */
  lowerCaseTerms?: boolean;
  /**
   * A regular expression pattern to match token separators. Default is an expression that matches
   * one or more whitespace characters. Default value: '\W+'.
   */
  pattern?: string;
  /**
   * Regular expression flags.
   */
  flags?: string;
  /**
   * A list of stopwords.
   */
  stopwords?: string[];
}

/**
 * Standard Apache Lucene analyzer; Composed of the standard tokenizer, lowercase filter and stop
 * filter.
 */
export interface StandardAnalyzer {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.StandardAnalyzer";
  /**
   * The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores,
   * can only start and end with alphanumeric characters, and is limited to 128 characters.
   */
  name: string;
  /**
   * The maximum token length. Default is 255. Tokens longer than the maximum length are split. The
   * maximum token length that can be used is 300 characters. Default value: 255.
   */
  maxTokenLength?: number;
  /**
   * A list of stopwords.
   */
  stopwords?: string[];
}

/**
 * Divides text at non-letters; Applies the lowercase and stopword token filters. This analyzer is
 * implemented using Apache Lucene.
 */
export interface StopAnalyzer {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.StopAnalyzer";
  /**
   * The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores,
   * can only start and end with alphanumeric characters, and is limited to 128 characters.
   */
  name: string;
  /**
   * A list of stopwords.
   */
  stopwords?: string[];
}

/**
 * Contains the possible cases for Tokenizer.
 */
export type TokenizerUnion = Tokenizer | ClassicTokenizer | EdgeNGramTokenizer | KeywordTokenizer | KeywordTokenizerV2 | MicrosoftLanguageTokenizer | MicrosoftLanguageStemmingTokenizer | NGramTokenizer | PathHierarchyTokenizerV2 | PatternTokenizer | StandardTokenizer | StandardTokenizerV2 | UaxUrlEmailTokenizer;

/**
 * Abstract base class for tokenizers.
 */
export interface Tokenizer {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "Tokenizer";
  /**
   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
}

/**
 * Grammar-based tokenizer that is suitable for processing most European-language documents. This
 * tokenizer is implemented using Apache Lucene.
 */
export interface ClassicTokenizer {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.ClassicTokenizer";
  /**
   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The maximum token length. Default is 255. Tokens longer than the maximum length are split. The
   * maximum token length that can be used is 300 characters. Default value: 255.
   */
  maxTokenLength?: number;
}

/**
 * Tokenizes the input from an edge into n-grams of the given size(s). This tokenizer is
 * implemented using Apache Lucene.
 */
export interface EdgeNGramTokenizer {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.EdgeNGramTokenizer";
  /**
   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of
   * maxGram. Default value: 1.
   */
  minGram?: number;
  /**
   * The maximum n-gram length. Default is 2. Maximum is 300. Default value: 2.
   */
  maxGram?: number;
  /**
   * Character classes to keep in the tokens.
   */
  tokenChars?: TokenCharacterKind[];
}

/**
 * Emits the entire input as a single token. This tokenizer is implemented using Apache Lucene.
 */
export interface KeywordTokenizer {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.KeywordTokenizer";
  /**
   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The read buffer size in bytes. Default is 256. Default value: 256.
   */
  bufferSize?: number;
}

/**
 * Emits the entire input as a single token. This tokenizer is implemented using Apache Lucene.
 */
export interface KeywordTokenizerV2 {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.KeywordTokenizerV2";
  /**
   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The maximum token length. Default is 256. Tokens longer than the maximum length are split. The
   * maximum token length that can be used is 300 characters. Default value: 256.
   */
  maxTokenLength?: number;
}

/**
 * Divides text using language-specific rules.
 */
export interface MicrosoftLanguageTokenizer {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.MicrosoftLanguageTokenizer";
  /**
   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The maximum token length. Tokens longer than the maximum length are split. Maximum token
   * length that can be used is 300 characters. Tokens longer than 300 characters are first split
   * into tokens of length 300 and then each of those tokens is split based on the max token length
   * set. Default is 255. Default value: 255.
   */
  maxTokenLength?: number;
  /**
   * A value indicating how the tokenizer is used. Set to true if used as the search tokenizer, set
   * to false if used as the indexing tokenizer. Default is false. Default value: false.
   */
  isSearchTokenizer?: boolean;
  /**
   * The language to use. The default is English. Possible values include: 'bangla', 'bulgarian',
   * 'catalan', 'chineseSimplified', 'chineseTraditional', 'croatian', 'czech', 'danish', 'dutch',
   * 'english', 'french', 'german', 'greek', 'gujarati', 'hindi', 'icelandic', 'indonesian',
   * 'italian', 'japanese', 'kannada', 'korean', 'malay', 'malayalam', 'marathi',
   * 'norwegianBokmaal', 'polish', 'portuguese', 'portugueseBrazilian', 'punjabi', 'romanian',
   * 'russian', 'serbianCyrillic', 'serbianLatin', 'slovenian', 'spanish', 'swedish', 'tamil',
   * 'telugu', 'thai', 'ukrainian', 'urdu', 'vietnamese'
   */
  language?: MicrosoftTokenizerLanguage;
}

/**
 * Divides text using language-specific rules and reduces words to their base forms.
 */
export interface MicrosoftLanguageStemmingTokenizer {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.MicrosoftLanguageStemmingTokenizer";
  /**
   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The maximum token length. Tokens longer than the maximum length are split. Maximum token
   * length that can be used is 300 characters. Tokens longer than 300 characters are first split
   * into tokens of length 300 and then each of those tokens is split based on the max token length
   * set. Default is 255. Default value: 255.
   */
  maxTokenLength?: number;
  /**
   * A value indicating how the tokenizer is used. Set to true if used as the search tokenizer, set
   * to false if used as the indexing tokenizer. Default is false. Default value: false.
   */
  isSearchTokenizer?: boolean;
  /**
   * The language to use. The default is English. Possible values include: 'arabic', 'bangla',
   * 'bulgarian', 'catalan', 'croatian', 'czech', 'danish', 'dutch', 'english', 'estonian',
   * 'finnish', 'french', 'german', 'greek', 'gujarati', 'hebrew', 'hindi', 'hungarian',
   * 'icelandic', 'indonesian', 'italian', 'kannada', 'latvian', 'lithuanian', 'malay',
   * 'malayalam', 'marathi', 'norwegianBokmaal', 'polish', 'portuguese', 'portugueseBrazilian',
   * 'punjabi', 'romanian', 'russian', 'serbianCyrillic', 'serbianLatin', 'slovak', 'slovenian',
   * 'spanish', 'swedish', 'tamil', 'telugu', 'turkish', 'ukrainian', 'urdu'
   */
  language?: MicrosoftStemmingTokenizerLanguage;
}

/**
 * Tokenizes the input into n-grams of the given size(s). This tokenizer is implemented using
 * Apache Lucene.
 */
export interface NGramTokenizer {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.NGramTokenizer";
  /**
   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of
   * maxGram. Default value: 1.
   */
  minGram?: number;
  /**
   * The maximum n-gram length. Default is 2. Maximum is 300. Default value: 2.
   */
  maxGram?: number;
  /**
   * Character classes to keep in the tokens.
   */
  tokenChars?: TokenCharacterKind[];
}

/**
 * Tokenizer for path-like hierarchies. This tokenizer is implemented using Apache Lucene.
 */
export interface PathHierarchyTokenizerV2 {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.PathHierarchyTokenizerV2";
  /**
   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The delimiter character to use. Default is "/". Default value: '/'.
   */
  delimiter?: string;
  /**
   * A value that, if set, replaces the delimiter character. Default is "/". Default value: '/'.
   */
  replacement?: string;
  /**
   * The maximum token length. Default and maximum is 300. Default value: 300.
   */
  maxTokenLength?: number;
  /**
   * A value indicating whether to generate tokens in reverse order. Default is false. Default
   * value: false.
   */
  reverseTokenOrder?: boolean;
  /**
   * The number of initial tokens to skip. Default is 0. Default value: 0.
   */
  numberOfTokensToSkip?: number;
}

/**
 * Tokenizer that uses regex pattern matching to construct distinct tokens. This tokenizer is
 * implemented using Apache Lucene.
 */
export interface PatternTokenizer {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.PatternTokenizer";
  /**
   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * A regular expression pattern to match token separators. Default is an expression that matches
   * one or more whitespace characters. Default value: '\W+'.
   */
  pattern?: string;
  /**
   * Regular expression flags.
   */
  flags?: string;
  /**
   * The zero-based ordinal of the matching group in the regular expression pattern to extract into
   * tokens. Use -1 if you want to use the entire pattern to split the input into tokens,
   * irrespective of matching groups. Default is -1. Default value: -1.
   */
  group?: number;
}

/**
 * Breaks text following the Unicode Text Segmentation rules. This tokenizer is implemented using
 * Apache Lucene.
 */
export interface StandardTokenizer {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.StandardTokenizer";
  /**
   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The maximum token length. Default is 255. Tokens longer than the maximum length are split.
   * Default value: 255.
   */
  maxTokenLength?: number;
}

/**
 * Breaks text following the Unicode Text Segmentation rules. This tokenizer is implemented using
 * Apache Lucene.
 */
export interface StandardTokenizerV2 {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.StandardTokenizerV2";
  /**
   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The maximum token length. Default is 255. Tokens longer than the maximum length are split. The
   * maximum token length that can be used is 300 characters. Default value: 255.
   */
  maxTokenLength?: number;
}

/**
 * Tokenizes urls and emails as one token. This tokenizer is implemented using Apache Lucene.
 */
export interface UaxUrlEmailTokenizer {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.UaxUrlEmailTokenizer";
  /**
   * The name of the tokenizer. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The maximum token length. Default is 255. Tokens longer than the maximum length are split. The
   * maximum token length that can be used is 300 characters. Default value: 255.
   */
  maxTokenLength?: number;
}

/**
 * Contains the possible cases for TokenFilter.
 */
export type TokenFilterUnion = TokenFilter | AsciiFoldingTokenFilter | CjkBigramTokenFilter | CommonGramTokenFilter | DictionaryDecompounderTokenFilter | EdgeNGramTokenFilter | EdgeNGramTokenFilterV2 | ElisionTokenFilter | KeepTokenFilter | KeywordMarkerTokenFilter | LengthTokenFilter | LimitTokenFilter | NGramTokenFilter | NGramTokenFilterV2 | PatternCaptureTokenFilter | PatternReplaceTokenFilter | PhoneticTokenFilter | ShingleTokenFilter | SnowballTokenFilter | StemmerTokenFilter | StemmerOverrideTokenFilter | StopwordsTokenFilter | SynonymTokenFilter | TruncateTokenFilter | UniqueTokenFilter | WordDelimiterTokenFilter;

/**
 * Abstract base class for token filters.
 */
export interface TokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "TokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
}

/**
 * Converts alphabetic, numeric, and symbolic Unicode characters which are not in the first 127
 * ASCII characters (the "Basic Latin" Unicode block) into their ASCII equivalents, if such
 * equivalents exist. This token filter is implemented using Apache Lucene.
 */
export interface AsciiFoldingTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.AsciiFoldingTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * A value indicating whether the original token will be kept. Default is false. Default value:
   * false.
   */
  preserveOriginal?: boolean;
}

/**
 * Forms bigrams of CJK terms that are generated from StandardTokenizer. This token filter is
 * implemented using Apache Lucene.
 */
export interface CjkBigramTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.CjkBigramTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The scripts to ignore.
   */
  ignoreScripts?: CjkBigramTokenFilterScripts[];
  /**
   * A value indicating whether to output both unigrams and bigrams (if true), or just bigrams (if
   * false). Default is false. Default value: false.
   */
  outputUnigrams?: boolean;
}

/**
 * Construct bigrams for frequently occurring terms while indexing. Single terms are still indexed
 * too, with bigrams overlaid. This token filter is implemented using Apache Lucene.
 */
export interface CommonGramTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.CommonGramTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The set of common words.
   */
  commonWords: string[];
  /**
   * A value indicating whether common words matching will be case insensitive. Default is false.
   * Default value: false.
   */
  ignoreCase?: boolean;
  /**
   * A value that indicates whether the token filter is in query mode. When in query mode, the
   * token filter generates bigrams and then removes common words and single terms followed by a
   * common word. Default is false. Default value: false.
   */
  useQueryMode?: boolean;
}

/**
 * Decomposes compound words found in many Germanic languages. This token filter is implemented
 * using Apache Lucene.
 */
export interface DictionaryDecompounderTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.DictionaryDecompounderTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The list of words to match against.
   */
  wordList: string[];
  /**
   * The minimum word size. Only words longer than this get processed. Default is 5. Maximum is
   * 300. Default value: 5.
   */
  minWordSize?: number;
  /**
   * The minimum subword size. Only subwords longer than this are outputted. Default is 2. Maximum
   * is 300. Default value: 2.
   */
  minSubwordSize?: number;
  /**
   * The maximum subword size. Only subwords shorter than this are outputted. Default is 15.
   * Maximum is 300. Default value: 15.
   */
  maxSubwordSize?: number;
  /**
   * A value indicating whether to add only the longest matching subword to the output. Default is
   * false. Default value: false.
   */
  onlyLongestMatch?: boolean;
}

/**
 * Generates n-grams of the given size(s) starting from the front or the back of an input token.
 * This token filter is implemented using Apache Lucene.
 */
export interface EdgeNGramTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.EdgeNGramTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The minimum n-gram length. Default is 1. Must be less than the value of maxGram. Default
   * value: 1.
   */
  minGram?: number;
  /**
   * The maximum n-gram length. Default is 2. Default value: 2.
   */
  maxGram?: number;
  /**
   * Specifies which side of the input the n-gram should be generated from. Default is "front".
   * Possible values include: 'front', 'back'
   */
  side?: EdgeNGramTokenFilterSide;
}

/**
 * Generates n-grams of the given size(s) starting from the front or the back of an input token.
 * This token filter is implemented using Apache Lucene.
 */
export interface EdgeNGramTokenFilterV2 {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.EdgeNGramTokenFilterV2";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of
   * maxGram. Default value: 1.
   */
  minGram?: number;
  /**
   * The maximum n-gram length. Default is 2. Maximum is 300. Default value: 2.
   */
  maxGram?: number;
  /**
   * Specifies which side of the input the n-gram should be generated from. Default is "front".
   * Possible values include: 'front', 'back'
   */
  side?: EdgeNGramTokenFilterSide;
}

/**
 * Removes elisions. For example, "l'avion" (the plane) will be converted to "avion" (plane). This
 * token filter is implemented using Apache Lucene.
 */
export interface ElisionTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.ElisionTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The set of articles to remove.
   */
  articles?: string[];
}

/**
 * A token filter that only keeps tokens with text contained in a specified list of words. This
 * token filter is implemented using Apache Lucene.
 */
export interface KeepTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.KeepTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The list of words to keep.
   */
  keepWords: string[];
  /**
   * A value indicating whether to lower case all words first. Default is false. Default value:
   * false.
   */
  lowerCaseKeepWords?: boolean;
}

/**
 * Marks terms as keywords. This token filter is implemented using Apache Lucene.
 */
export interface KeywordMarkerTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.KeywordMarkerTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * A list of words to mark as keywords.
   */
  keywords: string[];
  /**
   * A value indicating whether to ignore case. If true, all words are converted to lower case
   * first. Default is false. Default value: false.
   */
  ignoreCase?: boolean;
}

/**
 * Removes words that are too long or too short. This token filter is implemented using Apache
 * Lucene.
 */
export interface LengthTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.LengthTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The minimum length in characters. Default is 0. Maximum is 300. Must be less than the value of
   * max. Default value: 0.
   */
  min?: number;
  /**
   * The maximum length in characters. Default and maximum is 300. Default value: 300.
   */
  max?: number;
}

/**
 * Limits the number of tokens while indexing. This token filter is implemented using Apache
 * Lucene.
 */
export interface LimitTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.LimitTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The maximum number of tokens to produce. Default is 1. Default value: 1.
   */
  maxTokenCount?: number;
  /**
   * A value indicating whether all tokens from the input must be consumed even if maxTokenCount is
   * reached. Default is false. Default value: false.
   */
  consumeAllTokens?: boolean;
}

/**
 * Generates n-grams of the given size(s). This token filter is implemented using Apache Lucene.
 */
export interface NGramTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.NGramTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The minimum n-gram length. Default is 1. Must be less than the value of maxGram. Default
   * value: 1.
   */
  minGram?: number;
  /**
   * The maximum n-gram length. Default is 2. Default value: 2.
   */
  maxGram?: number;
}

/**
 * Generates n-grams of the given size(s). This token filter is implemented using Apache Lucene.
 */
export interface NGramTokenFilterV2 {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.NGramTokenFilterV2";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of
   * maxGram. Default value: 1.
   */
  minGram?: number;
  /**
   * The maximum n-gram length. Default is 2. Maximum is 300. Default value: 2.
   */
  maxGram?: number;
}

/**
 * Uses Java regexes to emit multiple tokens - one for each capture group in one or more patterns.
 * This token filter is implemented using Apache Lucene.
 */
export interface PatternCaptureTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.PatternCaptureTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * A list of patterns to match against each token.
   */
  patterns: string[];
  /**
   * A value indicating whether to return the original token even if one of the patterns matches.
   * Default is true. Default value: true.
   */
  preserveOriginal?: boolean;
}

/**
 * A character filter that replaces characters in the input string. It uses a regular expression to
 * identify character sequences to preserve and a replacement pattern to identify characters to
 * replace. For example, given the input text "aa bb aa bb", pattern "(aa)\s+(bb)", and replacement
 * "$1#$2", the result would be "aa#bb aa#bb". This token filter is implemented using Apache
 * Lucene.
 */
export interface PatternReplaceTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.PatternReplaceTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * A regular expression pattern.
   */
  pattern: string;
  /**
   * The replacement text.
   */
  replacement: string;
}

/**
 * Create tokens for phonetic matches. This token filter is implemented using Apache Lucene.
 */
export interface PhoneticTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.PhoneticTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The phonetic encoder to use. Default is "metaphone". Possible values include: 'metaphone',
   * 'doubleMetaphone', 'soundex', 'refinedSoundex', 'caverphone1', 'caverphone2', 'cologne',
   * 'nysiis', 'koelnerPhonetik', 'haasePhonetik', 'beiderMorse'
   */
  encoder?: PhoneticEncoder;
  /**
   * A value indicating whether encoded tokens should replace original tokens. If false, encoded
   * tokens are added as synonyms. Default is true. Default value: true.
   */
  replaceOriginalTokens?: boolean;
}

/**
 * Creates combinations of tokens as a single token. This token filter is implemented using Apache
 * Lucene.
 */
export interface ShingleTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.ShingleTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The maximum shingle size. Default and minimum value is 2. Default value: 2.
   */
  maxShingleSize?: number;
  /**
   * The minimum shingle size. Default and minimum value is 2. Must be less than the value of
   * maxShingleSize. Default value: 2.
   */
  minShingleSize?: number;
  /**
   * A value indicating whether the output stream will contain the input tokens (unigrams) as well
   * as shingles. Default is true. Default value: true.
   */
  outputUnigrams?: boolean;
  /**
   * A value indicating whether to output unigrams for those times when no shingles are available.
   * This property takes precedence when outputUnigrams is set to false. Default is false. Default
   * value: false.
   */
  outputUnigramsIfNoShingles?: boolean;
  /**
   * The string to use when joining adjacent tokens to form a shingle. Default is a single space ("
   * "). Default value: ''.
   */
  tokenSeparator?: string;
  /**
   * The string to insert for each position at which there is no token. Default is an underscore
   * ("_"). Default value: '_'.
   */
  filterToken?: string;
}

/**
 * A filter that stems words using a Snowball-generated stemmer. This token filter is implemented
 * using Apache Lucene.
 */
export interface SnowballTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.SnowballTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The language to use. Possible values include: 'armenian', 'basque', 'catalan', 'danish',
   * 'dutch', 'english', 'finnish', 'french', 'german', 'german2', 'hungarian', 'italian', 'kp',
   * 'lovins', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish',
   * 'turkish'
   */
  language: SnowballTokenFilterLanguage;
}

/**
 * Language specific stemming filter. This token filter is implemented using Apache Lucene.
 */
export interface StemmerTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.StemmerTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The language to use. Possible values include: 'arabic', 'armenian', 'basque', 'brazilian',
   * 'bulgarian', 'catalan', 'czech', 'danish', 'dutch', 'dutchKp', 'english', 'lightEnglish',
   * 'minimalEnglish', 'possessiveEnglish', 'porter2', 'lovins', 'finnish', 'lightFinnish',
   * 'french', 'lightFrench', 'minimalFrench', 'galician', 'minimalGalician', 'german', 'german2',
   * 'lightGerman', 'minimalGerman', 'greek', 'hindi', 'hungarian', 'lightHungarian', 'indonesian',
   * 'irish', 'italian', 'lightItalian', 'sorani', 'latvian', 'norwegian', 'lightNorwegian',
   * 'minimalNorwegian', 'lightNynorsk', 'minimalNynorsk', 'portuguese', 'lightPortuguese',
   * 'minimalPortuguese', 'portugueseRslp', 'romanian', 'russian', 'lightRussian', 'spanish',
   * 'lightSpanish', 'swedish', 'lightSwedish', 'turkish'
   */
  language: StemmerTokenFilterLanguage;
}

/**
 * Provides the ability to override other stemming filters with custom dictionary-based stemming.
 * Any dictionary-stemmed terms will be marked as keywords so that they will not be stemmed with
 * stemmers down the chain. Must be placed before any stemming filters. This token filter is
 * implemented using Apache Lucene.
 */
export interface StemmerOverrideTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.StemmerOverrideTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * A list of stemming rules in the following format: "word => stem", for example: "ran => run".
   */
  rules: string[];
}

/**
 * Removes stop words from a token stream. This token filter is implemented using Apache Lucene.
 */
export interface StopwordsTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.StopwordsTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The list of stopwords. This property and the stopwords list property cannot both be set.
   */
  stopwords?: string[];
  /**
   * A predefined list of stopwords to use. This property and the stopwords property cannot both be
   * set. Default is English. Possible values include: 'arabic', 'armenian', 'basque', 'brazilian',
   * 'bulgarian', 'catalan', 'czech', 'danish', 'dutch', 'english', 'finnish', 'french',
   * 'galician', 'german', 'greek', 'hindi', 'hungarian', 'indonesian', 'irish', 'italian',
   * 'latvian', 'norwegian', 'persian', 'portuguese', 'romanian', 'russian', 'sorani', 'spanish',
   * 'swedish', 'thai', 'turkish'
   */
  stopwordsList?: StopwordsList;
  /**
   * A value indicating whether to ignore case. If true, all words are converted to lower case
   * first. Default is false. Default value: false.
   */
  ignoreCase?: boolean;
  /**
   * A value indicating whether to ignore the last search term if it's a stop word. Default is
   * true. Default value: true.
   */
  removeTrailingStopWords?: boolean;
}

/**
 * Matches single or multi-word synonyms in a token stream. This token filter is implemented using
 * Apache Lucene.
 */
export interface SynonymTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.SynonymTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * A list of synonyms in following one of two formats: 1. incredible, unbelievable, fabulous =>
   * amazing - all terms on the left side of => symbol will be replaced with all terms on its right
   * side; 2. incredible, unbelievable, fabulous, amazing - comma separated list of equivalent
   * words. Set the expand option to change how this list is interpreted.
   */
  synonyms: string[];
  /**
   * A value indicating whether to case-fold input for matching. Default is false. Default value:
   * false.
   */
  ignoreCase?: boolean;
  /**
   * A value indicating whether all words in the list of synonyms (if => notation is not used) will
   * map to one another. If true, all words in the list of synonyms (if => notation is not used)
   * will map to one another. The following list: incredible, unbelievable, fabulous, amazing is
   * equivalent to: incredible, unbelievable, fabulous, amazing => incredible, unbelievable,
   * fabulous, amazing. If false, the following list: incredible, unbelievable, fabulous, amazing
   * will be equivalent to: incredible, unbelievable, fabulous, amazing => incredible. Default is
   * true. Default value: true.
   */
  expand?: boolean;
}

/**
 * Truncates the terms to a specific length. This token filter is implemented using Apache Lucene.
 */
export interface TruncateTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.TruncateTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * The length at which terms will be truncated. Default and maximum is 300. Default value: 300.
   */
  length?: number;
}

/**
 * Filters out tokens with same text as the previous token. This token filter is implemented using
 * Apache Lucene.
 */
export interface UniqueTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.UniqueTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * A value indicating whether to remove duplicates only at the same position. Default is false.
   * Default value: false.
   */
  onlyOnSamePosition?: boolean;
}

/**
 * Splits words into subwords and performs optional transformations on subword groups. This token
 * filter is implemented using Apache Lucene.
 */
export interface WordDelimiterTokenFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.WordDelimiterTokenFilter";
  /**
   * The name of the token filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * A value indicating whether to generate part words. If set, causes parts of words to be
   * generated; for example "AzureSearch" becomes "Azure" "Search". Default is true. Default value:
   * true.
   */
  generateWordParts?: boolean;
  /**
   * A value indicating whether to generate number subwords. Default is true. Default value: true.
   */
  generateNumberParts?: boolean;
  /**
   * A value indicating whether maximum runs of word parts will be catenated. For example, if this
   * is set to true, "Azure-Search" becomes "AzureSearch". Default is false. Default value: false.
   */
  catenateWords?: boolean;
  /**
   * A value indicating whether maximum runs of number parts will be catenated. For example, if
   * this is set to true, "1-2" becomes "12". Default is false. Default value: false.
   */
  catenateNumbers?: boolean;
  /**
   * A value indicating whether all subword parts will be catenated. For example, if this is set to
   * true, "Azure-Search-1" becomes "AzureSearch1". Default is false. Default value: false.
   */
  catenateAll?: boolean;
  /**
   * A value indicating whether to split words on caseChange. For example, if this is set to true,
   * "AzureSearch" becomes "Azure" "Search". Default is true. Default value: true.
   */
  splitOnCaseChange?: boolean;
  /**
   * A value indicating whether original words will be preserved and added to the subword list.
   * Default is false. Default value: false.
   */
  preserveOriginal?: boolean;
  /**
   * A value indicating whether to split on numbers. For example, if this is set to true,
   * "Azure1Search" becomes "Azure" "1" "Search". Default is true. Default value: true.
   */
  splitOnNumerics?: boolean;
  /**
   * A value indicating whether to remove trailing "'s" for each subword. Default is true. Default
   * value: true.
   */
  stemEnglishPossessive?: boolean;
  /**
   * A list of tokens to protect from being delimited.
   */
  protectedWords?: string[];
}

/**
 * Contains the possible cases for CharFilter.
 */
export type CharFilterUnion = CharFilter | MappingCharFilter | PatternReplaceCharFilter;

/**
 * Abstract base class for character filters.
 */
export interface CharFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "CharFilter";
  /**
   * The name of the char filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
}

/**
 * A character filter that applies mappings defined with the mappings option. Matching is greedy
 * (longest pattern matching at a given point wins). Replacement is allowed to be the empty string.
 * This character filter is implemented using Apache Lucene.
 */
export interface MappingCharFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.MappingCharFilter";
  /**
   * The name of the char filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * A list of mappings of the following format: "a=>b" (all occurrences of the character "a" will
   * be replaced with character "b").
   */
  mappings: string[];
}

/**
 * A character filter that replaces characters in the input string. It uses a regular expression to
 * identify character sequences to preserve and a replacement pattern to identify characters to
 * replace. For example, given the input text "aa bb aa bb", pattern "(aa)\s+(bb)", and replacement
 * "$1#$2", the result would be "aa#bb aa#bb". This character filter is implemented using Apache
 * Lucene.
 */
export interface PatternReplaceCharFilter {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.PatternReplaceCharFilter";
  /**
   * The name of the char filter. It must only contain letters, digits, spaces, dashes or
   * underscores, can only start and end with alphanumeric characters, and is limited to 128
   * characters.
   */
  name: string;
  /**
   * A regular expression pattern.
   */
  pattern: string;
  /**
   * The replacement text.
   */
  replacement: string;
}

/**
 * Represents credentials that can be used to connect to a datasource.
 */
export interface DataSourceCredentials {
  /**
   * The connection string for the datasource.
   */
  connectionString?: string;
}

/**
 * Represents information about the entity (such as Azure SQL table or CosmosDB collection) that
 * will be indexed.
 */
export interface DataContainer {
  /**
   * The name of the table or view (for Azure SQL data source) or collection (for CosmosDB data
   * source) that will be indexed.
   */
  name: string;
  /**
   * A query that is applied to this data container. The syntax and meaning of this parameter is
   * datasource-specific. Not supported by Azure SQL datasources.
   */
  query?: string;
}

/**
 * Contains the possible cases for DataChangeDetectionPolicy.
 */
export type DataChangeDetectionPolicyUnion = DataChangeDetectionPolicy | HighWaterMarkChangeDetectionPolicy | SqlIntegratedChangeTrackingPolicy;

/**
 * Abstract base class for data change detection policies.
 */
export interface DataChangeDetectionPolicy {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "DataChangeDetectionPolicy";
}

/**
 * Defines a data change detection policy that captures changes based on the value of a high water
 * mark column.
 */
export interface HighWaterMarkChangeDetectionPolicy {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.HighWaterMarkChangeDetectionPolicy";
  /**
   * The name of the high water mark column.
   */
  highWaterMarkColumnName: string;
}

/**
 * Defines a data change detection policy that captures changes using the Integrated Change
 * Tracking feature of Azure SQL Database.
 */
export interface SqlIntegratedChangeTrackingPolicy {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.SqlIntegratedChangeTrackingPolicy";
}

/**
 * Contains the possible cases for DataDeletionDetectionPolicy.
 */
export type DataDeletionDetectionPolicyUnion = DataDeletionDetectionPolicy | SoftDeleteColumnDeletionDetectionPolicy;

/**
 * Abstract base class for data deletion detection policies.
 */
export interface DataDeletionDetectionPolicy {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "DataDeletionDetectionPolicy";
}

/**
 * Defines a data deletion detection policy that implements a soft-deletion strategy. It determines
 * whether an item should be deleted based on the value of a designated 'soft delete' column.
 */
export interface SoftDeleteColumnDeletionDetectionPolicy {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.SoftDeleteColumnDeletionDetectionPolicy";
  /**
   * The name of the column to use for soft-deletion detection.
   */
  softDeleteColumnName?: string;
  /**
   * The marker value that identifies an item as deleted.
   */
  softDeleteMarkerValue?: string;
}

/**
 * Represents a datasource definition, which can be used to configure an indexer.
 */
export interface DataSource {
  /**
   * The name of the datasource.
   */
  name: string;
  /**
   * The description of the datasource.
   */
  description?: string;
  /**
   * The type of the datasource. Possible values include: 'AzureSql', 'CosmosDb', 'AzureBlob',
   * 'AzureTable', 'MySql'
   */
  type: DataSourceType;
  /**
   * Credentials for the datasource.
   */
  credentials: DataSourceCredentials;
  /**
   * The data container for the datasource.
   */
  container: DataContainer;
  /**
   * The data change detection policy for the datasource.
   */
  dataChangeDetectionPolicy?: DataChangeDetectionPolicyUnion;
  /**
   * The data deletion detection policy for the datasource.
   */
  dataDeletionDetectionPolicy?: DataDeletionDetectionPolicyUnion;
  /**
   * The ETag of the DataSource.
   */
  etag?: string;
}

/**
 * Response from a List Datasources request. If successful, it includes the full definitions of all
 * datasources.
 */
export interface ListDataSourcesResult {
  /**
   * The datasources in the Search service.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly dataSources: DataSource[];
}

/**
 * Represents a schedule for indexer execution.
 */
export interface IndexingSchedule {
  /**
   * The interval of time between indexer executions.
   */
  interval: string;
  /**
   * The time when an indexer should start running.
   */
  startTime?: Date;
}

/**
 * Represents parameters for indexer execution.
 */
export interface IndexingParameters {
  /**
   * The number of items that are read from the data source and indexed as a single batch in order
   * to improve performance. The default depends on the data source type.
   */
  batchSize?: number;
  /**
   * The maximum number of items that can fail indexing for indexer execution to still be
   * considered successful. -1 means no limit. Default is 0. Default value: 0.
   */
  maxFailedItems?: number;
  /**
   * The maximum number of items in a single batch that can fail indexing for the batch to still be
   * considered successful. -1 means no limit. Default is 0. Default value: 0.
   */
  maxFailedItemsPerBatch?: number;
  /**
   * A dictionary of indexer-specific configuration properties. Each name is the name of a specific
   * property. Each value must be of a primitive type.
   */
  configuration?: { [propertyName: string]: any };
}

/**
 * Represents a function that transforms a value from a data source before indexing.
 */
export interface FieldMappingFunction {
  /**
   * The name of the field mapping function.
   */
  name: string;
  /**
   * A dictionary of parameter name/value pairs to pass to the function. Each value must be of a
   * primitive type.
   */
  parameters?: { [propertyName: string]: any };
}

/**
 * Defines a mapping between a field in a data source and a target field in an index.
 */
export interface FieldMapping {
  /**
   * The name of the field in the data source.
   */
  sourceFieldName: string;
  /**
   * The name of the target field in the index. Same as the source field name by default.
   */
  targetFieldName?: string;
  /**
   * A function to apply to each source field value before indexing.
   */
  mappingFunction?: FieldMappingFunction;
}

/**
 * Represents an indexer.
 */
export interface Indexer {
  /**
   * The name of the indexer.
   */
  name: string;
  /**
   * The description of the indexer.
   */
  description?: string;
  /**
   * The name of the datasource from which this indexer reads data.
   */
  dataSourceName: string;
  /**
   * The name of the skillset executing with this indexer.
   */
  skillsetName?: string;
  /**
   * The name of the index to which this indexer writes data.
   */
  targetIndexName: string;
  /**
   * The schedule for this indexer.
   */
  schedule?: IndexingSchedule;
  /**
   * Parameters for indexer execution.
   */
  parameters?: IndexingParameters;
  /**
   * Defines mappings between fields in the data source and corresponding target fields in the
   * index.
   */
  fieldMappings?: FieldMapping[];
  /**
   * Output field mappings are applied after enrichment and immediately before indexing.
   */
  outputFieldMappings?: FieldMapping[];
  /**
   * A value indicating whether the indexer is disabled. Default is false. Default value: false.
   */
  isDisabled?: boolean;
  /**
   * The ETag of the Indexer.
   */
  etag?: string;
}

/**
 * Response from a List Indexers request. If successful, it includes the full definitions of all
 * indexers.
 */
export interface ListIndexersResult {
  /**
   * The indexers in the Search service.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly indexers: Indexer[];
}

/**
 * Represents an item- or document-level indexing error.
 */
export interface ItemError {
  /**
   * The key of the item for which indexing failed.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly key?: string;
  /**
   * The message describing the error that occurred while processing the item.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly errorMessage: string;
  /**
   * The status code indicating why the indexing operation failed. Possible values include: 400 for
   * a malformed input document, 404 for document not found, 409 for a version conflict, 422 when
   * the index is temporarily unavailable, or 503 for when the service is too busy.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly statusCode: number;
  /**
   * The name of the source at which the error originated. For example, this could refer to a
   * particular skill in the attached skillset. This may not be always available.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly name?: string;
  /**
   * Additional, verbose details about the error to assist in debugging the indexer. This may not
   * be always available.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly details?: string;
  /**
   * A link to a troubleshooting guide for these classes of errors. This may not be always
   * available.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly documentationLink?: string;
}

/**
 * Represents an item-level warning.
 */
export interface ItemWarning {
  /**
   * The key of the item which generated a warning.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly key?: string;
  /**
   * The message describing the warning that occurred while processing the item.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly message: string;
  /**
   * The name of the source at which the warning originated. For example, this could refer to a
   * particular skill in the attached skillset. This may not be always available.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly name?: string;
  /**
   * Additional, verbose details about the warning to assist in debugging the indexer. This may not
   * be always available.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly details?: string;
  /**
   * A link to a troubleshooting guide for these classes of warnings. This may not be always
   * available.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly documentationLink?: string;
}

/**
 * Represents the result of an individual indexer execution.
 */
export interface IndexerExecutionResult {
  /**
   * The outcome of this indexer execution. Possible values include: 'transientFailure', 'success',
   * 'inProgress', 'reset'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly status: IndexerExecutionStatus;
  /**
   * The error message indicating the top-level error, if any.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly errorMessage?: string;
  /**
   * The start time of this indexer execution.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly startTime?: Date;
  /**
   * The end time of this indexer execution, if the execution has already completed.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly endTime?: Date;
  /**
   * The item-level indexing errors.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly errors: ItemError[];
  /**
   * The item-level indexing warnings.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly warnings: ItemWarning[];
  /**
   * The number of items that were processed during this indexer execution. This includes both
   * successfully processed items and items where indexing was attempted but failed.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly itemCount: number;
  /**
   * The number of items that failed to be indexed during this indexer execution.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly failedItemCount: number;
  /**
   * Change tracking state with which an indexer execution started.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly initialTrackingState?: string;
  /**
   * Change tracking state with which an indexer execution finished.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly finalTrackingState?: string;
}

/**
 * An interface representing IndexerLimits.
 */
export interface IndexerLimits {
  /**
   * The maximum duration that the indexer is permitted to run for one execution.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly maxRunTime?: string;
  /**
   * The maximum size of a document, in bytes, which will be considered valid for indexing.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly maxDocumentExtractionSize?: number;
  /**
   * The maximum number of characters that will be extracted from a document picked up for
   * indexing.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly maxDocumentContentCharactersToExtract?: number;
}

/**
 * Represents the current status and execution history of an indexer.
 */
export interface IndexerExecutionInfo {
  /**
   * Overall indexer status. Possible values include: 'unknown', 'error', 'running'
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly status: IndexerStatus;
  /**
   * The result of the most recent or an in-progress indexer execution.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly lastResult?: IndexerExecutionResult;
  /**
   * History of the recent indexer executions, sorted in reverse chronological order.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly executionHistory: IndexerExecutionResult[];
  /**
   * The execution limits for the indexer.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly limits: IndexerLimits;
}

/**
 * Represents a field in an index definition, which describes the name, data type, and search
 * behavior of a field.
 */
export interface Field {
  /**
   * The name of the field, which must be unique within the fields collection of the index or
   * parent field.
   */
  name: string;
  /**
   * The data type of the field. Possible values include: 'Edm.String', 'Edm.Int32', 'Edm.Int64',
   * 'Edm.Double', 'Edm.Boolean', 'Edm.DateTimeOffset', 'Edm.GeographyPoint', 'Edm.ComplexType',
   * 'Collection(Edm.String)', 'Collection(Edm.Int32)', 'Collection(Edm.Int64)',
   * 'Collection(Edm.Double)', 'Collection(Edm.Boolean)', 'Collection(Edm.DateTimeOffset)',
   * 'Collection(Edm.GeographyPoint)', 'Collection(Edm.ComplexType)'
   */
  type: DataType;
  /**
   * A value indicating whether the field uniquely identifies documents in the index. Exactly one
   * top-level field in each index must be chosen as the key field and it must be of type
   * Edm.String. Key fields can be used to look up documents directly and update or delete specific
   * documents. Default is false for simple fields and null for complex fields.
   */
  key?: boolean;
  /**
   * A value indicating whether the field can be returned in a search result. You can disable this
   * option if you want to use a field (for example, margin) as a filter, sorting, or scoring
   * mechanism but do not want the field to be visible to the end user. This property must be true
   * for key fields, and it must be null for complex fields. This property can be changed on
   * existing fields. Enabling this property does not cause any increase in index storage
   * requirements. Default is true for simple fields and null for complex fields.
   */
  retrievable?: boolean;
  /**
   * A value indicating whether the field is full-text searchable. This means it will undergo
   * analysis such as word-breaking during indexing. If you set a searchable field to a value like
   * "sunny day", internally it will be split into the individual tokens "sunny" and "day". This
   * enables full-text searches for these terms. Fields of type Edm.String or
   * Collection(Edm.String) are searchable by default. This property must be false for simple
   * fields of other non-string data types, and it must be null for complex fields. Note:
   * searchable fields consume extra space in your index since Azure Cognitive Search will store an
   * additional tokenized version of the field value for full-text searches. If you want to save
   * space in your index and you don't need a field to be included in searches, set searchable to
   * false.
   */
  searchable?: boolean;
  /**
   * A value indicating whether to enable the field to be referenced in $filter queries. filterable
   * differs from searchable in how strings are handled. Fields of type Edm.String or
   * Collection(Edm.String) that are filterable do not undergo word-breaking, so comparisons are
   * for exact matches only. For example, if you set such a field f to "sunny day", $filter=f eq
   * 'sunny' will find no matches, but $filter=f eq 'sunny day' will. This property must be null
   * for complex fields. Default is true for simple fields and null for complex fields.
   */
  filterable?: boolean;
  /**
   * A value indicating whether to enable the field to be referenced in $orderby expressions. By
   * default Azure Cognitive Search sorts results by score, but in many experiences users will want
   * to sort by fields in the documents. A simple field can be sortable only if it is single-valued
   * (it has a single value in the scope of the parent document). Simple collection fields cannot
   * be sortable, since they are multi-valued. Simple sub-fields of complex collections are also
   * multi-valued, and therefore cannot be sortable. This is true whether it's an immediate parent
   * field, or an ancestor field, that's the complex collection. Complex fields cannot be sortable
   * and the sortable property must be null for such fields. The default for sortable is true for
   * single-valued simple fields, false for multi-valued simple fields, and null for complex
   * fields.
   */
  sortable?: boolean;
  /**
   * A value indicating whether to enable the field to be referenced in facet queries. Typically
   * used in a presentation of search results that includes hit count by category (for example,
   * search for digital cameras and see hits by brand, by megapixels, by price, and so on). This
   * property must be null for complex fields. Fields of type Edm.GeographyPoint or
   * Collection(Edm.GeographyPoint) cannot be facetable. Default is true for all other simple
   * fields.
   */
  facetable?: boolean;
  /**
   * The name of the language analyzer to use for the field. This option can be used only with
   * searchable fields and it can't be set together with either searchAnalyzer or indexAnalyzer.
   * Once the analyzer is chosen, it cannot be changed for the field. Must be null for complex
   * fields. KnownAnalyzerNames is an enum containing known values.
   */
  analyzer?: string;
  /**
   * The name of the analyzer used at search time for the field. This option can be used only with
   * searchable fields. It must be set together with indexAnalyzer and it cannot be set together
   * with the analyzer option. This analyzer can be updated on an existing field. Must be null for
   * complex fields. KnownAnalyzerNames is an enum containing known values.
   */
  searchAnalyzer?: string;
  /**
   * The name of the analyzer used at indexing time for the field. This option can be used only
   * with searchable fields. It must be set together with searchAnalyzer and it cannot be set
   * together with the analyzer option. Once the analyzer is chosen, it cannot be changed for the
   * field. Must be null for complex fields. KnownAnalyzerNames is an enum containing known values.
   */
  indexAnalyzer?: string;
  /**
   * A list of the names of synonym maps to associate with this field. This option can be used only
   * with searchable fields. Currently only one synonym map per field is supported. Assigning a
   * synonym map to a field ensures that query terms targeting that field are expanded at
   * query-time using the rules in the synonym map. This attribute can be changed on existing
   * fields. Must be null or an empty collection for complex fields.
   */
  synonymMaps?: string[];
  /**
   * A list of sub-fields if this is a field of type Edm.ComplexType or
   * Collection(Edm.ComplexType). Must be null or empty for simple fields.
   */
  fields?: Field[];
}

/**
 * Defines weights on index fields for which matches should boost scoring in search queries.
 */
export interface TextWeights {
  /**
   * The dictionary of per-field weights to boost document scoring. The keys are field names and
   * the values are the weights for each field.
   */
  weights: { [propertyName: string]: number };
}

/**
 * Contains the possible cases for ScoringFunction.
 */
export type ScoringFunctionUnion = ScoringFunction | DistanceScoringFunction | FreshnessScoringFunction | MagnitudeScoringFunction | TagScoringFunction;

/**
 * Abstract base class for functions that can modify document scores during ranking.
 */
export interface ScoringFunction {
  /**
   * Polymorphic Discriminator
   */
  type: "ScoringFunction";
  /**
   * The name of the field used as input to the scoring function.
   */
  fieldName: string;
  /**
   * A multiplier for the raw score. Must be a positive number not equal to 1.0.
   */
  boost: number;
  /**
   * A value indicating how boosting will be interpolated across document scores; defaults to
   * "Linear". Possible values include: 'linear', 'constant', 'quadratic', 'logarithmic'
   */
  interpolation?: ScoringFunctionInterpolation;
}

/**
 * Provides parameter values to a distance scoring function.
 */
export interface DistanceScoringParameters {
  /**
   * The name of the parameter passed in search queries to specify the reference location.
   */
  referencePointParameter: string;
  /**
   * The distance in kilometers from the reference location where the boosting range ends.
   */
  boostingDistance: number;
}

/**
 * Defines a function that boosts scores based on distance from a geographic location.
 */
export interface DistanceScoringFunction {
  /**
   * Polymorphic Discriminator
   */
  type: "distance";
  /**
   * The name of the field used as input to the scoring function.
   */
  fieldName: string;
  /**
   * A multiplier for the raw score. Must be a positive number not equal to 1.0.
   */
  boost: number;
  /**
   * A value indicating how boosting will be interpolated across document scores; defaults to
   * "Linear". Possible values include: 'linear', 'constant', 'quadratic', 'logarithmic'
   */
  interpolation?: ScoringFunctionInterpolation;
  /**
   * Parameter values for the distance scoring function.
   */
  parameters: DistanceScoringParameters;
}

/**
 * Provides parameter values to a freshness scoring function.
 */
export interface FreshnessScoringParameters {
  /**
   * The expiration period after which boosting will stop for a particular document.
   */
  boostingDuration: string;
}

/**
 * Defines a function that boosts scores based on the value of a date-time field.
 */
export interface FreshnessScoringFunction {
  /**
   * Polymorphic Discriminator
   */
  type: "freshness";
  /**
   * The name of the field used as input to the scoring function.
   */
  fieldName: string;
  /**
   * A multiplier for the raw score. Must be a positive number not equal to 1.0.
   */
  boost: number;
  /**
   * A value indicating how boosting will be interpolated across document scores; defaults to
   * "Linear". Possible values include: 'linear', 'constant', 'quadratic', 'logarithmic'
   */
  interpolation?: ScoringFunctionInterpolation;
  /**
   * Parameter values for the freshness scoring function.
   */
  parameters: FreshnessScoringParameters;
}

/**
 * Provides parameter values to a magnitude scoring function.
 */
export interface MagnitudeScoringParameters {
  /**
   * The field value at which boosting starts.
   */
  boostingRangeStart: number;
  /**
   * The field value at which boosting ends.
   */
  boostingRangeEnd: number;
  /**
   * A value indicating whether to apply a constant boost for field values beyond the range end
   * value; default is false.
   */
  shouldBoostBeyondRangeByConstant?: boolean;
}

/**
 * Defines a function that boosts scores based on the magnitude of a numeric field.
 */
export interface MagnitudeScoringFunction {
  /**
   * Polymorphic Discriminator
   */
  type: "magnitude";
  /**
   * The name of the field used as input to the scoring function.
   */
  fieldName: string;
  /**
   * A multiplier for the raw score. Must be a positive number not equal to 1.0.
   */
  boost: number;
  /**
   * A value indicating how boosting will be interpolated across document scores; defaults to
   * "Linear". Possible values include: 'linear', 'constant', 'quadratic', 'logarithmic'
   */
  interpolation?: ScoringFunctionInterpolation;
  /**
   * Parameter values for the magnitude scoring function.
   */
  parameters: MagnitudeScoringParameters;
}

/**
 * Provides parameter values to a tag scoring function.
 */
export interface TagScoringParameters {
  /**
   * The name of the parameter passed in search queries to specify the list of tags to compare
   * against the target field.
   */
  tagsParameter: string;
}

/**
 * Defines a function that boosts scores of documents with string values matching a given list of
 * tags.
 */
export interface TagScoringFunction {
  /**
   * Polymorphic Discriminator
   */
  type: "tag";
  /**
   * The name of the field used as input to the scoring function.
   */
  fieldName: string;
  /**
   * A multiplier for the raw score. Must be a positive number not equal to 1.0.
   */
  boost: number;
  /**
   * A value indicating how boosting will be interpolated across document scores; defaults to
   * "Linear". Possible values include: 'linear', 'constant', 'quadratic', 'logarithmic'
   */
  interpolation?: ScoringFunctionInterpolation;
  /**
   * Parameter values for the tag scoring function.
   */
  parameters: TagScoringParameters;
}

/**
 * Defines parameters for a search index that influence scoring in search queries.
 */
export interface ScoringProfile {
  /**
   * The name of the scoring profile.
   */
  name: string;
  /**
   * Parameters that boost scoring based on text matches in certain index fields.
   */
  textWeights?: TextWeights;
  /**
   * The collection of functions that influence the scoring of documents.
   */
  functions?: ScoringFunctionUnion[];
  /**
   * A value indicating how the results of individual scoring functions should be combined.
   * Defaults to "Sum". Ignored if there are no scoring functions. Possible values include: 'sum',
   * 'average', 'minimum', 'maximum', 'firstMatching'
   */
  functionAggregation?: ScoringFunctionAggregation;
}

/**
 * Defines options to control Cross-Origin Resource Sharing (CORS) for an index.
 */
export interface CorsOptions {
  /**
   * The list of origins from which JavaScript code will be granted access to your index. Can
   * contain a list of hosts of the form {protocol}://{fully-qualified-domain-name}[:{port#}], or a
   * single '*' to allow all origins (not recommended).
   */
  allowedOrigins: string[];
  /**
   * The duration for which browsers should cache CORS preflight responses. Defaults to 5 minutes.
   */
  maxAgeInSeconds?: number;
}

/**
 * Defines how the Suggest API should apply to a group of fields in the index.
 */
export interface Suggester {
  /**
   * The name of the suggester.
   */
  name: string;
  /**
   * The list of field names to which the suggester applies. Each field must be searchable.
   */
  sourceFields: string[];
}

/**
 * Credentials of a registered application created for your search service, used for authenticated
 * access to the encryption keys stored in Azure Key Vault.
 */
export interface AzureActiveDirectoryApplicationCredentials {
  /**
   * An AAD Application ID that was granted the required access permissions to the Azure Key Vault
   * that is to be used when encrypting your data at rest. The Application ID should not be
   * confused with the Object ID for your AAD Application.
   */
  applicationId: string;
  /**
   * The authentication key of the specified AAD application.
   */
  applicationSecret?: string;
}

/**
 * A customer-managed encryption key in Azure Key Vault. Keys that you create and manage can be
 * used to encrypt or decrypt data-at-rest in Azure Cognitive Search, such as indexes and synonym
 * maps.
 */
export interface EncryptionKey {
  /**
   * The name of your Azure Key Vault key to be used to encrypt your data at rest.
   */
  keyVaultKeyName: string;
  /**
   * The version of your Azure Key Vault key to be used to encrypt your data at rest.
   */
  keyVaultKeyVersion: string;
  /**
   * The URI of your Azure Key Vault, also referred to as DNS name, that contains the key to be
   * used to encrypt your data at rest. An example URI might be
   * https://my-keyvault-name.vault.azure.net.
   */
  keyVaultUri: string;
  /**
   * Optional Azure Active Directory credentials used for accessing your Azure Key Vault. Not
   * required if using managed identity instead.
   */
  accessCredentials?: AzureActiveDirectoryApplicationCredentials;
}

/**
 * Represents a search index definition, which describes the fields and search behavior of an
 * index.
 */
export interface Index {
  /**
   * The name of the index.
   */
  name: string;
  /**
   * The fields of the index.
   */
  fields: Field[];
  /**
   * The scoring profiles for the index.
   */
  scoringProfiles?: ScoringProfile[];
  /**
   * The name of the scoring profile to use if none is specified in the query. If this property is
   * not set and no scoring profile is specified in the query, then default scoring (tf-idf) will
   * be used.
   */
  defaultScoringProfile?: string;
  /**
   * Options to control Cross-Origin Resource Sharing (CORS) for the index.
   */
  corsOptions?: CorsOptions;
  /**
   * The suggesters for the index.
   */
  suggesters?: Suggester[];
  /**
   * The analyzers for the index.
   */
  analyzers?: AnalyzerUnion[];
  /**
   * The tokenizers for the index.
   */
  tokenizers?: TokenizerUnion[];
  /**
   * The token filters for the index.
   */
  tokenFilters?: TokenFilterUnion[];
  /**
   * The character filters for the index.
   */
  charFilters?: CharFilterUnion[];
  /**
   * A description of an encryption key that you create in Azure Key Vault. This key is used to
   * provide an additional level of encryption-at-rest for your data when you want full assurance
   * that no one, not even Microsoft, can decrypt your data in Azure Cognitive Search. Once you
   * have encrypted your data, it will always remain encrypted. Azure Cognitive Search will ignore
   * attempts to set this property to null. You can change this property as needed if you want to
   * rotate your encryption key; Your data will be unaffected. Encryption with customer-managed
   * keys is not available for free search services, and is only available for paid services
   * created on or after January 1, 2019.
   */
  encryptionKey?: EncryptionKey;
  /**
   * The ETag of the index.
   */
  etag?: string;
}

/**
 * Statistics for a given index. Statistics are collected periodically and are not guaranteed to
 * always be up-to-date.
 */
export interface GetIndexStatisticsResult {
  /**
   * The number of documents in the index.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly documentCount: number;
  /**
   * The amount of storage in bytes consumed by the index.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly storageSize: number;
}

/**
 * Response from a List Indexes request. If successful, it includes the full definitions of all
 * indexes.
 */
export interface ListIndexesResult {
  /**
   * The indexes in the Search service.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly indexes: Index[];
}

/**
 * Input field mapping for a skill.
 */
export interface InputFieldMappingEntry {
  /**
   * The name of the input.
   */
  name: string;
  /**
   * The source of the input.
   */
  source?: string;
  /**
   * The source context used for selecting recursive inputs.
   */
  sourceContext?: string;
  /**
   * The recursive inputs used when creating a complex type.
   */
  inputs?: InputFieldMappingEntry[];
}

/**
 * Output field mapping for a skill.
 */
export interface OutputFieldMappingEntry {
  /**
   * The name of the output defined by the skill.
   */
  name: string;
  /**
   * The target name of the output. It is optional and default to name.
   */
  targetName?: string;
}

/**
 * Contains the possible cases for Skill.
 */
export type SkillUnion = Skill | ConditionalSkill | KeyPhraseExtractionSkill | OcrSkill | ImageAnalysisSkill | LanguageDetectionSkill | ShaperSkill | MergeSkill | EntityRecognitionSkill | SentimentSkill | SplitSkill | TextTranslationSkill | WebApiSkill;

/**
 * Abstract base class for skills.
 */
export interface Skill {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "Skill";
  /**
   * The name of the skill which uniquely identifies it within the skillset. A skill with no name
   * defined will be given a default name of its 1-based index in the skills array, prefixed with
   * the character '#'.
   */
  name?: string;
  /**
   * The description of the skill which describes the inputs, outputs, and usage of the skill.
   */
  description?: string;
  /**
   * Represents the level at which operations take place, such as the document root or document
   * content (for example, /document or /document/content). The default is /document.
   */
  context?: string;
  /**
   * Inputs of the skills could be a column in the source data set, or the output of an upstream
   * skill.
   */
  inputs: InputFieldMappingEntry[];
  /**
   * The output of a skill is either a field in a search index, or a value that can be consumed as
   * an input by another skill.
   */
  outputs: OutputFieldMappingEntry[];
}

/**
 * Contains the possible cases for CognitiveServicesAccount.
 */
export type CognitiveServicesAccountUnion = CognitiveServicesAccount | DefaultCognitiveServicesAccount | CognitiveServicesAccountKey;

/**
 * Abstract base class for describing any cognitive service resource attached to the skillset.
 */
export interface CognitiveServicesAccount {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "CognitiveServicesAccount";
  description?: string;
}

/**
 * A list of skills.
 */
export interface Skillset {
  /**
   * The name of the skillset.
   */
  name: string;
  /**
   * The description of the skillset.
   */
  description: string;
  /**
   * A list of skills in the skillset.
   */
  skills: SkillUnion[];
  /**
   * Details about cognitive services to be used when running skills.
   */
  cognitiveServicesAccount?: CognitiveServicesAccountUnion;
  /**
   * The ETag of the skillset.
   */
  etag?: string;
}

/**
 * An empty object that represents the default cognitive service resource for a skillset.
 */
export interface DefaultCognitiveServicesAccount {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.DefaultCognitiveServices";
  description?: string;
}

/**
 * A cognitive service resource provisioned with a key that is attached to a skillset.
 */
export interface CognitiveServicesAccountKey {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Azure.Search.CognitiveServicesByKey";
  description?: string;
  key: string;
}

/**
 * A skill that enables scenarios that require a Boolean operation to determine the data to assign
 * to an output.
 */
export interface ConditionalSkill {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Skills.Util.ConditionalSkill";
  /**
   * The name of the skill which uniquely identifies it within the skillset. A skill with no name
   * defined will be given a default name of its 1-based index in the skills array, prefixed with
   * the character '#'.
   */
  name?: string;
  /**
   * The description of the skill which describes the inputs, outputs, and usage of the skill.
   */
  description?: string;
  /**
   * Represents the level at which operations take place, such as the document root or document
   * content (for example, /document or /document/content). The default is /document.
   */
  context?: string;
  /**
   * Inputs of the skills could be a column in the source data set, or the output of an upstream
   * skill.
   */
  inputs: InputFieldMappingEntry[];
  /**
   * The output of a skill is either a field in a search index, or a value that can be consumed as
   * an input by another skill.
   */
  outputs: OutputFieldMappingEntry[];
}

/**
 * A skill that uses text analytics for key phrase extraction.
 */
export interface KeyPhraseExtractionSkill {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Skills.Text.KeyPhraseExtractionSkill";
  /**
   * The name of the skill which uniquely identifies it within the skillset. A skill with no name
   * defined will be given a default name of its 1-based index in the skills array, prefixed with
   * the character '#'.
   */
  name?: string;
  /**
   * The description of the skill which describes the inputs, outputs, and usage of the skill.
   */
  description?: string;
  /**
   * Represents the level at which operations take place, such as the document root or document
   * content (for example, /document or /document/content). The default is /document.
   */
  context?: string;
  /**
   * Inputs of the skills could be a column in the source data set, or the output of an upstream
   * skill.
   */
  inputs: InputFieldMappingEntry[];
  /**
   * The output of a skill is either a field in a search index, or a value that can be consumed as
   * an input by another skill.
   */
  outputs: OutputFieldMappingEntry[];
  /**
   * A value indicating which language code to use. Default is en. Possible values include: 'da',
   * 'nl', 'en', 'fi', 'fr', 'de', 'it', 'ja', 'ko', 'no', 'pl', 'pt-PT', 'pt-BR', 'ru', 'es', 'sv'
   */
  defaultLanguageCode?: KeyPhraseExtractionSkillLanguage;
  /**
   * A number indicating how many key phrases to return. If absent, all identified key phrases will
   * be returned.
   */
  maxKeyPhraseCount?: number;
}

/**
 * A skill that extracts text from image files.
 */
export interface OcrSkill {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Skills.Vision.OcrSkill";
  /**
   * The name of the skill which uniquely identifies it within the skillset. A skill with no name
   * defined will be given a default name of its 1-based index in the skills array, prefixed with
   * the character '#'.
   */
  name?: string;
  /**
   * The description of the skill which describes the inputs, outputs, and usage of the skill.
   */
  description?: string;
  /**
   * Represents the level at which operations take place, such as the document root or document
   * content (for example, /document or /document/content). The default is /document.
   */
  context?: string;
  /**
   * Inputs of the skills could be a column in the source data set, or the output of an upstream
   * skill.
   */
  inputs: InputFieldMappingEntry[];
  /**
   * The output of a skill is either a field in a search index, or a value that can be consumed as
   * an input by another skill.
   */
  outputs: OutputFieldMappingEntry[];
  /**
   * A value indicating which algorithm to use for extracting text. Default is printed. Possible
   * values include: 'printed', 'handwritten'
   */
  textExtractionAlgorithm?: TextExtractionAlgorithm;
  /**
   * A value indicating which language code to use. Default is en. Possible values include:
   * 'zh-Hans', 'zh-Hant', 'cs', 'da', 'nl', 'en', 'fi', 'fr', 'de', 'el', 'hu', 'it', 'ja', 'ko',
   * 'nb', 'pl', 'pt', 'ru', 'es', 'sv', 'tr', 'ar', 'ro', 'sr-Cyrl', 'sr-Latn', 'sk'
   */
  defaultLanguageCode?: OcrSkillLanguage;
  /**
   * A value indicating to turn orientation detection on or not. Default is false. Default value:
   * false.
   */
  shouldDetectOrientation?: boolean;
}

/**
 * A skill that analyzes image files. It extracts a rich set of visual features based on the image
 * content.
 */
export interface ImageAnalysisSkill {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Skills.Vision.ImageAnalysisSkill";
  /**
   * The name of the skill which uniquely identifies it within the skillset. A skill with no name
   * defined will be given a default name of its 1-based index in the skills array, prefixed with
   * the character '#'.
   */
  name?: string;
  /**
   * The description of the skill which describes the inputs, outputs, and usage of the skill.
   */
  description?: string;
  /**
   * Represents the level at which operations take place, such as the document root or document
   * content (for example, /document or /document/content). The default is /document.
   */
  context?: string;
  /**
   * Inputs of the skills could be a column in the source data set, or the output of an upstream
   * skill.
   */
  inputs: InputFieldMappingEntry[];
  /**
   * The output of a skill is either a field in a search index, or a value that can be consumed as
   * an input by another skill.
   */
  outputs: OutputFieldMappingEntry[];
  /**
   * A value indicating which language code to use. Default is en. Possible values include: 'en',
   * 'es', 'ja', 'pt', 'zh'
   */
  defaultLanguageCode?: ImageAnalysisSkillLanguage;
  /**
   * A list of visual features.
   */
  visualFeatures?: VisualFeature[];
  /**
   * A string indicating which domain-specific details to return.
   */
  details?: ImageDetail[];
}

/**
 * A skill that detects the language of input text and reports a single language code for every
 * document submitted on the request. The language code is paired with a score indicating the
 * confidence of the analysis.
 */
export interface LanguageDetectionSkill {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Skills.Text.LanguageDetectionSkill";
  /**
   * The name of the skill which uniquely identifies it within the skillset. A skill with no name
   * defined will be given a default name of its 1-based index in the skills array, prefixed with
   * the character '#'.
   */
  name?: string;
  /**
   * The description of the skill which describes the inputs, outputs, and usage of the skill.
   */
  description?: string;
  /**
   * Represents the level at which operations take place, such as the document root or document
   * content (for example, /document or /document/content). The default is /document.
   */
  context?: string;
  /**
   * Inputs of the skills could be a column in the source data set, or the output of an upstream
   * skill.
   */
  inputs: InputFieldMappingEntry[];
  /**
   * The output of a skill is either a field in a search index, or a value that can be consumed as
   * an input by another skill.
   */
  outputs: OutputFieldMappingEntry[];
}

/**
 * A skill for reshaping the outputs. It creates a complex type to support composite fields (also
 * known as multipart fields).
 */
export interface ShaperSkill {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Skills.Util.ShaperSkill";
  /**
   * The name of the skill which uniquely identifies it within the skillset. A skill with no name
   * defined will be given a default name of its 1-based index in the skills array, prefixed with
   * the character '#'.
   */
  name?: string;
  /**
   * The description of the skill which describes the inputs, outputs, and usage of the skill.
   */
  description?: string;
  /**
   * Represents the level at which operations take place, such as the document root or document
   * content (for example, /document or /document/content). The default is /document.
   */
  context?: string;
  /**
   * Inputs of the skills could be a column in the source data set, or the output of an upstream
   * skill.
   */
  inputs: InputFieldMappingEntry[];
  /**
   * The output of a skill is either a field in a search index, or a value that can be consumed as
   * an input by another skill.
   */
  outputs: OutputFieldMappingEntry[];
}

/**
 * A skill for merging two or more strings into a single unified string, with an optional
 * user-defined delimiter separating each component part.
 */
export interface MergeSkill {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Skills.Text.MergeSkill";
  /**
   * The name of the skill which uniquely identifies it within the skillset. A skill with no name
   * defined will be given a default name of its 1-based index in the skills array, prefixed with
   * the character '#'.
   */
  name?: string;
  /**
   * The description of the skill which describes the inputs, outputs, and usage of the skill.
   */
  description?: string;
  /**
   * Represents the level at which operations take place, such as the document root or document
   * content (for example, /document or /document/content). The default is /document.
   */
  context?: string;
  /**
   * Inputs of the skills could be a column in the source data set, or the output of an upstream
   * skill.
   */
  inputs: InputFieldMappingEntry[];
  /**
   * The output of a skill is either a field in a search index, or a value that can be consumed as
   * an input by another skill.
   */
  outputs: OutputFieldMappingEntry[];
  /**
   * The tag indicates the start of the merged text. By default, the tag is an empty space. Default
   * value: ''.
   */
  insertPreTag?: string;
  /**
   * The tag indicates the end of the merged text. By default, the tag is an empty space. Default
   * value: ''.
   */
  insertPostTag?: string;
}

/**
 * Text analytics entity recognition.
 */
export interface EntityRecognitionSkill {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Skills.Text.EntityRecognitionSkill";
  /**
   * The name of the skill which uniquely identifies it within the skillset. A skill with no name
   * defined will be given a default name of its 1-based index in the skills array, prefixed with
   * the character '#'.
   */
  name?: string;
  /**
   * The description of the skill which describes the inputs, outputs, and usage of the skill.
   */
  description?: string;
  /**
   * Represents the level at which operations take place, such as the document root or document
   * content (for example, /document or /document/content). The default is /document.
   */
  context?: string;
  /**
   * Inputs of the skills could be a column in the source data set, or the output of an upstream
   * skill.
   */
  inputs: InputFieldMappingEntry[];
  /**
   * The output of a skill is either a field in a search index, or a value that can be consumed as
   * an input by another skill.
   */
  outputs: OutputFieldMappingEntry[];
  /**
   * A list of entity categories that should be extracted.
   */
  categories?: EntityCategory[];
  /**
   * A value indicating which language code to use. Default is en. Possible values include: 'ar',
   * 'cs', 'zh-Hans', 'zh-Hant', 'da', 'nl', 'en', 'fi', 'fr', 'de', 'el', 'hu', 'it', 'ja', 'ko',
   * 'no', 'pl', 'pt-PT', 'pt-BR', 'ru', 'es', 'sv', 'tr'
   */
  defaultLanguageCode?: EntityRecognitionSkillLanguage;
  /**
   * Determines whether or not to include entities which are well known but don't conform to a
   * pre-defined type. If this configuration is not set (default), set to null or set to false,
   * entities which don't conform to one of the pre-defined types will not be surfaced.
   */
  includeTypelessEntities?: boolean;
  /**
   * A value between 0 and 1 that be used to only include entities whose confidence score is
   * greater than the value specified. If not set (default), or if explicitly set to null, all
   * entities will be included.
   */
  minimumPrecision?: number;
}

/**
 * Text analytics positive-negative sentiment analysis, scored as a floating point value in a range
 * of zero to 1.
 */
export interface SentimentSkill {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Skills.Text.SentimentSkill";
  /**
   * The name of the skill which uniquely identifies it within the skillset. A skill with no name
   * defined will be given a default name of its 1-based index in the skills array, prefixed with
   * the character '#'.
   */
  name?: string;
  /**
   * The description of the skill which describes the inputs, outputs, and usage of the skill.
   */
  description?: string;
  /**
   * Represents the level at which operations take place, such as the document root or document
   * content (for example, /document or /document/content). The default is /document.
   */
  context?: string;
  /**
   * Inputs of the skills could be a column in the source data set, or the output of an upstream
   * skill.
   */
  inputs: InputFieldMappingEntry[];
  /**
   * The output of a skill is either a field in a search index, or a value that can be consumed as
   * an input by another skill.
   */
  outputs: OutputFieldMappingEntry[];
  /**
   * A value indicating which language code to use. Default is en. Possible values include: 'da',
   * 'nl', 'en', 'fi', 'fr', 'de', 'el', 'it', 'no', 'pl', 'pt-PT', 'ru', 'es', 'sv', 'tr'
   */
  defaultLanguageCode?: SentimentSkillLanguage;
}

/**
 * A skill to split a string into chunks of text.
 */
export interface SplitSkill {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Skills.Text.SplitSkill";
  /**
   * The name of the skill which uniquely identifies it within the skillset. A skill with no name
   * defined will be given a default name of its 1-based index in the skills array, prefixed with
   * the character '#'.
   */
  name?: string;
  /**
   * The description of the skill which describes the inputs, outputs, and usage of the skill.
   */
  description?: string;
  /**
   * Represents the level at which operations take place, such as the document root or document
   * content (for example, /document or /document/content). The default is /document.
   */
  context?: string;
  /**
   * Inputs of the skills could be a column in the source data set, or the output of an upstream
   * skill.
   */
  inputs: InputFieldMappingEntry[];
  /**
   * The output of a skill is either a field in a search index, or a value that can be consumed as
   * an input by another skill.
   */
  outputs: OutputFieldMappingEntry[];
  /**
   * A value indicating which language code to use. Default is en. Possible values include: 'da',
   * 'de', 'en', 'es', 'fi', 'fr', 'it', 'ko', 'pt'
   */
  defaultLanguageCode?: SplitSkillLanguage;
  /**
   * A value indicating which split mode to perform. Possible values include: 'pages', 'sentences'
   */
  textSplitMode?: TextSplitMode;
  /**
   * The desired maximum page length. Default is 10000.
   */
  maximumPageLength?: number;
}

/**
 * A skill to translate text from one language to another.
 */
export interface TextTranslationSkill {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Skills.Text.TranslationSkill";
  /**
   * The name of the skill which uniquely identifies it within the skillset. A skill with no name
   * defined will be given a default name of its 1-based index in the skills array, prefixed with
   * the character '#'.
   */
  name?: string;
  /**
   * The description of the skill which describes the inputs, outputs, and usage of the skill.
   */
  description?: string;
  /**
   * Represents the level at which operations take place, such as the document root or document
   * content (for example, /document or /document/content). The default is /document.
   */
  context?: string;
  /**
   * Inputs of the skills could be a column in the source data set, or the output of an upstream
   * skill.
   */
  inputs: InputFieldMappingEntry[];
  /**
   * The output of a skill is either a field in a search index, or a value that can be consumed as
   * an input by another skill.
   */
  outputs: OutputFieldMappingEntry[];
  /**
   * The language code to translate documents into for documents that don't specify the to language
   * explicitly. Possible values include: 'af', 'ar', 'bn', 'bs', 'bg', 'yue', 'ca', 'zh-Hans',
   * 'zh-Hant', 'hr', 'cs', 'da', 'nl', 'en', 'et', 'fj', 'fil', 'fi', 'fr', 'de', 'el', 'ht',
   * 'he', 'hi', 'mww', 'hu', 'is', 'id', 'it', 'ja', 'sw', 'tlh', 'ko', 'lv', 'lt', 'mg', 'ms',
   * 'mt', 'nb', 'fa', 'pl', 'pt', 'otq', 'ro', 'ru', 'sm', 'sr-Cyrl', 'sr-Latn', 'sk', 'sl', 'es',
   * 'sv', 'ty', 'ta', 'te', 'th', 'to', 'tr', 'uk', 'ur', 'vi', 'cy', 'yua'
   */
  defaultToLanguageCode: TextTranslationSkillLanguage;
  /**
   * The language code to translate documents from for documents that don't specify the from
   * language explicitly. Possible values include: 'af', 'ar', 'bn', 'bs', 'bg', 'yue', 'ca',
   * 'zh-Hans', 'zh-Hant', 'hr', 'cs', 'da', 'nl', 'en', 'et', 'fj', 'fil', 'fi', 'fr', 'de', 'el',
   * 'ht', 'he', 'hi', 'mww', 'hu', 'is', 'id', 'it', 'ja', 'sw', 'tlh', 'ko', 'lv', 'lt', 'mg',
   * 'ms', 'mt', 'nb', 'fa', 'pl', 'pt', 'otq', 'ro', 'ru', 'sm', 'sr-Cyrl', 'sr-Latn', 'sk', 'sl',
   * 'es', 'sv', 'ty', 'ta', 'te', 'th', 'to', 'tr', 'uk', 'ur', 'vi', 'cy', 'yua'
   */
  defaultFromLanguageCode?: TextTranslationSkillLanguage;
  /**
   * The language code to translate documents from when neither the fromLanguageCode input nor the
   * defaultFromLanguageCode parameter are provided, and the automatic language detection is
   * unsuccessful. Default is en. Possible values include: 'af', 'ar', 'bn', 'bs', 'bg', 'yue',
   * 'ca', 'zh-Hans', 'zh-Hant', 'hr', 'cs', 'da', 'nl', 'en', 'et', 'fj', 'fil', 'fi', 'fr', 'de',
   * 'el', 'ht', 'he', 'hi', 'mww', 'hu', 'is', 'id', 'it', 'ja', 'sw', 'tlh', 'ko', 'lv', 'lt',
   * 'mg', 'ms', 'mt', 'nb', 'fa', 'pl', 'pt', 'otq', 'ro', 'ru', 'sm', 'sr-Cyrl', 'sr-Latn', 'sk',
   * 'sl', 'es', 'sv', 'ty', 'ta', 'te', 'th', 'to', 'tr', 'uk', 'ur', 'vi', 'cy', 'yua'
   */
  suggestedFrom?: TextTranslationSkillLanguage;
}

/**
 * A skill that can call a Web API endpoint, allowing you to extend a skillset by having it call
 * your custom code.
 */
export interface WebApiSkill {
  /**
   * Polymorphic Discriminator
   */
  odatatype: "#Microsoft.Skills.Custom.WebApiSkill";
  /**
   * The name of the skill which uniquely identifies it within the skillset. A skill with no name
   * defined will be given a default name of its 1-based index in the skills array, prefixed with
   * the character '#'.
   */
  name?: string;
  /**
   * The description of the skill which describes the inputs, outputs, and usage of the skill.
   */
  description?: string;
  /**
   * Represents the level at which operations take place, such as the document root or document
   * content (for example, /document or /document/content). The default is /document.
   */
  context?: string;
  /**
   * Inputs of the skills could be a column in the source data set, or the output of an upstream
   * skill.
   */
  inputs: InputFieldMappingEntry[];
  /**
   * The output of a skill is either a field in a search index, or a value that can be consumed as
   * an input by another skill.
   */
  outputs: OutputFieldMappingEntry[];
  /**
   * The url for the Web API.
   */
  uri: string;
  /**
   * The headers required to make the http request.
   */
  httpHeaders?: { [propertyName: string]: string };
  /**
   * The method for the http request.
   */
  httpMethod?: string;
  /**
   * The desired timeout for the request. Default is 30 seconds.
   */
  timeout?: string;
  /**
   * The desired batch size which indicates number of documents.
   */
  batchSize?: number;
  /**
   * If set, the number of parallel calls that can be made to the Web API.
   */
  degreeOfParallelism?: number;
}

/**
 * Response from a list Skillset request. If successful, it includes the full definitions of all
 * skillsets.
 */
export interface ListSkillsetsResult {
  /**
   * The skillsets defined in the Search service.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly skillsets: Skillset[];
}

/**
 * Represents a synonym map definition.
 */
export interface SynonymMap {
  /**
   * The name of the synonym map.
   */
  name: string;
  /**
   * A series of synonym rules in the specified synonym map format. The rules must be separated by
   * newlines.
   */
  synonyms: string;
  /**
   * A description of an encryption key that you create in Azure Key Vault. This key is used to
   * provide an additional level of encryption-at-rest for your data when you want full assurance
   * that no one, not even Microsoft, can decrypt your data in Azure Cognitive Search. Once you
   * have encrypted your data, it will always remain encrypted. Azure Cognitive Search will ignore
   * attempts to set this property to null. You can change this property as needed if you want to
   * rotate your encryption key; Your data will be unaffected. Encryption with customer-managed
   * keys is not available for free search services, and is only available for paid services
   * created on or after January 1, 2019.
   */
  encryptionKey?: EncryptionKey;
  /**
   * The ETag of the synonym map.
   */
  etag?: string;
}

/**
 * Response from a List SynonymMaps request. If successful, it includes the full definitions of all
 * synonym maps.
 */
export interface ListSynonymMapsResult {
  /**
   * The synonym maps in the Search service.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly synonymMaps: SynonymMap[];
}

/**
 * Represents a resource's usage and quota.
 */
export interface ResourceCounter {
  /**
   * The resource usage amount.
   */
  usage: number;
  /**
   * The resource amount quota.
   */
  quota?: number;
}

/**
 * Represents service-level resource counters and quotas.
 */
export interface ServiceCounters {
  /**
   * Total number of documents across all indexes in the service.
   */
  documentCounter: ResourceCounter;
  /**
   * Total number of indexes.
   */
  indexCounter: ResourceCounter;
  /**
   * Total number of indexers.
   */
  indexerCounter: ResourceCounter;
  /**
   * Total number of data sources.
   */
  dataSourceCounter: ResourceCounter;
  /**
   * Total size of used storage in bytes.
   */
  storageSizeCounter: ResourceCounter;
  /**
   * Total number of synonym maps.
   */
  synonymMapCounter: ResourceCounter;
  /**
   * Total number of skillsets.
   */
  skillsetCounter: ResourceCounter;
}

/**
 * Represents various service level limits.
 */
export interface ServiceLimits {
  /**
   * The maximum allowed fields per index.
   */
  maxFieldsPerIndex?: number;
  /**
   * The maximum depth which you can nest sub-fields in an index, including the top-level complex
   * field. For example, a/b/c has a nesting depth of 3.
   */
  maxFieldNestingDepthPerIndex?: number;
  /**
   * The maximum number of fields of type Collection(Edm.ComplexType) allowed in an index.
   */
  maxComplexCollectionFieldsPerIndex?: number;
  /**
   * The maximum number of objects in complex collections allowed per document.
   */
  maxComplexObjectsInCollectionsPerDocument?: number;
}

/**
 * Response from a get service statistics request. If successful, it includes service level
 * counters and limits.
 */
export interface ServiceStatistics {
  /**
   * Service level resource counters.
   */
  counters: ServiceCounters;
  /**
   * Service level general limits.
   */
  limits: ServiceLimits;
}

/**
 * Describes an error condition for the Azure Cognitive Search API.
 */
export interface SearchError {
  /**
   * One of a server-defined set of error codes.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly code?: string;
  /**
   * A human-readable representation of the error.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly message: string;
  /**
   * An array of details about specific errors that led to this reported error.
   * **NOTE: This property will not be serialized. It can only be populated by the server.**
   */
  readonly details?: SearchError[];
}

/**
 * Additional parameters for a set of operations.
 */
export interface AccessCondition {
  /**
   * Defines the If-Match condition. The operation will be performed only if the ETag on the server
   * matches this value.
   */
  ifMatch?: string;
  /**
   * Defines the If-None-Match condition. The operation will be performed only if the ETag on the
   * server does not match this value.
   */
  ifNoneMatch?: string;
}

/**
 * Optional Parameters.
 */
export interface DataSourcesCreateOrUpdateOptionalParams extends coreHttp.RequestOptionsBase {
  /**
   * Additional parameters for the operation
   */
  accessCondition?: AccessCondition;
}

/**
 * Optional Parameters.
 */
export interface DataSourcesDeleteMethodOptionalParams extends coreHttp.RequestOptionsBase {
  /**
   * Additional parameters for the operation
   */
  accessCondition?: AccessCondition;
}

/**
 * Optional Parameters.
 */
export interface DataSourcesListOptionalParams extends coreHttp.RequestOptionsBase {
  /**
   * Selects which top-level properties of the data sources to retrieve. Specified as a
   * comma-separated list of JSON property names, or '*' for all properties. The default is all
   * properties.
   */
  select?: string;
}

/**
 * Optional Parameters.
 */
export interface IndexersCreateOrUpdateOptionalParams extends coreHttp.RequestOptionsBase {
  /**
   * Additional parameters for the operation
   */
  accessCondition?: AccessCondition;
}

/**
 * Optional Parameters.
 */
export interface IndexersDeleteMethodOptionalParams extends coreHttp.RequestOptionsBase {
  /**
   * Additional parameters for the operation
   */
  accessCondition?: AccessCondition;
}

/**
 * Optional Parameters.
 */
export interface IndexersListOptionalParams extends coreHttp.RequestOptionsBase {
  /**
   * Selects which top-level properties of the indexers to retrieve. Specified as a comma-separated
   * list of JSON property names, or '*' for all properties. The default is all properties.
   */
  select?: string;
}

/**
 * Optional Parameters.
 */
export interface SkillsetsCreateOrUpdateOptionalParams extends coreHttp.RequestOptionsBase {
  /**
   * Additional parameters for the operation
   */
  accessCondition?: AccessCondition;
}

/**
 * Optional Parameters.
 */
export interface SkillsetsDeleteMethodOptionalParams extends coreHttp.RequestOptionsBase {
  /**
   * Additional parameters for the operation
   */
  accessCondition?: AccessCondition;
}

/**
 * Optional Parameters.
 */
export interface SkillsetsListOptionalParams extends coreHttp.RequestOptionsBase {
  /**
   * Selects which top-level properties of the skillsets to retrieve. Specified as a
   * comma-separated list of JSON property names, or '*' for all properties. The default is all
   * properties.
   */
  select?: string;
}

/**
 * Optional Parameters.
 */
export interface SynonymMapsCreateOrUpdateOptionalParams extends coreHttp.RequestOptionsBase {
  /**
   * Additional parameters for the operation
   */
  accessCondition?: AccessCondition;
}

/**
 * Optional Parameters.
 */
export interface SynonymMapsDeleteMethodOptionalParams extends coreHttp.RequestOptionsBase {
  /**
   * Additional parameters for the operation
   */
  accessCondition?: AccessCondition;
}

/**
 * Optional Parameters.
 */
export interface SynonymMapsListOptionalParams extends coreHttp.RequestOptionsBase {
  /**
   * Selects which top-level properties of the synonym maps to retrieve. Specified as a
   * comma-separated list of JSON property names, or '*' for all properties. The default is all
   * properties.
   */
  select?: string;
}

/**
 * Optional Parameters.
 */
export interface IndexesListOptionalParams extends coreHttp.RequestOptionsBase {
  /**
   * Selects which top-level properties of the index definitions to retrieve. Specified as a
   * comma-separated list of JSON property names, or '*' for all properties. The default is all
   * properties.
   */
  select?: string;
}

/**
 * Optional Parameters.
 */
export interface IndexesCreateOrUpdateOptionalParams extends coreHttp.RequestOptionsBase {
  /**
   * Allows new analyzers, tokenizers, token filters, or char filters to be added to an index by
   * taking the index offline for at least a few seconds. This temporarily causes indexing and
   * query requests to fail. Performance and write availability of the index can be impaired for
   * several minutes after the index is updated, or longer for very large indexes.
   */
  allowIndexDowntime?: boolean;
  /**
   * Additional parameters for the operation
   */
  accessCondition?: AccessCondition;
}

/**
 * Optional Parameters.
 */
export interface IndexesDeleteMethodOptionalParams extends coreHttp.RequestOptionsBase {
  /**
   * Additional parameters for the operation
   */
  accessCondition?: AccessCondition;
}

/**
 * Defines values for AnalyzerName.
 * Possible values include: 'ar.microsoft', 'ar.lucene', 'hy.lucene', 'bn.microsoft', 'eu.lucene',
 * 'bg.microsoft', 'bg.lucene', 'ca.microsoft', 'ca.lucene', 'zh-Hans.microsoft', 'zh-Hans.lucene',
 * 'zh-Hant.microsoft', 'zh-Hant.lucene', 'hr.microsoft', 'cs.microsoft', 'cs.lucene',
 * 'da.microsoft', 'da.lucene', 'nl.microsoft', 'nl.lucene', 'en.microsoft', 'en.lucene',
 * 'et.microsoft', 'fi.microsoft', 'fi.lucene', 'fr.microsoft', 'fr.lucene', 'gl.lucene',
 * 'de.microsoft', 'de.lucene', 'el.microsoft', 'el.lucene', 'gu.microsoft', 'he.microsoft',
 * 'hi.microsoft', 'hi.lucene', 'hu.microsoft', 'hu.lucene', 'is.microsoft', 'id.microsoft',
 * 'id.lucene', 'ga.lucene', 'it.microsoft', 'it.lucene', 'ja.microsoft', 'ja.lucene',
 * 'kn.microsoft', 'ko.microsoft', 'ko.lucene', 'lv.microsoft', 'lv.lucene', 'lt.microsoft',
 * 'ml.microsoft', 'ms.microsoft', 'mr.microsoft', 'nb.microsoft', 'no.lucene', 'fa.lucene',
 * 'pl.microsoft', 'pl.lucene', 'pt-BR.microsoft', 'pt-BR.lucene', 'pt-PT.microsoft',
 * 'pt-PT.lucene', 'pa.microsoft', 'ro.microsoft', 'ro.lucene', 'ru.microsoft', 'ru.lucene',
 * 'sr-cyrillic.microsoft', 'sr-latin.microsoft', 'sk.microsoft', 'sl.microsoft', 'es.microsoft',
 * 'es.lucene', 'sv.microsoft', 'sv.lucene', 'ta.microsoft', 'te.microsoft', 'th.microsoft',
 * 'th.lucene', 'tr.microsoft', 'tr.lucene', 'uk.microsoft', 'ur.microsoft', 'vi.microsoft',
 * 'standard.lucene', 'standardasciifolding.lucene', 'keyword', 'pattern', 'simple', 'stop',
 * 'whitespace'
 * @readonly
 * @enum {string}
 */
export type AnalyzerName = 'ar.microsoft' | 'ar.lucene' | 'hy.lucene' | 'bn.microsoft' | 'eu.lucene' | 'bg.microsoft' | 'bg.lucene' | 'ca.microsoft' | 'ca.lucene' | 'zh-Hans.microsoft' | 'zh-Hans.lucene' | 'zh-Hant.microsoft' | 'zh-Hant.lucene' | 'hr.microsoft' | 'cs.microsoft' | 'cs.lucene' | 'da.microsoft' | 'da.lucene' | 'nl.microsoft' | 'nl.lucene' | 'en.microsoft' | 'en.lucene' | 'et.microsoft' | 'fi.microsoft' | 'fi.lucene' | 'fr.microsoft' | 'fr.lucene' | 'gl.lucene' | 'de.microsoft' | 'de.lucene' | 'el.microsoft' | 'el.lucene' | 'gu.microsoft' | 'he.microsoft' | 'hi.microsoft' | 'hi.lucene' | 'hu.microsoft' | 'hu.lucene' | 'is.microsoft' | 'id.microsoft' | 'id.lucene' | 'ga.lucene' | 'it.microsoft' | 'it.lucene' | 'ja.microsoft' | 'ja.lucene' | 'kn.microsoft' | 'ko.microsoft' | 'ko.lucene' | 'lv.microsoft' | 'lv.lucene' | 'lt.microsoft' | 'ml.microsoft' | 'ms.microsoft' | 'mr.microsoft' | 'nb.microsoft' | 'no.lucene' | 'fa.lucene' | 'pl.microsoft' | 'pl.lucene' | 'pt-BR.microsoft' | 'pt-BR.lucene' | 'pt-PT.microsoft' | 'pt-PT.lucene' | 'pa.microsoft' | 'ro.microsoft' | 'ro.lucene' | 'ru.microsoft' | 'ru.lucene' | 'sr-cyrillic.microsoft' | 'sr-latin.microsoft' | 'sk.microsoft' | 'sl.microsoft' | 'es.microsoft' | 'es.lucene' | 'sv.microsoft' | 'sv.lucene' | 'ta.microsoft' | 'te.microsoft' | 'th.microsoft' | 'th.lucene' | 'tr.microsoft' | 'tr.lucene' | 'uk.microsoft' | 'ur.microsoft' | 'vi.microsoft' | 'standard.lucene' | 'standardasciifolding.lucene' | 'keyword' | 'pattern' | 'simple' | 'stop' | 'whitespace';

/**
 * Defines values for TokenizerName.
 * Possible values include: 'Classic', 'EdgeNGram', 'Keyword', 'Letter', 'Lowercase',
 * 'MicrosoftLanguageTokenizer', 'MicrosoftLanguageStemmingTokenizer', 'NGram', 'PathHierarchy',
 * 'Pattern', 'Standard', 'UaxUrlEmail', 'Whitespace'
 * @readonly
 * @enum {string}
 */
export type TokenizerName = 'classic' | 'edgeNGram' | 'keyword_v2' | 'letter' | 'lowercase' | 'microsoft_language_tokenizer' | 'microsoft_language_stemming_tokenizer' | 'nGram' | 'path_hierarchy_v2' | 'pattern' | 'standard_v2' | 'uax_url_email' | 'whitespace';

/**
 * Defines values for TokenFilterName.
 * Possible values include: 'ArabicNormalization', 'Apostrophe', 'AsciiFolding', 'CjkBigram',
 * 'CjkWidth', 'Classic', 'CommonGram', 'EdgeNGram', 'Elision', 'GermanNormalization',
 * 'HindiNormalization', 'IndicNormalization', 'KeywordRepeat', 'KStem', 'Length', 'Limit',
 * 'Lowercase', 'NGram', 'PersianNormalization', 'Phonetic', 'PorterStem', 'Reverse',
 * 'ScandinavianNormalization', 'ScandinavianFoldingNormalization', 'Shingle', 'Snowball',
 * 'SoraniNormalization', 'Stemmer', 'Stopwords', 'Trim', 'Truncate', 'Unique', 'Uppercase',
 * 'WordDelimiter'
 * @readonly
 * @enum {string}
 */
export type TokenFilterName = 'arabic_normalization' | 'apostrophe' | 'asciifolding' | 'cjk_bigram' | 'cjk_width' | 'classic' | 'common_grams' | 'edgeNGram_v2' | 'elision' | 'german_normalization' | 'hindi_normalization' | 'indic_normalization' | 'keyword_repeat' | 'kstem' | 'length' | 'limit' | 'lowercase' | 'nGram_v2' | 'persian_normalization' | 'phonetic' | 'porter_stem' | 'reverse' | 'scandinavian_normalization' | 'scandinavian_folding' | 'shingle' | 'snowball' | 'sorani_normalization' | 'stemmer' | 'stopwords' | 'trim' | 'truncate' | 'unique' | 'uppercase' | 'word_delimiter';

/**
 * Defines values for CharFilterName.
 * Possible values include: 'HtmlStrip'
 * @readonly
 * @enum {string}
 */
export type CharFilterName = 'html_strip';

/**
 * Defines values for RegexFlags.
 * Possible values include: 'CANON_EQ', 'CASE_INSENSITIVE', 'COMMENTS', 'DOTALL', 'LITERAL',
 * 'MULTILINE', 'UNICODE_CASE', 'UNIX_LINES'
 * @readonly
 * @enum {string}
 */
export type RegexFlags = 'CANON_EQ' | 'CASE_INSENSITIVE' | 'COMMENTS' | 'DOTALL' | 'LITERAL' | 'MULTILINE' | 'UNICODE_CASE' | 'UNIX_LINES';

/**
 * Defines values for DataType.
 * Possible values include: 'Edm.String', 'Edm.Int32', 'Edm.Int64', 'Edm.Double', 'Edm.Boolean',
 * 'Edm.DateTimeOffset', 'Edm.GeographyPoint', 'Edm.ComplexType', 'Collection(Edm.String)',
 * 'Collection(Edm.Int32)', 'Collection(Edm.Int64)', 'Collection(Edm.Double)',
 * 'Collection(Edm.Boolean)', 'Collection(Edm.DateTimeOffset)', 'Collection(Edm.GeographyPoint)',
 * 'Collection(Edm.ComplexType)'
 * @readonly
 * @enum {string}
 */
export type DataType = 'Edm.String' | 'Edm.Int32' | 'Edm.Int64' | 'Edm.Double' | 'Edm.Boolean' | 'Edm.DateTimeOffset' | 'Edm.GeographyPoint' | 'Edm.ComplexType' | 'Collection(Edm.String)' | 'Collection(Edm.Int32)' | 'Collection(Edm.Int64)' | 'Collection(Edm.Double)' | 'Collection(Edm.Boolean)' | 'Collection(Edm.DateTimeOffset)' | 'Collection(Edm.GeographyPoint)' | 'Collection(Edm.ComplexType)';

/**
 * Defines values for TokenCharacterKind.
 * Possible values include: 'letter', 'digit', 'whitespace', 'punctuation', 'symbol'
 * @readonly
 * @enum {string}
 */
export type TokenCharacterKind = 'letter' | 'digit' | 'whitespace' | 'punctuation' | 'symbol';

/**
 * Defines values for MicrosoftTokenizerLanguage.
 * Possible values include: 'bangla', 'bulgarian', 'catalan', 'chineseSimplified',
 * 'chineseTraditional', 'croatian', 'czech', 'danish', 'dutch', 'english', 'french', 'german',
 * 'greek', 'gujarati', 'hindi', 'icelandic', 'indonesian', 'italian', 'japanese', 'kannada',
 * 'korean', 'malay', 'malayalam', 'marathi', 'norwegianBokmaal', 'polish', 'portuguese',
 * 'portugueseBrazilian', 'punjabi', 'romanian', 'russian', 'serbianCyrillic', 'serbianLatin',
 * 'slovenian', 'spanish', 'swedish', 'tamil', 'telugu', 'thai', 'ukrainian', 'urdu', 'vietnamese'
 * @readonly
 * @enum {string}
 */
export type MicrosoftTokenizerLanguage = 'bangla' | 'bulgarian' | 'catalan' | 'chineseSimplified' | 'chineseTraditional' | 'croatian' | 'czech' | 'danish' | 'dutch' | 'english' | 'french' | 'german' | 'greek' | 'gujarati' | 'hindi' | 'icelandic' | 'indonesian' | 'italian' | 'japanese' | 'kannada' | 'korean' | 'malay' | 'malayalam' | 'marathi' | 'norwegianBokmaal' | 'polish' | 'portuguese' | 'portugueseBrazilian' | 'punjabi' | 'romanian' | 'russian' | 'serbianCyrillic' | 'serbianLatin' | 'slovenian' | 'spanish' | 'swedish' | 'tamil' | 'telugu' | 'thai' | 'ukrainian' | 'urdu' | 'vietnamese';

/**
 * Defines values for MicrosoftStemmingTokenizerLanguage.
 * Possible values include: 'arabic', 'bangla', 'bulgarian', 'catalan', 'croatian', 'czech',
 * 'danish', 'dutch', 'english', 'estonian', 'finnish', 'french', 'german', 'greek', 'gujarati',
 * 'hebrew', 'hindi', 'hungarian', 'icelandic', 'indonesian', 'italian', 'kannada', 'latvian',
 * 'lithuanian', 'malay', 'malayalam', 'marathi', 'norwegianBokmaal', 'polish', 'portuguese',
 * 'portugueseBrazilian', 'punjabi', 'romanian', 'russian', 'serbianCyrillic', 'serbianLatin',
 * 'slovak', 'slovenian', 'spanish', 'swedish', 'tamil', 'telugu', 'turkish', 'ukrainian', 'urdu'
 * @readonly
 * @enum {string}
 */
export type MicrosoftStemmingTokenizerLanguage = 'arabic' | 'bangla' | 'bulgarian' | 'catalan' | 'croatian' | 'czech' | 'danish' | 'dutch' | 'english' | 'estonian' | 'finnish' | 'french' | 'german' | 'greek' | 'gujarati' | 'hebrew' | 'hindi' | 'hungarian' | 'icelandic' | 'indonesian' | 'italian' | 'kannada' | 'latvian' | 'lithuanian' | 'malay' | 'malayalam' | 'marathi' | 'norwegianBokmaal' | 'polish' | 'portuguese' | 'portugueseBrazilian' | 'punjabi' | 'romanian' | 'russian' | 'serbianCyrillic' | 'serbianLatin' | 'slovak' | 'slovenian' | 'spanish' | 'swedish' | 'tamil' | 'telugu' | 'turkish' | 'ukrainian' | 'urdu';

/**
 * Defines values for CjkBigramTokenFilterScripts.
 * Possible values include: 'han', 'hiragana', 'katakana', 'hangul'
 * @readonly
 * @enum {string}
 */
export type CjkBigramTokenFilterScripts = 'han' | 'hiragana' | 'katakana' | 'hangul';

/**
 * Defines values for EdgeNGramTokenFilterSide.
 * Possible values include: 'front', 'back'
 * @readonly
 * @enum {string}
 */
export type EdgeNGramTokenFilterSide = 'front' | 'back';

/**
 * Defines values for PhoneticEncoder.
 * Possible values include: 'metaphone', 'doubleMetaphone', 'soundex', 'refinedSoundex',
 * 'caverphone1', 'caverphone2', 'cologne', 'nysiis', 'koelnerPhonetik', 'haasePhonetik',
 * 'beiderMorse'
 * @readonly
 * @enum {string}
 */
export type PhoneticEncoder = 'metaphone' | 'doubleMetaphone' | 'soundex' | 'refinedSoundex' | 'caverphone1' | 'caverphone2' | 'cologne' | 'nysiis' | 'koelnerPhonetik' | 'haasePhonetik' | 'beiderMorse';

/**
 * Defines values for SnowballTokenFilterLanguage.
 * Possible values include: 'armenian', 'basque', 'catalan', 'danish', 'dutch', 'english',
 * 'finnish', 'french', 'german', 'german2', 'hungarian', 'italian', 'kp', 'lovins', 'norwegian',
 * 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish', 'turkish'
 * @readonly
 * @enum {string}
 */
export type SnowballTokenFilterLanguage = 'armenian' | 'basque' | 'catalan' | 'danish' | 'dutch' | 'english' | 'finnish' | 'french' | 'german' | 'german2' | 'hungarian' | 'italian' | 'kp' | 'lovins' | 'norwegian' | 'porter' | 'portuguese' | 'romanian' | 'russian' | 'spanish' | 'swedish' | 'turkish';

/**
 * Defines values for StemmerTokenFilterLanguage.
 * Possible values include: 'arabic', 'armenian', 'basque', 'brazilian', 'bulgarian', 'catalan',
 * 'czech', 'danish', 'dutch', 'dutchKp', 'english', 'lightEnglish', 'minimalEnglish',
 * 'possessiveEnglish', 'porter2', 'lovins', 'finnish', 'lightFinnish', 'french', 'lightFrench',
 * 'minimalFrench', 'galician', 'minimalGalician', 'german', 'german2', 'lightGerman',
 * 'minimalGerman', 'greek', 'hindi', 'hungarian', 'lightHungarian', 'indonesian', 'irish',
 * 'italian', 'lightItalian', 'sorani', 'latvian', 'norwegian', 'lightNorwegian',
 * 'minimalNorwegian', 'lightNynorsk', 'minimalNynorsk', 'portuguese', 'lightPortuguese',
 * 'minimalPortuguese', 'portugueseRslp', 'romanian', 'russian', 'lightRussian', 'spanish',
 * 'lightSpanish', 'swedish', 'lightSwedish', 'turkish'
 * @readonly
 * @enum {string}
 */
export type StemmerTokenFilterLanguage = 'arabic' | 'armenian' | 'basque' | 'brazilian' | 'bulgarian' | 'catalan' | 'czech' | 'danish' | 'dutch' | 'dutchKp' | 'english' | 'lightEnglish' | 'minimalEnglish' | 'possessiveEnglish' | 'porter2' | 'lovins' | 'finnish' | 'lightFinnish' | 'french' | 'lightFrench' | 'minimalFrench' | 'galician' | 'minimalGalician' | 'german' | 'german2' | 'lightGerman' | 'minimalGerman' | 'greek' | 'hindi' | 'hungarian' | 'lightHungarian' | 'indonesian' | 'irish' | 'italian' | 'lightItalian' | 'sorani' | 'latvian' | 'norwegian' | 'lightNorwegian' | 'minimalNorwegian' | 'lightNynorsk' | 'minimalNynorsk' | 'portuguese' | 'lightPortuguese' | 'minimalPortuguese' | 'portugueseRslp' | 'romanian' | 'russian' | 'lightRussian' | 'spanish' | 'lightSpanish' | 'swedish' | 'lightSwedish' | 'turkish';

/**
 * Defines values for StopwordsList.
 * Possible values include: 'arabic', 'armenian', 'basque', 'brazilian', 'bulgarian', 'catalan',
 * 'czech', 'danish', 'dutch', 'english', 'finnish', 'french', 'galician', 'german', 'greek',
 * 'hindi', 'hungarian', 'indonesian', 'irish', 'italian', 'latvian', 'norwegian', 'persian',
 * 'portuguese', 'romanian', 'russian', 'sorani', 'spanish', 'swedish', 'thai', 'turkish'
 * @readonly
 * @enum {string}
 */
export type StopwordsList = 'arabic' | 'armenian' | 'basque' | 'brazilian' | 'bulgarian' | 'catalan' | 'czech' | 'danish' | 'dutch' | 'english' | 'finnish' | 'french' | 'galician' | 'german' | 'greek' | 'hindi' | 'hungarian' | 'indonesian' | 'irish' | 'italian' | 'latvian' | 'norwegian' | 'persian' | 'portuguese' | 'romanian' | 'russian' | 'sorani' | 'spanish' | 'swedish' | 'thai' | 'turkish';

/**
 * Defines values for DataSourceType.
 * Possible values include: 'AzureSql', 'CosmosDb', 'AzureBlob', 'AzureTable', 'MySql'
 * @readonly
 * @enum {string}
 */
export type DataSourceType = 'azuresql' | 'cosmosdb' | 'azureblob' | 'azuretable' | 'mysql';

/**
 * Defines values for IndexerExecutionStatus.
 * Possible values include: 'transientFailure', 'success', 'inProgress', 'reset'
 * @readonly
 * @enum {string}
 */
export type IndexerExecutionStatus = 'transientFailure' | 'success' | 'inProgress' | 'reset';

/**
 * Defines values for IndexerStatus.
 * Possible values include: 'unknown', 'error', 'running'
 * @readonly
 * @enum {string}
 */
export type IndexerStatus = 'unknown' | 'error' | 'running';

/**
 * Defines values for ScoringFunctionInterpolation.
 * Possible values include: 'linear', 'constant', 'quadratic', 'logarithmic'
 * @readonly
 * @enum {string}
 */
export type ScoringFunctionInterpolation = 'linear' | 'constant' | 'quadratic' | 'logarithmic';

/**
 * Defines values for ScoringFunctionAggregation.
 * Possible values include: 'sum', 'average', 'minimum', 'maximum', 'firstMatching'
 * @readonly
 * @enum {string}
 */
export type ScoringFunctionAggregation = 'sum' | 'average' | 'minimum' | 'maximum' | 'firstMatching';

/**
 * Defines values for KeyPhraseExtractionSkillLanguage.
 * Possible values include: 'da', 'nl', 'en', 'fi', 'fr', 'de', 'it', 'ja', 'ko', 'no', 'pl',
 * 'pt-PT', 'pt-BR', 'ru', 'es', 'sv'
 * @readonly
 * @enum {string}
 */
export type KeyPhraseExtractionSkillLanguage = 'da' | 'nl' | 'en' | 'fi' | 'fr' | 'de' | 'it' | 'ja' | 'ko' | 'no' | 'pl' | 'pt-PT' | 'pt-BR' | 'ru' | 'es' | 'sv';

/**
 * Defines values for TextExtractionAlgorithm.
 * Possible values include: 'printed', 'handwritten'
 * @readonly
 * @enum {string}
 */
export type TextExtractionAlgorithm = 'printed' | 'handwritten';

/**
 * Defines values for OcrSkillLanguage.
 * Possible values include: 'zh-Hans', 'zh-Hant', 'cs', 'da', 'nl', 'en', 'fi', 'fr', 'de', 'el',
 * 'hu', 'it', 'ja', 'ko', 'nb', 'pl', 'pt', 'ru', 'es', 'sv', 'tr', 'ar', 'ro', 'sr-Cyrl',
 * 'sr-Latn', 'sk'
 * @readonly
 * @enum {string}
 */
export type OcrSkillLanguage = 'zh-Hans' | 'zh-Hant' | 'cs' | 'da' | 'nl' | 'en' | 'fi' | 'fr' | 'de' | 'el' | 'hu' | 'it' | 'ja' | 'ko' | 'nb' | 'pl' | 'pt' | 'ru' | 'es' | 'sv' | 'tr' | 'ar' | 'ro' | 'sr-Cyrl' | 'sr-Latn' | 'sk';

/**
 * Defines values for ImageAnalysisSkillLanguage.
 * Possible values include: 'en', 'es', 'ja', 'pt', 'zh'
 * @readonly
 * @enum {string}
 */
export type ImageAnalysisSkillLanguage = 'en' | 'es' | 'ja' | 'pt' | 'zh';

/**
 * Defines values for VisualFeature.
 * Possible values include: 'adult', 'brands', 'categories', 'description', 'faces', 'objects',
 * 'tags'
 * @readonly
 * @enum {string}
 */
export type VisualFeature = 'adult' | 'brands' | 'categories' | 'description' | 'faces' | 'objects' | 'tags';

/**
 * Defines values for ImageDetail.
 * Possible values include: 'celebrities', 'landmarks'
 * @readonly
 * @enum {string}
 */
export type ImageDetail = 'celebrities' | 'landmarks';

/**
 * Defines values for EntityCategory.
 * Possible values include: 'location', 'organization', 'person', 'quantity', 'datetime', 'url',
 * 'email'
 * @readonly
 * @enum {string}
 */
export type EntityCategory = 'location' | 'organization' | 'person' | 'quantity' | 'datetime' | 'url' | 'email';

/**
 * Defines values for EntityRecognitionSkillLanguage.
 * Possible values include: 'ar', 'cs', 'zh-Hans', 'zh-Hant', 'da', 'nl', 'en', 'fi', 'fr', 'de',
 * 'el', 'hu', 'it', 'ja', 'ko', 'no', 'pl', 'pt-PT', 'pt-BR', 'ru', 'es', 'sv', 'tr'
 * @readonly
 * @enum {string}
 */
export type EntityRecognitionSkillLanguage = 'ar' | 'cs' | 'zh-Hans' | 'zh-Hant' | 'da' | 'nl' | 'en' | 'fi' | 'fr' | 'de' | 'el' | 'hu' | 'it' | 'ja' | 'ko' | 'no' | 'pl' | 'pt-PT' | 'pt-BR' | 'ru' | 'es' | 'sv' | 'tr';

/**
 * Defines values for SentimentSkillLanguage.
 * Possible values include: 'da', 'nl', 'en', 'fi', 'fr', 'de', 'el', 'it', 'no', 'pl', 'pt-PT',
 * 'ru', 'es', 'sv', 'tr'
 * @readonly
 * @enum {string}
 */
export type SentimentSkillLanguage = 'da' | 'nl' | 'en' | 'fi' | 'fr' | 'de' | 'el' | 'it' | 'no' | 'pl' | 'pt-PT' | 'ru' | 'es' | 'sv' | 'tr';

/**
 * Defines values for SplitSkillLanguage.
 * Possible values include: 'da', 'de', 'en', 'es', 'fi', 'fr', 'it', 'ko', 'pt'
 * @readonly
 * @enum {string}
 */
export type SplitSkillLanguage = 'da' | 'de' | 'en' | 'es' | 'fi' | 'fr' | 'it' | 'ko' | 'pt';

/**
 * Defines values for TextSplitMode.
 * Possible values include: 'pages', 'sentences'
 * @readonly
 * @enum {string}
 */
export type TextSplitMode = 'pages' | 'sentences';

/**
 * Defines values for TextTranslationSkillLanguage.
 * Possible values include: 'af', 'ar', 'bn', 'bs', 'bg', 'yue', 'ca', 'zh-Hans', 'zh-Hant', 'hr',
 * 'cs', 'da', 'nl', 'en', 'et', 'fj', 'fil', 'fi', 'fr', 'de', 'el', 'ht', 'he', 'hi', 'mww',
 * 'hu', 'is', 'id', 'it', 'ja', 'sw', 'tlh', 'ko', 'lv', 'lt', 'mg', 'ms', 'mt', 'nb', 'fa', 'pl',
 * 'pt', 'otq', 'ro', 'ru', 'sm', 'sr-Cyrl', 'sr-Latn', 'sk', 'sl', 'es', 'sv', 'ty', 'ta', 'te',
 * 'th', 'to', 'tr', 'uk', 'ur', 'vi', 'cy', 'yua'
 * @readonly
 * @enum {string}
 */
export type TextTranslationSkillLanguage = 'af' | 'ar' | 'bn' | 'bs' | 'bg' | 'yue' | 'ca' | 'zh-Hans' | 'zh-Hant' | 'hr' | 'cs' | 'da' | 'nl' | 'en' | 'et' | 'fj' | 'fil' | 'fi' | 'fr' | 'de' | 'el' | 'ht' | 'he' | 'hi' | 'mww' | 'hu' | 'is' | 'id' | 'it' | 'ja' | 'sw' | 'tlh' | 'ko' | 'lv' | 'lt' | 'mg' | 'ms' | 'mt' | 'nb' | 'fa' | 'pl' | 'pt' | 'otq' | 'ro' | 'ru' | 'sm' | 'sr-Cyrl' | 'sr-Latn' | 'sk' | 'sl' | 'es' | 'sv' | 'ty' | 'ta' | 'te' | 'th' | 'to' | 'tr' | 'uk' | 'ur' | 'vi' | 'cy' | 'yua';

/**
 * Contains response data for the createOrUpdate operation.
 */
export type DataSourcesCreateOrUpdateResponse = DataSource & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: DataSource;
    };
};

/**
 * Contains response data for the get operation.
 */
export type DataSourcesGetResponse = DataSource & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: DataSource;
    };
};

/**
 * Contains response data for the list operation.
 */
export type DataSourcesListResponse = ListDataSourcesResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: ListDataSourcesResult;
    };
};

/**
 * Contains response data for the create operation.
 */
export type DataSourcesCreateResponse = DataSource & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: DataSource;
    };
};

/**
 * Contains response data for the createOrUpdate operation.
 */
export type IndexersCreateOrUpdateResponse = Indexer & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: Indexer;
    };
};

/**
 * Contains response data for the get operation.
 */
export type IndexersGetResponse = Indexer & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: Indexer;
    };
};

/**
 * Contains response data for the list operation.
 */
export type IndexersListResponse = ListIndexersResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: ListIndexersResult;
    };
};

/**
 * Contains response data for the create operation.
 */
export type IndexersCreateResponse = Indexer & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: Indexer;
    };
};

/**
 * Contains response data for the getStatus operation.
 */
export type IndexersGetStatusResponse = IndexerExecutionInfo & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: IndexerExecutionInfo;
    };
};

/**
 * Contains response data for the createOrUpdate operation.
 */
export type SkillsetsCreateOrUpdateResponse = Skillset & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: Skillset;
    };
};

/**
 * Contains response data for the get operation.
 */
export type SkillsetsGetResponse = Skillset & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: Skillset;
    };
};

/**
 * Contains response data for the list operation.
 */
export type SkillsetsListResponse = ListSkillsetsResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: ListSkillsetsResult;
    };
};

/**
 * Contains response data for the create operation.
 */
export type SkillsetsCreateResponse = Skillset & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: Skillset;
    };
};

/**
 * Contains response data for the createOrUpdate operation.
 */
export type SynonymMapsCreateOrUpdateResponse = SynonymMap & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: SynonymMap;
    };
};

/**
 * Contains response data for the get operation.
 */
export type SynonymMapsGetResponse = SynonymMap & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: SynonymMap;
    };
};

/**
 * Contains response data for the list operation.
 */
export type SynonymMapsListResponse = ListSynonymMapsResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: ListSynonymMapsResult;
    };
};

/**
 * Contains response data for the create operation.
 */
export type SynonymMapsCreateResponse = SynonymMap & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: SynonymMap;
    };
};

/**
 * Contains response data for the create operation.
 */
export type IndexesCreateResponse = Index & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: Index;
    };
};

/**
 * Contains response data for the list operation.
 */
export type IndexesListResponse = ListIndexesResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: ListIndexesResult;
    };
};

/**
 * Contains response data for the createOrUpdate operation.
 */
export type IndexesCreateOrUpdateResponse = Index & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: Index;
    };
};

/**
 * Contains response data for the get operation.
 */
export type IndexesGetResponse = Index & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: Index;
    };
};

/**
 * Contains response data for the getStatistics operation.
 */
export type IndexesGetStatisticsResponse = GetIndexStatisticsResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: GetIndexStatisticsResult;
    };
};

/**
 * Contains response data for the analyze operation.
 */
export type IndexesAnalyzeResponse = AnalyzeResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: AnalyzeResult;
    };
};

/**
 * Contains response data for the getServiceStatistics operation.
 */
export type GetServiceStatisticsResponse = ServiceStatistics & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
      /**
       * The response body as text (string format)
       */
      bodyAsText: string;

      /**
       * The response body as parsed JSON or XML
       */
      parsedBody: ServiceStatistics;
    };
};
